{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Laboratoire 3 : Machines à vecteur de support et réseaux neuronaux\n",
    "#### Département du génie logiciel et des technologies de l’information\n",
    "\n",
    "| Étudiants             | Alexandre Bleau — BLEA14058906 / David Létinaud  — LETD05129708 / Thomas Lioret   — LIOT20069605|\n",
    "|-----------------------|---------------------------------------------------------|\n",
    "| Cours                 | GTI770 - Systèmes intelligents et apprentissage machine |\n",
    "| Session               | Automne 2019                                            |\n",
    "| Groupe                | 1                                                       |\n",
    "| Numéro du laboratoire | 3                                                       |\n",
    "| Professeur            | Prof. LOMBAERT                                          |\n",
    "| Chargé de laboratoire | Pierre-Luc Delisle                                      |\n",
    "| Date                  | 18/11/2019                                              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Au cours de ce troisième laboratoire nous allons étudier deux nouveaux algorithme de classification pour résoudre le problème de classification des galaxies : les réseaux neuronaux et les machines à vecteurs de support (SVM). \n",
    "Dans un premier temps, nous allons concevoir un modèle de réseaux neuronaux basé sur le Multi-Layer Perceptron. Nous entrainerons ce modèle afin qu'il puisse classer les galaxies en \"smooth\" ou \"spiral\" en utilisant l'ensemble des primitives. Nous utiliserons le module keras de Google tensorflow\n",
    "Le deuxième modèle d'apprentissage s'appuie sur un modèle d'optimisation convexe dans le cas du SVM. Dans ce cas, nous n'utiliserons qu'une partie des primitives proposées et seront couplées à nos primitives développées lors du premier laboratoire.\n",
    "Nous étudierons également l'influence des hyperparamètres de ces deux méthodes afin de proposer le modèle le plus optimal dans le cas de notre problème de classification de galaxies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Question 1\n",
    "(grille de correction : Les approches de validations sont présentées et utilisées correctement.)\n",
    "\n",
    "Dans le cas des réseaux neuronaux, l'utilisation d'une méthode de validation croisée prendrait beaucoup de temps. En effet, il faudrait répéter plusieurs tests avec un nombre \"d'epochs\" conséquent : ceci serait très chronophage. Nous avons donc décider d'utiliser la méthode de validation hold-out avec 70% de donées d'entrainement et 30% de test.\n",
    "\n",
    "Concernant la méthode SVM, nous utilisons la méthode ......... (k-fold????)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Question 2\n",
    "(La méthode de normalisation des données est décrite et correcte.)\n",
    "\n",
    "Nous normalisons nos données grâce à la méthode \"normalize\" de la librairie preprocessing. La normalisation se fait par rapport à la valeur maximale. Nous avons décider de normaliser par rapport aux primitives (axis = 0). Par ailleurs, nous avons remarqué avec nos premier tests que les performances étaient meilleures si l'on normalisait par rapport à la valeur maximale (norm = 'max')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Question 3\n",
    "(Le modèle MLP est décrit (structure, nombre de couches). La fonction de coût choisie est mentionnée ainsi que les raisons pour lesquelles elle a été choisie.)\n",
    "\n",
    "La principale contrainte du modèle MLP est de trouver un compromis entre le temps d'apprentissage et l'accuracy. La première architecture proposée par l'énoncé est de trois couches : 100, 100, 2 (nombre de perceptrons). Nous avons étudié différents cas avec 60 epochs :  \n",
    "1 - l'influence du nombre de perceptrons avec un nombre de couche fixé.  \n",
    "2 - l'influence du nombre de couches avec le même nombre de perceptrons par couche.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ens/AQ38840/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/compat.py:175: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# Initialisation\n",
    "# conda install -c conda-forge tensorflow \n",
    "from RN_model import *\n",
    "from functions import get_data , plot_perf, plot_delay\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "X_train, X_test, Y_train, Y_test = get_data()\n",
    "\n",
    "layer_sizes = [100, 100, 2]\n",
    "epochs = 60\n",
    "learning_rate = 0.0005\n",
    "batch_size = 100\n",
    "\n",
    "dropout = 0.5\n",
    "\n",
    "# Pour affichage\n",
    "sub_title = ['loss','acc','f1','val_loss','val_acc', 'val_f1']\n",
    "x_lab = \"epochs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "Voici donc nos résultats avec différentes structures de réseaux de neurones :\n",
    "\n",
    "1 - a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ens/AQ38840/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ens/AQ38840/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "WARNING:tensorflow:From /home/ens/AQ38840/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/60\n",
      "12800/12800 [==============================] - 0s 28us/sample - loss: 0.6932 - acc: 0.5165 - f1: 0.6763 - val_loss: 0.6927 - val_acc: 0.5191 - val_f1: 0.6827\n",
      "Epoch 2/60\n",
      "12800/12800 [==============================] - 0s 11us/sample - loss: 0.6926 - acc: 0.5195 - f1: 0.6811 - val_loss: 0.6924 - val_acc: 0.5191 - val_f1: 0.6814\n",
      "Epoch 3/60\n",
      "12800/12800 [==============================] - 0s 11us/sample - loss: 0.6925 - acc: 0.5176 - f1: 0.6802 - val_loss: 0.6922 - val_acc: 0.5191 - val_f1: 0.6821\n",
      "Epoch 4/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.6823 - acc: 0.5073 - f1: 0.6697 - val_loss: 0.6564 - val_acc: 0.5191 - val_f1: 0.6821\n",
      "Epoch 5/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.6512 - acc: 0.5150 - f1: 0.6784 - val_loss: 0.6042 - val_acc: 0.5191 - val_f1: 0.6821\n",
      "Epoch 6/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.6282 - acc: 0.6591 - f1: 0.5898 - val_loss: 0.5667 - val_acc: 0.8234 - val_f1: 0.8493\n",
      "Epoch 7/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.6090 - acc: 0.6877 - f1: 0.6199 - val_loss: 0.5537 - val_acc: 0.9016 - val_f1: 0.9052\n",
      "Epoch 8/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.5896 - acc: 0.6986 - f1: 0.6317 - val_loss: 0.5057 - val_acc: 0.9103 - val_f1: 0.9139\n",
      "Epoch 9/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.5634 - acc: 0.7194 - f1: 0.6606 - val_loss: 0.4591 - val_acc: 0.9153 - val_f1: 0.9190\n",
      "Epoch 10/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.5544 - acc: 0.7252 - f1: 0.6672 - val_loss: 0.4322 - val_acc: 0.9212 - val_f1: 0.9234\n",
      "Epoch 11/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.5347 - acc: 0.7509 - f1: 0.7031 - val_loss: 0.4083 - val_acc: 0.9228 - val_f1: 0.9244\n",
      "Epoch 12/60\n",
      "12800/12800 [==============================] - 0s 11us/sample - loss: 0.5349 - acc: 0.7462 - f1: 0.6927 - val_loss: 0.4148 - val_acc: 0.9209 - val_f1: 0.9197\n",
      "Epoch 13/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.5189 - acc: 0.7555 - f1: 0.7063 - val_loss: 0.3725 - val_acc: 0.9297 - val_f1: 0.9309\n",
      "Epoch 14/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.5061 - acc: 0.7605 - f1: 0.7118 - val_loss: 0.3522 - val_acc: 0.9203 - val_f1: 0.9256\n",
      "Epoch 15/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.5103 - acc: 0.7608 - f1: 0.7109 - val_loss: 0.3532 - val_acc: 0.9319 - val_f1: 0.9329\n",
      "Epoch 16/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4866 - acc: 0.7759 - f1: 0.7328 - val_loss: 0.3747 - val_acc: 0.9050 - val_f1: 0.9006\n",
      "Epoch 17/60\n",
      "12800/12800 [==============================] - 0s 11us/sample - loss: 0.4915 - acc: 0.7728 - f1: 0.7284 - val_loss: 0.3233 - val_acc: 0.9344 - val_f1: 0.9369\n",
      "Epoch 18/60\n",
      "12800/12800 [==============================] - 0s 11us/sample - loss: 0.4870 - acc: 0.7784 - f1: 0.7359 - val_loss: 0.3389 - val_acc: 0.9253 - val_f1: 0.9233\n",
      "Epoch 19/60\n",
      "12800/12800 [==============================] - 0s 11us/sample - loss: 0.4852 - acc: 0.7771 - f1: 0.7336 - val_loss: 0.3154 - val_acc: 0.9394 - val_f1: 0.9398\n",
      "Epoch 20/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4710 - acc: 0.7880 - f1: 0.7504 - val_loss: 0.3258 - val_acc: 0.9253 - val_f1: 0.9235\n",
      "Epoch 21/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4730 - acc: 0.7850 - f1: 0.7453 - val_loss: 0.3107 - val_acc: 0.9391 - val_f1: 0.9390\n",
      "Epoch 22/60\n",
      "12800/12800 [==============================] - 0s 11us/sample - loss: 0.4735 - acc: 0.7857 - f1: 0.7468 - val_loss: 0.3011 - val_acc: 0.9381 - val_f1: 0.9387\n",
      "Epoch 23/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4714 - acc: 0.7809 - f1: 0.7407 - val_loss: 0.3012 - val_acc: 0.9350 - val_f1: 0.9351\n",
      "Epoch 24/60\n",
      "12800/12800 [==============================] - 0s 11us/sample - loss: 0.4729 - acc: 0.7829 - f1: 0.7423 - val_loss: 0.2968 - val_acc: 0.9388 - val_f1: 0.9390\n",
      "Epoch 25/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4658 - acc: 0.7880 - f1: 0.7489 - val_loss: 0.2984 - val_acc: 0.9312 - val_f1: 0.9307\n",
      "Epoch 26/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4672 - acc: 0.7865 - f1: 0.7462 - val_loss: 0.3013 - val_acc: 0.9262 - val_f1: 0.9246\n",
      "Epoch 27/60\n",
      "12800/12800 [==============================] - 0s 11us/sample - loss: 0.4689 - acc: 0.7873 - f1: 0.7480 - val_loss: 0.2834 - val_acc: 0.9453 - val_f1: 0.9464\n",
      "Epoch 28/60\n",
      "12800/12800 [==============================] - 0s 11us/sample - loss: 0.4551 - acc: 0.7956 - f1: 0.7594 - val_loss: 0.2817 - val_acc: 0.9431 - val_f1: 0.9446\n",
      "Epoch 29/60\n",
      "12800/12800 [==============================] - 0s 11us/sample - loss: 0.4621 - acc: 0.7921 - f1: 0.7542 - val_loss: 0.2854 - val_acc: 0.9366 - val_f1: 0.9368\n",
      "Epoch 30/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4539 - acc: 0.7959 - f1: 0.7596 - val_loss: 0.2762 - val_acc: 0.9444 - val_f1: 0.9446\n",
      "Epoch 31/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4612 - acc: 0.7896 - f1: 0.7510 - val_loss: 0.2775 - val_acc: 0.9447 - val_f1: 0.9456\n",
      "Epoch 32/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4604 - acc: 0.7961 - f1: 0.7604 - val_loss: 0.2741 - val_acc: 0.9447 - val_f1: 0.9454\n",
      "Epoch 33/60\n",
      "12800/12800 [==============================] - 0s 11us/sample - loss: 0.4583 - acc: 0.7926 - f1: 0.7561 - val_loss: 0.2939 - val_acc: 0.9234 - val_f1: 0.9209\n",
      "Epoch 34/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4505 - acc: 0.7973 - f1: 0.7619 - val_loss: 0.2704 - val_acc: 0.9466 - val_f1: 0.9482\n",
      "Epoch 35/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4620 - acc: 0.7893 - f1: 0.7509 - val_loss: 0.2703 - val_acc: 0.9463 - val_f1: 0.9474\n",
      "Epoch 36/60\n",
      "12800/12800 [==============================] - 0s 11us/sample - loss: 0.4540 - acc: 0.7986 - f1: 0.7647 - val_loss: 0.2690 - val_acc: 0.9456 - val_f1: 0.9471\n",
      "Epoch 37/60\n",
      "12800/12800 [==============================] - 0s 11us/sample - loss: 0.4597 - acc: 0.7897 - f1: 0.7500 - val_loss: 0.2727 - val_acc: 0.9422 - val_f1: 0.9421\n",
      "Epoch 38/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4539 - acc: 0.7921 - f1: 0.7544 - val_loss: 0.2704 - val_acc: 0.9475 - val_f1: 0.9481\n",
      "Epoch 39/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4553 - acc: 0.7958 - f1: 0.7601 - val_loss: 0.2881 - val_acc: 0.9281 - val_f1: 0.9272\n",
      "Epoch 40/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4551 - acc: 0.7988 - f1: 0.7651 - val_loss: 0.2658 - val_acc: 0.9459 - val_f1: 0.9468\n",
      "Epoch 41/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4539 - acc: 0.7926 - f1: 0.7541 - val_loss: 0.2708 - val_acc: 0.9394 - val_f1: 0.9398\n",
      "Epoch 42/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4632 - acc: 0.7897 - f1: 0.7498 - val_loss: 0.2770 - val_acc: 0.9366 - val_f1: 0.9356\n",
      "Epoch 43/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4492 - acc: 0.7997 - f1: 0.7654 - val_loss: 0.2821 - val_acc: 0.9291 - val_f1: 0.9271\n",
      "Epoch 44/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4509 - acc: 0.7940 - f1: 0.7569 - val_loss: 0.2721 - val_acc: 0.9397 - val_f1: 0.9390\n",
      "Epoch 45/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4542 - acc: 0.7938 - f1: 0.7574 - val_loss: 0.2797 - val_acc: 0.9312 - val_f1: 0.9296\n",
      "Epoch 46/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4523 - acc: 0.8009 - f1: 0.7656 - val_loss: 0.2751 - val_acc: 0.9356 - val_f1: 0.9353\n",
      "Epoch 47/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4513 - acc: 0.7945 - f1: 0.7570 - val_loss: 0.2815 - val_acc: 0.9275 - val_f1: 0.9263\n",
      "Epoch 48/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4461 - acc: 0.7999 - f1: 0.7649 - val_loss: 0.2719 - val_acc: 0.9388 - val_f1: 0.9386\n",
      "Epoch 49/60\n",
      "12800/12800 [==============================] - 0s 11us/sample - loss: 0.4528 - acc: 0.7962 - f1: 0.7597 - val_loss: 0.2724 - val_acc: 0.9369 - val_f1: 0.9364\n",
      "Epoch 50/60\n",
      "12800/12800 [==============================] - 0s 11us/sample - loss: 0.4623 - acc: 0.7891 - f1: 0.7492 - val_loss: 0.2653 - val_acc: 0.9472 - val_f1: 0.9483\n",
      "Epoch 51/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4532 - acc: 0.7962 - f1: 0.7594 - val_loss: 0.2630 - val_acc: 0.9494 - val_f1: 0.9504\n",
      "Epoch 52/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4502 - acc: 0.7987 - f1: 0.7635 - val_loss: 0.2688 - val_acc: 0.9463 - val_f1: 0.9479\n",
      "Epoch 53/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4499 - acc: 0.7973 - f1: 0.7609 - val_loss: 0.2719 - val_acc: 0.9397 - val_f1: 0.9388\n",
      "Epoch 54/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4508 - acc: 0.7959 - f1: 0.7590 - val_loss: 0.2709 - val_acc: 0.9391 - val_f1: 0.9382\n",
      "Epoch 55/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4334 - acc: 0.8030 - f1: 0.7683 - val_loss: 0.2633 - val_acc: 0.9447 - val_f1: 0.9444\n",
      "Epoch 56/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4521 - acc: 0.7946 - f1: 0.7562 - val_loss: 0.2785 - val_acc: 0.9303 - val_f1: 0.9289\n",
      "Epoch 57/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4454 - acc: 0.8028 - f1: 0.7695 - val_loss: 0.2778 - val_acc: 0.9294 - val_f1: 0.9280\n",
      "Epoch 58/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4567 - acc: 0.7920 - f1: 0.7527 - val_loss: 0.2684 - val_acc: 0.9500 - val_f1: 0.9518\n",
      "Epoch 59/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4482 - acc: 0.7977 - f1: 0.7626 - val_loss: 0.2991 - val_acc: 0.9125 - val_f1: 0.9087\n",
      "Epoch 60/60\n",
      "12800/12800 [==============================] - 0s 10us/sample - loss: 0.4489 - acc: 0.7982 - f1: 0.7619 - val_loss: 0.2601 - val_acc: 0.9491 - val_f1: 0.9497\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/60\n",
      "12800/12800 [==============================] - 0s 33us/sample - loss: 0.6947 - acc: 0.5028 - f1: 0.4201 - val_loss: 0.6926 - val_acc: 0.5191 - val_f1: 0.6816\n",
      "Epoch 2/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.6533 - acc: 0.5773 - f1: 0.6714 - val_loss: 0.5607 - val_acc: 0.8647 - val_f1: 0.8735\n",
      "Epoch 3/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.5527 - acc: 0.7585 - f1: 0.7469 - val_loss: 0.4458 - val_acc: 0.9034 - val_f1: 0.9091\n",
      "Epoch 4/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.5071 - acc: 0.7957 - f1: 0.7765 - val_loss: 0.4005 - val_acc: 0.9059 - val_f1: 0.9146\n",
      "Epoch 5/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4878 - acc: 0.8010 - f1: 0.7790 - val_loss: 0.3870 - val_acc: 0.9322 - val_f1: 0.9325\n",
      "Epoch 6/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4645 - acc: 0.8141 - f1: 0.7938 - val_loss: 0.3599 - val_acc: 0.9366 - val_f1: 0.9389\n",
      "Epoch 7/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4592 - acc: 0.8141 - f1: 0.7912 - val_loss: 0.3600 - val_acc: 0.9278 - val_f1: 0.9270\n",
      "Epoch 8/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4493 - acc: 0.8145 - f1: 0.7908 - val_loss: 0.3289 - val_acc: 0.9409 - val_f1: 0.9422\n",
      "Epoch 9/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.4360 - acc: 0.8234 - f1: 0.8032 - val_loss: 0.3501 - val_acc: 0.9197 - val_f1: 0.9169\n",
      "Epoch 10/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4332 - acc: 0.8175 - f1: 0.7949 - val_loss: 0.3275 - val_acc: 0.9303 - val_f1: 0.9299\n",
      "Epoch 11/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4324 - acc: 0.8185 - f1: 0.7959 - val_loss: 0.3130 - val_acc: 0.9384 - val_f1: 0.9386\n",
      "Epoch 12/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4263 - acc: 0.8242 - f1: 0.8027 - val_loss: 0.3077 - val_acc: 0.9397 - val_f1: 0.9396\n",
      "Epoch 13/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4225 - acc: 0.8244 - f1: 0.8023 - val_loss: 0.2881 - val_acc: 0.9447 - val_f1: 0.9460\n",
      "Epoch 14/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4151 - acc: 0.8283 - f1: 0.8071 - val_loss: 0.2833 - val_acc: 0.9466 - val_f1: 0.9482\n",
      "Epoch 15/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4177 - acc: 0.8235 - f1: 0.8016 - val_loss: 0.2825 - val_acc: 0.9444 - val_f1: 0.9441\n",
      "Epoch 16/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4142 - acc: 0.8261 - f1: 0.8040 - val_loss: 0.2803 - val_acc: 0.9466 - val_f1: 0.9473\n",
      "Epoch 17/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4075 - acc: 0.8305 - f1: 0.8106 - val_loss: 0.3081 - val_acc: 0.9203 - val_f1: 0.9166\n",
      "Epoch 18/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4215 - acc: 0.8180 - f1: 0.7935 - val_loss: 0.2711 - val_acc: 0.9472 - val_f1: 0.9477\n",
      "Epoch 19/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4139 - acc: 0.8206 - f1: 0.7973 - val_loss: 0.2621 - val_acc: 0.9488 - val_f1: 0.9479\n",
      "Epoch 20/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4068 - acc: 0.8282 - f1: 0.8068 - val_loss: 0.2535 - val_acc: 0.9491 - val_f1: 0.9514\n",
      "Epoch 21/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3966 - acc: 0.8339 - f1: 0.8143 - val_loss: 0.2723 - val_acc: 0.9359 - val_f1: 0.9353\n",
      "Epoch 22/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.4033 - acc: 0.8314 - f1: 0.8115 - val_loss: 0.2625 - val_acc: 0.9419 - val_f1: 0.9416\n",
      "Epoch 23/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.4008 - acc: 0.8275 - f1: 0.8057 - val_loss: 0.2568 - val_acc: 0.9478 - val_f1: 0.9482\n",
      "Epoch 24/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.4035 - acc: 0.8289 - f1: 0.8079 - val_loss: 0.2410 - val_acc: 0.9534 - val_f1: 0.9551\n",
      "Epoch 25/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.4056 - acc: 0.8257 - f1: 0.8037 - val_loss: 0.2541 - val_acc: 0.9466 - val_f1: 0.9461\n",
      "Epoch 26/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4047 - acc: 0.8255 - f1: 0.8028 - val_loss: 0.2396 - val_acc: 0.9516 - val_f1: 0.9535\n",
      "Epoch 27/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3917 - acc: 0.8342 - f1: 0.8146 - val_loss: 0.2682 - val_acc: 0.9328 - val_f1: 0.9309\n",
      "Epoch 28/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.3971 - acc: 0.8320 - f1: 0.8120 - val_loss: 0.2356 - val_acc: 0.9519 - val_f1: 0.9538\n",
      "Epoch 29/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3935 - acc: 0.8328 - f1: 0.8130 - val_loss: 0.2370 - val_acc: 0.9531 - val_f1: 0.9541\n",
      "Epoch 30/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4023 - acc: 0.8287 - f1: 0.8072 - val_loss: 0.2310 - val_acc: 0.9506 - val_f1: 0.9524\n",
      "Epoch 31/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3973 - acc: 0.8302 - f1: 0.8104 - val_loss: 0.2369 - val_acc: 0.9488 - val_f1: 0.9496\n",
      "Epoch 32/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3958 - acc: 0.8298 - f1: 0.8085 - val_loss: 0.2326 - val_acc: 0.9519 - val_f1: 0.9520\n",
      "Epoch 33/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.3863 - acc: 0.8363 - f1: 0.8171 - val_loss: 0.2253 - val_acc: 0.9575 - val_f1: 0.9591\n",
      "Epoch 34/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.3911 - acc: 0.8333 - f1: 0.8127 - val_loss: 0.2284 - val_acc: 0.9519 - val_f1: 0.9524\n",
      "Epoch 35/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4060 - acc: 0.8235 - f1: 0.7996 - val_loss: 0.2559 - val_acc: 0.9350 - val_f1: 0.9336\n",
      "Epoch 36/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.3938 - acc: 0.8312 - f1: 0.8104 - val_loss: 0.2437 - val_acc: 0.9453 - val_f1: 0.9446\n",
      "Epoch 37/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.3922 - acc: 0.8320 - f1: 0.8121 - val_loss: 0.2252 - val_acc: 0.9553 - val_f1: 0.9560\n",
      "Epoch 38/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.3901 - acc: 0.8356 - f1: 0.8151 - val_loss: 0.2312 - val_acc: 0.9509 - val_f1: 0.9510\n",
      "Epoch 39/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.3878 - acc: 0.8332 - f1: 0.8125 - val_loss: 0.2429 - val_acc: 0.9419 - val_f1: 0.9411\n",
      "Epoch 40/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.3859 - acc: 0.8375 - f1: 0.8179 - val_loss: 0.2498 - val_acc: 0.9381 - val_f1: 0.9377\n",
      "Epoch 41/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.3998 - acc: 0.8284 - f1: 0.8064 - val_loss: 0.2207 - val_acc: 0.9572 - val_f1: 0.9572\n",
      "Epoch 42/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.3911 - acc: 0.8338 - f1: 0.8133 - val_loss: 0.2423 - val_acc: 0.9406 - val_f1: 0.9442\n",
      "Epoch 43/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3914 - acc: 0.8315 - f1: 0.8107 - val_loss: 0.2324 - val_acc: 0.9478 - val_f1: 0.9471\n",
      "Epoch 44/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.3860 - acc: 0.8359 - f1: 0.8154 - val_loss: 0.2213 - val_acc: 0.9550 - val_f1: 0.9559\n",
      "Epoch 45/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.3924 - acc: 0.8303 - f1: 0.8096 - val_loss: 0.2167 - val_acc: 0.9575 - val_f1: 0.9585\n",
      "Epoch 46/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3897 - acc: 0.8341 - f1: 0.8143 - val_loss: 0.2246 - val_acc: 0.9534 - val_f1: 0.9543\n",
      "Epoch 47/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.3798 - acc: 0.8371 - f1: 0.8174 - val_loss: 0.2221 - val_acc: 0.9528 - val_f1: 0.9537\n",
      "Epoch 48/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3897 - acc: 0.8334 - f1: 0.8132 - val_loss: 0.2362 - val_acc: 0.9419 - val_f1: 0.9407\n",
      "Epoch 49/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3868 - acc: 0.8366 - f1: 0.8168 - val_loss: 0.2153 - val_acc: 0.9563 - val_f1: 0.9574\n",
      "Epoch 50/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3871 - acc: 0.8361 - f1: 0.8163 - val_loss: 0.2213 - val_acc: 0.9550 - val_f1: 0.9561\n",
      "Epoch 51/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3850 - acc: 0.8367 - f1: 0.8170 - val_loss: 0.2145 - val_acc: 0.9578 - val_f1: 0.9595\n",
      "Epoch 52/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3853 - acc: 0.8354 - f1: 0.8156 - val_loss: 0.2190 - val_acc: 0.9566 - val_f1: 0.9573\n",
      "Epoch 53/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.3902 - acc: 0.8341 - f1: 0.8132 - val_loss: 0.2212 - val_acc: 0.9531 - val_f1: 0.9534\n",
      "Epoch 54/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.3845 - acc: 0.8384 - f1: 0.8205 - val_loss: 0.2296 - val_acc: 0.9509 - val_f1: 0.9507\n",
      "Epoch 55/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.3817 - acc: 0.8391 - f1: 0.8195 - val_loss: 0.2170 - val_acc: 0.9566 - val_f1: 0.9573\n",
      "Epoch 56/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3958 - acc: 0.8276 - f1: 0.8041 - val_loss: 0.2152 - val_acc: 0.9559 - val_f1: 0.9584\n",
      "Epoch 57/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3843 - acc: 0.8345 - f1: 0.8145 - val_loss: 0.2158 - val_acc: 0.9578 - val_f1: 0.9584\n",
      "Epoch 58/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3853 - acc: 0.8364 - f1: 0.8160 - val_loss: 0.2196 - val_acc: 0.9550 - val_f1: 0.9562\n",
      "Epoch 59/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3779 - acc: 0.8404 - f1: 0.8226 - val_loss: 0.2142 - val_acc: 0.9566 - val_f1: 0.9583\n",
      "Epoch 60/60\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3732 - acc: 0.8428 - f1: 0.8252 - val_loss: 0.2110 - val_acc: 0.9563 - val_f1: 0.9575\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/60\n",
      "12800/12800 [==============================] - 1s 77us/sample - loss: 0.5002 - acc: 0.7407 - f1: 0.7518 - val_loss: 0.2583 - val_acc: 0.8972 - val_f1: 0.9003\n",
      "Epoch 2/60\n",
      "12800/12800 [==============================] - 1s 60us/sample - loss: 0.2443 - acc: 0.9042 - f1: 0.9071 - val_loss: 0.1887 - val_acc: 0.9291 - val_f1: 0.9335\n",
      "Epoch 3/60\n",
      "12800/12800 [==============================] - 1s 59us/sample - loss: 0.1938 - acc: 0.9241 - f1: 0.9265 - val_loss: 0.1520 - val_acc: 0.9444 - val_f1: 0.9467\n",
      "Epoch 4/60\n",
      "12800/12800 [==============================] - 1s 59us/sample - loss: 0.1656 - acc: 0.9370 - f1: 0.9386 - val_loss: 0.1541 - val_acc: 0.9444 - val_f1: 0.9475\n",
      "Epoch 5/60\n",
      "12800/12800 [==============================] - 1s 59us/sample - loss: 0.1650 - acc: 0.9377 - f1: 0.9396 - val_loss: 0.1340 - val_acc: 0.9478 - val_f1: 0.9491\n",
      "Epoch 6/60\n",
      "12800/12800 [==============================] - 1s 59us/sample - loss: 0.1539 - acc: 0.9413 - f1: 0.9432 - val_loss: 0.1327 - val_acc: 0.9491 - val_f1: 0.9517\n",
      "Epoch 7/60\n",
      "12800/12800 [==============================] - 1s 58us/sample - loss: 0.1434 - acc: 0.9466 - f1: 0.9483 - val_loss: 0.1279 - val_acc: 0.9534 - val_f1: 0.9546\n",
      "Epoch 8/60\n",
      "12800/12800 [==============================] - 1s 60us/sample - loss: 0.1410 - acc: 0.9472 - f1: 0.9488 - val_loss: 0.1819 - val_acc: 0.9309 - val_f1: 0.9364\n",
      "Epoch 9/60\n",
      "12800/12800 [==============================] - 1s 59us/sample - loss: 0.1360 - acc: 0.9506 - f1: 0.9524 - val_loss: 0.1286 - val_acc: 0.9516 - val_f1: 0.9536\n",
      "Epoch 10/60\n",
      "12800/12800 [==============================] - 1s 60us/sample - loss: 0.1323 - acc: 0.9507 - f1: 0.9520 - val_loss: 0.1142 - val_acc: 0.9588 - val_f1: 0.9602\n",
      "Epoch 11/60\n",
      "12800/12800 [==============================] - 1s 59us/sample - loss: 0.1291 - acc: 0.9528 - f1: 0.9540 - val_loss: 0.1413 - val_acc: 0.9459 - val_f1: 0.9491\n",
      "Epoch 12/60\n",
      "12800/12800 [==============================] - 1s 61us/sample - loss: 0.1220 - acc: 0.9546 - f1: 0.9559 - val_loss: 0.1153 - val_acc: 0.9588 - val_f1: 0.9607\n",
      "Epoch 13/60\n",
      "12800/12800 [==============================] - 1s 60us/sample - loss: 0.1255 - acc: 0.9556 - f1: 0.9573 - val_loss: 0.1123 - val_acc: 0.9594 - val_f1: 0.9607\n",
      "Epoch 14/60\n",
      "12800/12800 [==============================] - 1s 61us/sample - loss: 0.1233 - acc: 0.9549 - f1: 0.9562 - val_loss: 0.1261 - val_acc: 0.9531 - val_f1: 0.9538\n",
      "Epoch 15/60\n",
      "12800/12800 [==============================] - 1s 62us/sample - loss: 0.1166 - acc: 0.9584 - f1: 0.9595 - val_loss: 0.1094 - val_acc: 0.9581 - val_f1: 0.9592\n",
      "Epoch 16/60\n",
      "12800/12800 [==============================] - 1s 60us/sample - loss: 0.1199 - acc: 0.9549 - f1: 0.9562 - val_loss: 0.1090 - val_acc: 0.9563 - val_f1: 0.9572\n",
      "Epoch 17/60\n",
      "12800/12800 [==============================] - 1s 61us/sample - loss: 0.1166 - acc: 0.9553 - f1: 0.9567 - val_loss: 0.1159 - val_acc: 0.9575 - val_f1: 0.9585\n",
      "Epoch 18/60\n",
      "12800/12800 [==============================] - 1s 60us/sample - loss: 0.1114 - acc: 0.9589 - f1: 0.9601 - val_loss: 0.1034 - val_acc: 0.9616 - val_f1: 0.9631\n",
      "Epoch 19/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 58us/sample - loss: 0.1138 - acc: 0.9580 - f1: 0.9591 - val_loss: 0.1055 - val_acc: 0.9609 - val_f1: 0.9631\n",
      "Epoch 20/60\n",
      "12800/12800 [==============================] - 1s 58us/sample - loss: 0.1114 - acc: 0.9595 - f1: 0.9608 - val_loss: 0.1020 - val_acc: 0.9619 - val_f1: 0.9630\n",
      "Epoch 21/60\n",
      "12800/12800 [==============================] - 1s 58us/sample - loss: 0.1098 - acc: 0.9577 - f1: 0.9587 - val_loss: 0.1039 - val_acc: 0.9613 - val_f1: 0.9625\n",
      "Epoch 22/60\n",
      "12800/12800 [==============================] - 1s 59us/sample - loss: 0.1100 - acc: 0.9591 - f1: 0.9606 - val_loss: 0.1045 - val_acc: 0.9597 - val_f1: 0.9608\n",
      "Epoch 23/60\n",
      "12800/12800 [==============================] - 1s 61us/sample - loss: 0.1100 - acc: 0.9582 - f1: 0.9594 - val_loss: 0.1122 - val_acc: 0.9566 - val_f1: 0.9568\n",
      "Epoch 24/60\n",
      "12800/12800 [==============================] - 1s 61us/sample - loss: 0.1077 - acc: 0.9598 - f1: 0.9610 - val_loss: 0.1414 - val_acc: 0.9469 - val_f1: 0.9498\n",
      "Epoch 25/60\n",
      "12800/12800 [==============================] - 1s 61us/sample - loss: 0.1113 - acc: 0.9581 - f1: 0.9594 - val_loss: 0.1061 - val_acc: 0.9631 - val_f1: 0.9639\n",
      "Epoch 26/60\n",
      "12800/12800 [==============================] - 1s 62us/sample - loss: 0.1021 - acc: 0.9636 - f1: 0.9649 - val_loss: 0.1075 - val_acc: 0.9600 - val_f1: 0.9615\n",
      "Epoch 27/60\n",
      "12800/12800 [==============================] - 1s 63us/sample - loss: 0.1012 - acc: 0.9617 - f1: 0.9627 - val_loss: 0.1136 - val_acc: 0.9581 - val_f1: 0.9598\n",
      "Epoch 28/60\n",
      "12800/12800 [==============================] - 1s 65us/sample - loss: 0.1044 - acc: 0.9601 - f1: 0.9612 - val_loss: 0.1079 - val_acc: 0.9603 - val_f1: 0.9623\n",
      "Epoch 29/60\n",
      "12800/12800 [==============================] - 1s 63us/sample - loss: 0.1003 - acc: 0.9619 - f1: 0.9631 - val_loss: 0.1111 - val_acc: 0.9597 - val_f1: 0.9601\n",
      "Epoch 30/60\n",
      "12800/12800 [==============================] - 1s 62us/sample - loss: 0.1070 - acc: 0.9592 - f1: 0.9604 - val_loss: 0.1008 - val_acc: 0.9628 - val_f1: 0.9633\n",
      "Epoch 31/60\n",
      "12800/12800 [==============================] - 1s 64us/sample - loss: 0.1049 - acc: 0.9616 - f1: 0.9628 - val_loss: 0.1019 - val_acc: 0.9628 - val_f1: 0.9641\n",
      "Epoch 32/60\n",
      "12800/12800 [==============================] - 1s 62us/sample - loss: 0.1012 - acc: 0.9638 - f1: 0.9647 - val_loss: 0.0966 - val_acc: 0.9638 - val_f1: 0.9645\n",
      "Epoch 33/60\n",
      "12800/12800 [==============================] - 1s 63us/sample - loss: 0.1007 - acc: 0.9623 - f1: 0.9632 - val_loss: 0.0955 - val_acc: 0.9638 - val_f1: 0.9651\n",
      "Epoch 34/60\n",
      "12800/12800 [==============================] - 1s 63us/sample - loss: 0.0965 - acc: 0.9641 - f1: 0.9653 - val_loss: 0.1013 - val_acc: 0.9622 - val_f1: 0.9633\n",
      "Epoch 35/60\n",
      "12800/12800 [==============================] - 1s 60us/sample - loss: 0.0964 - acc: 0.9637 - f1: 0.9651 - val_loss: 0.0952 - val_acc: 0.9669 - val_f1: 0.9680\n",
      "Epoch 36/60\n",
      "12800/12800 [==============================] - 1s 60us/sample - loss: 0.0964 - acc: 0.9643 - f1: 0.9655 - val_loss: 0.1124 - val_acc: 0.9559 - val_f1: 0.9556\n",
      "Epoch 37/60\n",
      "12800/12800 [==============================] - 1s 60us/sample - loss: 0.0941 - acc: 0.9662 - f1: 0.9671 - val_loss: 0.0979 - val_acc: 0.9631 - val_f1: 0.9648\n",
      "Epoch 38/60\n",
      "12800/12800 [==============================] - 1s 62us/sample - loss: 0.0956 - acc: 0.9643 - f1: 0.9654 - val_loss: 0.1008 - val_acc: 0.9641 - val_f1: 0.9653\n",
      "Epoch 39/60\n",
      "12800/12800 [==============================] - 1s 62us/sample - loss: 0.0943 - acc: 0.9663 - f1: 0.9673 - val_loss: 0.1130 - val_acc: 0.9606 - val_f1: 0.9611\n",
      "Epoch 40/60\n",
      "12800/12800 [==============================] - 1s 62us/sample - loss: 0.0965 - acc: 0.9634 - f1: 0.9645 - val_loss: 0.1071 - val_acc: 0.9609 - val_f1: 0.9618\n",
      "Epoch 41/60\n",
      "12800/12800 [==============================] - 1s 62us/sample - loss: 0.0945 - acc: 0.9655 - f1: 0.9662 - val_loss: 0.1001 - val_acc: 0.9650 - val_f1: 0.9661\n",
      "Epoch 42/60\n",
      "12800/12800 [==============================] - 1s 62us/sample - loss: 0.0911 - acc: 0.9659 - f1: 0.9669 - val_loss: 0.1043 - val_acc: 0.9631 - val_f1: 0.9644\n",
      "Epoch 43/60\n",
      "12800/12800 [==============================] - 1s 62us/sample - loss: 0.0937 - acc: 0.9662 - f1: 0.9673 - val_loss: 0.1075 - val_acc: 0.9609 - val_f1: 0.9613\n",
      "Epoch 44/60\n",
      "12800/12800 [==============================] - 1s 63us/sample - loss: 0.0940 - acc: 0.9668 - f1: 0.9676 - val_loss: 0.1083 - val_acc: 0.9625 - val_f1: 0.9643\n",
      "Epoch 45/60\n",
      "12800/12800 [==============================] - 1s 63us/sample - loss: 0.0907 - acc: 0.9663 - f1: 0.9675 - val_loss: 0.1107 - val_acc: 0.9613 - val_f1: 0.9623\n",
      "Epoch 46/60\n",
      "12800/12800 [==============================] - 1s 63us/sample - loss: 0.0900 - acc: 0.9660 - f1: 0.9670 - val_loss: 0.1078 - val_acc: 0.9631 - val_f1: 0.9647\n",
      "Epoch 47/60\n",
      "12800/12800 [==============================] - 1s 62us/sample - loss: 0.0897 - acc: 0.9675 - f1: 0.9685 - val_loss: 0.1202 - val_acc: 0.9581 - val_f1: 0.9589\n",
      "Epoch 48/60\n",
      "12800/12800 [==============================] - 1s 63us/sample - loss: 0.0857 - acc: 0.9696 - f1: 0.9706 - val_loss: 0.1044 - val_acc: 0.9672 - val_f1: 0.9689\n",
      "Epoch 49/60\n",
      "12800/12800 [==============================] - 1s 62us/sample - loss: 0.0877 - acc: 0.9666 - f1: 0.9675 - val_loss: 0.1051 - val_acc: 0.9656 - val_f1: 0.9665\n",
      "Epoch 50/60\n",
      "12800/12800 [==============================] - 1s 62us/sample - loss: 0.0847 - acc: 0.9694 - f1: 0.9703 - val_loss: 0.1093 - val_acc: 0.9625 - val_f1: 0.9639\n",
      "Epoch 51/60\n",
      "12800/12800 [==============================] - 1s 63us/sample - loss: 0.0820 - acc: 0.9705 - f1: 0.9718 - val_loss: 0.1021 - val_acc: 0.9659 - val_f1: 0.9667\n",
      "Epoch 52/60\n",
      "12800/12800 [==============================] - 1s 63us/sample - loss: 0.0865 - acc: 0.9677 - f1: 0.9685 - val_loss: 0.1060 - val_acc: 0.9619 - val_f1: 0.9631\n",
      "Epoch 53/60\n",
      "12800/12800 [==============================] - 1s 63us/sample - loss: 0.0852 - acc: 0.9691 - f1: 0.9700 - val_loss: 0.1013 - val_acc: 0.9644 - val_f1: 0.9648\n",
      "Epoch 54/60\n",
      "12800/12800 [==============================] - 1s 62us/sample - loss: 0.0829 - acc: 0.9701 - f1: 0.9709 - val_loss: 0.1032 - val_acc: 0.9653 - val_f1: 0.9670\n",
      "Epoch 55/60\n",
      "12800/12800 [==============================] - 1s 60us/sample - loss: 0.0863 - acc: 0.9687 - f1: 0.9695 - val_loss: 0.1020 - val_acc: 0.9669 - val_f1: 0.9670\n",
      "Epoch 56/60\n",
      "12800/12800 [==============================] - 1s 60us/sample - loss: 0.0935 - acc: 0.9667 - f1: 0.9679 - val_loss: 0.1126 - val_acc: 0.9591 - val_f1: 0.9602\n",
      "Epoch 57/60\n",
      "12800/12800 [==============================] - 1s 60us/sample - loss: 0.0841 - acc: 0.9698 - f1: 0.9708 - val_loss: 0.1078 - val_acc: 0.9628 - val_f1: 0.9640\n",
      "Epoch 58/60\n",
      "12800/12800 [==============================] - 1s 63us/sample - loss: 0.0819 - acc: 0.9696 - f1: 0.9705 - val_loss: 0.1186 - val_acc: 0.9575 - val_f1: 0.9601\n",
      "Epoch 59/60\n",
      "12800/12800 [==============================] - 1s 65us/sample - loss: 0.0845 - acc: 0.9701 - f1: 0.9711 - val_loss: 0.1198 - val_acc: 0.9584 - val_f1: 0.9606\n",
      "Epoch 60/60\n",
      "12800/12800 [==============================] - 1s 64us/sample - loss: 0.0809 - acc: 0.9711 - f1: 0.9718 - val_loss: 0.1057 - val_acc: 0.9634 - val_f1: 0.9642\n"
     ]
    }
   ],
   "source": [
    "training_delay_RN = []\n",
    "predicting_delay_RN = []\n",
    "history_obj = []\n",
    "cpt = 0\n",
    "best_accuracy_RN = 0\n",
    "\n",
    "nb_perceptrons_range = [[5, 4, 4],[100, 100, 2],[500, 500, 500]]                                                                                                                      \n",
    "\n",
    "for nb_perceptrons in nb_perceptrons_range:                                                                                                                                                  \n",
    "    model = RN_model(nb_perceptrons, dropout, learning_rate)                                                                                                                              \n",
    "    #### Apprentissage                                                                                                                                                             \n",
    "    start = time.time()                                                                                                                                                            \n",
    "    hist_obj = model.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs, validation_data=(X_test, Y_test))                                                             \n",
    "\n",
    "    end = time.time()                                                                                                                                                              \n",
    "    training_delay_RN.append(end - start)                                                                                                                                          \n",
    "\n",
    "    history_obj.append( list(hist_obj.history.values()))\n",
    "\n",
    "    #### Prédiction                                                                                                                                                                \n",
    "    start = time.time()                                                                                                                                                            \n",
    "\n",
    "    Y_pred = np.where(model.predict(X_test) > 0.5, 1, 0)                                                                                                                           \n",
    "\n",
    "    end = time.time()                                                                                                                                                              \n",
    "    predicting_delay_RN.append(end - start)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mise en forme des données pour l'affichage\n",
    "ho = np.array(history_obj)\n",
    "ho = ho.transpose(1,2,0)\n",
    "\n",
    "leg = [str(i) for i in nb_perceptrons_range]                                                                                                                                          \n",
    "\n",
    "titre = \"RN : HyperParam = layer size\"   \n",
    "\n",
    "plot_perf(ho, leg, titre ,sub_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "Nous remarquons qu'avec trois couches, les performances d'accuracy et de f1_score sont meilleurs dans le cas de [500,500,500] (dépassant les 95%). On remarque également que la perte (\"loss\") est, en quelque sorte, inversement proportionnelle aux f1_score et accuracy dans ce cas. C'est à dire que pour un f1_score et une accuracy plus faible (nombre de perceptrons inférieur) la valeur de perte sera plus importante que les architectures avec plus de perceptrons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_delay(training_delay_RN,predicting_delay_RN,titre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "\n",
    "En revanche, ajouter un nombre de perceptrons par couche important influence grandement le temps d'entrainement et de prediction.\n",
    "\n",
    "\n",
    "1 - b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/60\n",
      "12800/12800 [==============================] - 1s 49us/sample - loss: 0.6234 - acc: 0.6728 - f1: 0.7001 - val_loss: 0.4902 - val_acc: 0.8391 - val_f1: 0.8390\n",
      "Epoch 2/60\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.4013 - acc: 0.8518 - f1: 0.8558 - val_loss: 0.3231 - val_acc: 0.8778 - val_f1: 0.8792\n",
      "Epoch 3/60\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3019 - acc: 0.8876 - f1: 0.8905 - val_loss: 0.2578 - val_acc: 0.9103 - val_f1: 0.9156\n",
      "Epoch 4/60\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.2532 - acc: 0.9087 - f1: 0.9110 - val_loss: 0.2169 - val_acc: 0.9237 - val_f1: 0.9281\n",
      "Epoch 5/60\n",
      "12800/12800 [==============================] - 0s 22us/sample - loss: 0.2171 - acc: 0.9202 - f1: 0.9224 - val_loss: 0.1956 - val_acc: 0.9216 - val_f1: 0.9213\n",
      "Epoch 6/60\n",
      "12800/12800 [==============================] - 0s 18us/sample - loss: 0.1957 - acc: 0.9275 - f1: 0.9298 - val_loss: 0.1773 - val_acc: 0.9388 - val_f1: 0.9418\n",
      "Epoch 7/60\n",
      "12800/12800 [==============================] - 0s 18us/sample - loss: 0.1810 - acc: 0.9345 - f1: 0.9364 - val_loss: 0.1783 - val_acc: 0.9272 - val_f1: 0.9266\n",
      "Epoch 8/60\n",
      "12800/12800 [==============================] - 0s 17us/sample - loss: 0.1706 - acc: 0.9395 - f1: 0.9410 - val_loss: 0.1547 - val_acc: 0.9416 - val_f1: 0.9422\n",
      "Epoch 9/60\n",
      "12800/12800 [==============================] - 0s 17us/sample - loss: 0.1645 - acc: 0.9408 - f1: 0.9427 - val_loss: 0.1581 - val_acc: 0.9434 - val_f1: 0.9463\n",
      "Epoch 10/60\n",
      "12800/12800 [==============================] - 0s 18us/sample - loss: 0.1559 - acc: 0.9436 - f1: 0.9453 - val_loss: 0.1585 - val_acc: 0.9362 - val_f1: 0.9358\n",
      "Epoch 11/60\n",
      "12800/12800 [==============================] - 0s 17us/sample - loss: 0.1528 - acc: 0.9459 - f1: 0.9476 - val_loss: 0.1456 - val_acc: 0.9469 - val_f1: 0.9493\n",
      "Epoch 12/60\n",
      "12800/12800 [==============================] - 0s 17us/sample - loss: 0.1509 - acc: 0.9451 - f1: 0.9469 - val_loss: 0.1367 - val_acc: 0.9481 - val_f1: 0.9495\n",
      "Epoch 13/60\n",
      "12800/12800 [==============================] - 0s 18us/sample - loss: 0.1480 - acc: 0.9469 - f1: 0.9482 - val_loss: 0.1334 - val_acc: 0.9481 - val_f1: 0.9495\n",
      "Epoch 14/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.1481 - acc: 0.9454 - f1: 0.9472 - val_loss: 0.1313 - val_acc: 0.9491 - val_f1: 0.9512\n",
      "Epoch 15/60\n",
      "12800/12800 [==============================] - 0s 25us/sample - loss: 0.1428 - acc: 0.9469 - f1: 0.9484 - val_loss: 0.1288 - val_acc: 0.9516 - val_f1: 0.9531\n",
      "Epoch 16/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.1376 - acc: 0.9516 - f1: 0.9530 - val_loss: 0.1295 - val_acc: 0.9506 - val_f1: 0.9532\n",
      "Epoch 17/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.1363 - acc: 0.9505 - f1: 0.9517 - val_loss: 0.1278 - val_acc: 0.9522 - val_f1: 0.9540\n",
      "Epoch 18/60\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.1359 - acc: 0.9493 - f1: 0.9510 - val_loss: 0.1307 - val_acc: 0.9475 - val_f1: 0.9486\n",
      "Epoch 19/60\n",
      "12800/12800 [==============================] - 0s 17us/sample - loss: 0.1336 - acc: 0.9515 - f1: 0.9530 - val_loss: 0.1305 - val_acc: 0.9509 - val_f1: 0.9536\n",
      "Epoch 20/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.1319 - acc: 0.9515 - f1: 0.9525 - val_loss: 0.1223 - val_acc: 0.9522 - val_f1: 0.9525\n",
      "Epoch 21/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.1310 - acc: 0.9516 - f1: 0.9531 - val_loss: 0.1375 - val_acc: 0.9513 - val_f1: 0.9542\n",
      "Epoch 22/60\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.1297 - acc: 0.9539 - f1: 0.9555 - val_loss: 0.1260 - val_acc: 0.9531 - val_f1: 0.9555\n",
      "Epoch 23/60\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.1335 - acc: 0.9495 - f1: 0.9506 - val_loss: 0.1233 - val_acc: 0.9547 - val_f1: 0.9569\n",
      "Epoch 24/60\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.1295 - acc: 0.9539 - f1: 0.9551 - val_loss: 0.1223 - val_acc: 0.9538 - val_f1: 0.9545\n",
      "Epoch 25/60\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.1276 - acc: 0.9527 - f1: 0.9538 - val_loss: 0.1170 - val_acc: 0.9569 - val_f1: 0.9575\n",
      "Epoch 26/60\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.1264 - acc: 0.9541 - f1: 0.9553 - val_loss: 0.1200 - val_acc: 0.9559 - val_f1: 0.9572\n",
      "Epoch 27/60\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.1255 - acc: 0.9560 - f1: 0.9575 - val_loss: 0.1151 - val_acc: 0.9578 - val_f1: 0.9596\n",
      "Epoch 28/60\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.1202 - acc: 0.9562 - f1: 0.9576 - val_loss: 0.1196 - val_acc: 0.9559 - val_f1: 0.9576\n",
      "Epoch 29/60\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.1238 - acc: 0.9557 - f1: 0.9568 - val_loss: 0.1156 - val_acc: 0.9556 - val_f1: 0.9575\n",
      "Epoch 30/60\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.1200 - acc: 0.9572 - f1: 0.9588 - val_loss: 0.1179 - val_acc: 0.9566 - val_f1: 0.9572\n",
      "Epoch 31/60\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.1213 - acc: 0.9566 - f1: 0.9578 - val_loss: 0.1132 - val_acc: 0.9566 - val_f1: 0.9577\n",
      "Epoch 32/60\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.1241 - acc: 0.9556 - f1: 0.9568 - val_loss: 0.1120 - val_acc: 0.9578 - val_f1: 0.9597\n",
      "Epoch 33/60\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.1180 - acc: 0.9590 - f1: 0.9601 - val_loss: 0.1279 - val_acc: 0.9525 - val_f1: 0.9533\n",
      "Epoch 34/60\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.1235 - acc: 0.9530 - f1: 0.9544 - val_loss: 0.1330 - val_acc: 0.9506 - val_f1: 0.9511\n",
      "Epoch 35/60\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.1189 - acc: 0.9563 - f1: 0.9575 - val_loss: 0.1106 - val_acc: 0.9581 - val_f1: 0.9588\n",
      "Epoch 36/60\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.1183 - acc: 0.9584 - f1: 0.9595 - val_loss: 0.1215 - val_acc: 0.9547 - val_f1: 0.9572\n",
      "Epoch 37/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.1163 - acc: 0.9573 - f1: 0.9586 - val_loss: 0.1123 - val_acc: 0.9544 - val_f1: 0.9557\n",
      "Epoch 38/60\n",
      "12800/12800 [==============================] - 0s 13us/sample - loss: 0.1134 - acc: 0.9609 - f1: 0.9620 - val_loss: 0.1103 - val_acc: 0.9588 - val_f1: 0.9610\n",
      "Epoch 39/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.1196 - acc: 0.9584 - f1: 0.9596 - val_loss: 0.1255 - val_acc: 0.9547 - val_f1: 0.9563\n",
      "Epoch 40/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.1139 - acc: 0.9605 - f1: 0.9617 - val_loss: 0.1067 - val_acc: 0.9606 - val_f1: 0.9618\n",
      "Epoch 41/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.1134 - acc: 0.9583 - f1: 0.9599 - val_loss: 0.1143 - val_acc: 0.9594 - val_f1: 0.9596\n",
      "Epoch 42/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.1177 - acc: 0.9584 - f1: 0.9594 - val_loss: 0.1065 - val_acc: 0.9603 - val_f1: 0.9619\n",
      "Epoch 43/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.1114 - acc: 0.9594 - f1: 0.9607 - val_loss: 0.1097 - val_acc: 0.9600 - val_f1: 0.9616\n",
      "Epoch 44/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.1135 - acc: 0.9589 - f1: 0.9603 - val_loss: 0.1110 - val_acc: 0.9597 - val_f1: 0.9610\n",
      "Epoch 45/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.1124 - acc: 0.9577 - f1: 0.9588 - val_loss: 0.1184 - val_acc: 0.9584 - val_f1: 0.9608\n",
      "Epoch 46/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.1154 - acc: 0.9588 - f1: 0.9597 - val_loss: 0.1300 - val_acc: 0.9544 - val_f1: 0.9574\n",
      "Epoch 47/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.1132 - acc: 0.9591 - f1: 0.9604 - val_loss: 0.1093 - val_acc: 0.9616 - val_f1: 0.9631\n",
      "Epoch 48/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.1101 - acc: 0.9605 - f1: 0.9616 - val_loss: 0.1112 - val_acc: 0.9603 - val_f1: 0.9620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/60\n",
      "12800/12800 [==============================] - 0s 13us/sample - loss: 0.1091 - acc: 0.9613 - f1: 0.9624 - val_loss: 0.1123 - val_acc: 0.9544 - val_f1: 0.9546\n",
      "Epoch 50/60\n",
      "12800/12800 [==============================] - 0s 13us/sample - loss: 0.1133 - acc: 0.9581 - f1: 0.9597 - val_loss: 0.1062 - val_acc: 0.9591 - val_f1: 0.9603\n",
      "Epoch 51/60\n",
      "12800/12800 [==============================] - 0s 13us/sample - loss: 0.1111 - acc: 0.9606 - f1: 0.9618 - val_loss: 0.1046 - val_acc: 0.9600 - val_f1: 0.9608\n",
      "Epoch 52/60\n",
      "12800/12800 [==============================] - 0s 13us/sample - loss: 0.1069 - acc: 0.9612 - f1: 0.9626 - val_loss: 0.1056 - val_acc: 0.9622 - val_f1: 0.9633\n",
      "Epoch 53/60\n",
      "12800/12800 [==============================] - 0s 13us/sample - loss: 0.1068 - acc: 0.9614 - f1: 0.9627 - val_loss: 0.1040 - val_acc: 0.9603 - val_f1: 0.9623\n",
      "Epoch 54/60\n",
      "12800/12800 [==============================] - 0s 14us/sample - loss: 0.1098 - acc: 0.9605 - f1: 0.9616 - val_loss: 0.1028 - val_acc: 0.9613 - val_f1: 0.9627\n",
      "Epoch 55/60\n",
      "12800/12800 [==============================] - 0s 13us/sample - loss: 0.1049 - acc: 0.9613 - f1: 0.9621 - val_loss: 0.1063 - val_acc: 0.9588 - val_f1: 0.9598\n",
      "Epoch 56/60\n",
      "12800/12800 [==============================] - 0s 13us/sample - loss: 0.1039 - acc: 0.9614 - f1: 0.9625 - val_loss: 0.1081 - val_acc: 0.9600 - val_f1: 0.9612\n",
      "Epoch 57/60\n",
      "12800/12800 [==============================] - 0s 13us/sample - loss: 0.1071 - acc: 0.9618 - f1: 0.9632 - val_loss: 0.1028 - val_acc: 0.9616 - val_f1: 0.9629\n",
      "Epoch 58/60\n",
      "12800/12800 [==============================] - 0s 13us/sample - loss: 0.1044 - acc: 0.9617 - f1: 0.9624 - val_loss: 0.1087 - val_acc: 0.9578 - val_f1: 0.9588\n",
      "Epoch 59/60\n",
      "12800/12800 [==============================] - 0s 13us/sample - loss: 0.1067 - acc: 0.9606 - f1: 0.9618 - val_loss: 0.1027 - val_acc: 0.9638 - val_f1: 0.9654\n",
      "Epoch 60/60\n",
      "12800/12800 [==============================] - 0s 13us/sample - loss: 0.1053 - acc: 0.9626 - f1: 0.9639 - val_loss: 0.1039 - val_acc: 0.9619 - val_f1: 0.9630\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/60\n",
      "12800/12800 [==============================] - 1s 64us/sample - loss: 0.6801 - acc: 0.5645 - f1: 0.5767 - val_loss: 0.6230 - val_acc: 0.7716 - val_f1: 0.7336\n",
      "Epoch 2/60\n",
      "12800/12800 [==============================] - 0s 18us/sample - loss: 0.6025 - acc: 0.7311 - f1: 0.7464 - val_loss: 0.5195 - val_acc: 0.8697 - val_f1: 0.8713\n",
      "Epoch 3/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.5444 - acc: 0.7798 - f1: 0.8062 - val_loss: 0.4548 - val_acc: 0.9016 - val_f1: 0.9060\n",
      "Epoch 4/60\n",
      "12800/12800 [==============================] - 0s 25us/sample - loss: 0.5091 - acc: 0.8049 - f1: 0.8299 - val_loss: 0.4139 - val_acc: 0.9112 - val_f1: 0.9126\n",
      "Epoch 5/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.4869 - acc: 0.8131 - f1: 0.8392 - val_loss: 0.3903 - val_acc: 0.9234 - val_f1: 0.9265\n",
      "Epoch 6/60\n",
      "12800/12800 [==============================] - 0s 24us/sample - loss: 0.4749 - acc: 0.8159 - f1: 0.8413 - val_loss: 0.3837 - val_acc: 0.9266 - val_f1: 0.9318\n",
      "Epoch 7/60\n",
      "12800/12800 [==============================] - 0s 23us/sample - loss: 0.4667 - acc: 0.8217 - f1: 0.8463 - val_loss: 0.3526 - val_acc: 0.9334 - val_f1: 0.9365\n",
      "Epoch 8/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.4616 - acc: 0.8180 - f1: 0.8436 - val_loss: 0.3472 - val_acc: 0.9356 - val_f1: 0.9396\n",
      "Epoch 9/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.4536 - acc: 0.8226 - f1: 0.8479 - val_loss: 0.3377 - val_acc: 0.9300 - val_f1: 0.9309\n",
      "Epoch 10/60\n",
      "12800/12800 [==============================] - 0s 18us/sample - loss: 0.4442 - acc: 0.8255 - f1: 0.8502 - val_loss: 0.3254 - val_acc: 0.9366 - val_f1: 0.9388\n",
      "Epoch 11/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.4350 - acc: 0.8279 - f1: 0.8522 - val_loss: 0.3167 - val_acc: 0.9375 - val_f1: 0.9389\n",
      "Epoch 12/60\n",
      "12800/12800 [==============================] - 0s 21us/sample - loss: 0.4381 - acc: 0.8241 - f1: 0.8495 - val_loss: 0.3120 - val_acc: 0.9388 - val_f1: 0.9423\n",
      "Epoch 13/60\n",
      "12800/12800 [==============================] - 0s 22us/sample - loss: 0.4259 - acc: 0.8315 - f1: 0.8556 - val_loss: 0.2967 - val_acc: 0.9444 - val_f1: 0.9466\n",
      "Epoch 14/60\n",
      "12800/12800 [==============================] - 0s 18us/sample - loss: 0.4224 - acc: 0.8289 - f1: 0.8534 - val_loss: 0.3143 - val_acc: 0.9316 - val_f1: 0.9368\n",
      "Epoch 15/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.4162 - acc: 0.8330 - f1: 0.8569 - val_loss: 0.2900 - val_acc: 0.9419 - val_f1: 0.9454\n",
      "Epoch 16/60\n",
      "12800/12800 [==============================] - 0s 18us/sample - loss: 0.4216 - acc: 0.8295 - f1: 0.8543 - val_loss: 0.2830 - val_acc: 0.9469 - val_f1: 0.9500\n",
      "Epoch 17/60\n",
      "12800/12800 [==============================] - 0s 21us/sample - loss: 0.4098 - acc: 0.8355 - f1: 0.8583 - val_loss: 0.2831 - val_acc: 0.9428 - val_f1: 0.9462\n",
      "Epoch 18/60\n",
      "12800/12800 [==============================] - 0s 22us/sample - loss: 0.4078 - acc: 0.8370 - f1: 0.8594 - val_loss: 0.2722 - val_acc: 0.9450 - val_f1: 0.9474\n",
      "Epoch 19/60\n",
      "12800/12800 [==============================] - 0s 22us/sample - loss: 0.3982 - acc: 0.8401 - f1: 0.8621 - val_loss: 0.2714 - val_acc: 0.9431 - val_f1: 0.9469\n",
      "Epoch 20/60\n",
      "12800/12800 [==============================] - 0s 20us/sample - loss: 0.4080 - acc: 0.8335 - f1: 0.8569 - val_loss: 0.3117 - val_acc: 0.9228 - val_f1: 0.9293\n",
      "Epoch 21/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.3958 - acc: 0.8415 - f1: 0.8629 - val_loss: 0.2535 - val_acc: 0.9488 - val_f1: 0.9509\n",
      "Epoch 22/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.3858 - acc: 0.8458 - f1: 0.8665 - val_loss: 0.2677 - val_acc: 0.9378 - val_f1: 0.9417\n",
      "Epoch 23/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.3888 - acc: 0.8443 - f1: 0.8655 - val_loss: 0.2516 - val_acc: 0.9500 - val_f1: 0.9523\n",
      "Epoch 24/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.3944 - acc: 0.8398 - f1: 0.8617 - val_loss: 0.2618 - val_acc: 0.9425 - val_f1: 0.9428\n",
      "Epoch 25/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.3952 - acc: 0.8377 - f1: 0.8599 - val_loss: 0.2476 - val_acc: 0.9441 - val_f1: 0.9447\n",
      "Epoch 26/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.3888 - acc: 0.8413 - f1: 0.8631 - val_loss: 0.2349 - val_acc: 0.9538 - val_f1: 0.9556\n",
      "Epoch 27/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.3965 - acc: 0.8373 - f1: 0.8596 - val_loss: 0.2330 - val_acc: 0.9525 - val_f1: 0.9533\n",
      "Epoch 28/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.3915 - acc: 0.8400 - f1: 0.8622 - val_loss: 0.2321 - val_acc: 0.9519 - val_f1: 0.9544\n",
      "Epoch 29/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.3874 - acc: 0.8416 - f1: 0.8633 - val_loss: 0.2295 - val_acc: 0.9519 - val_f1: 0.9539\n",
      "Epoch 30/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.3945 - acc: 0.8377 - f1: 0.8610 - val_loss: 0.2557 - val_acc: 0.9428 - val_f1: 0.9472\n",
      "Epoch 31/60\n",
      "12800/12800 [==============================] - 0s 18us/sample - loss: 0.3776 - acc: 0.8489 - f1: 0.8689 - val_loss: 0.2238 - val_acc: 0.9534 - val_f1: 0.9550\n",
      "Epoch 32/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.3847 - acc: 0.8454 - f1: 0.8664 - val_loss: 0.2259 - val_acc: 0.9528 - val_f1: 0.9547\n",
      "Epoch 33/60\n",
      "12800/12800 [==============================] - 0s 18us/sample - loss: 0.3895 - acc: 0.8402 - f1: 0.8624 - val_loss: 0.2205 - val_acc: 0.9547 - val_f1: 0.9564\n",
      "Epoch 34/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.3820 - acc: 0.8459 - f1: 0.8669 - val_loss: 0.2347 - val_acc: 0.9497 - val_f1: 0.9520\n",
      "Epoch 35/60\n",
      "12800/12800 [==============================] - 0s 18us/sample - loss: 0.3834 - acc: 0.8430 - f1: 0.8647 - val_loss: 0.2222 - val_acc: 0.9519 - val_f1: 0.9542\n",
      "Epoch 36/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.3833 - acc: 0.8428 - f1: 0.8642 - val_loss: 0.2527 - val_acc: 0.9381 - val_f1: 0.9426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/60\n",
      "12800/12800 [==============================] - 0s 18us/sample - loss: 0.3859 - acc: 0.8422 - f1: 0.8645 - val_loss: 0.2379 - val_acc: 0.9447 - val_f1: 0.9482\n",
      "Epoch 38/60\n",
      "12800/12800 [==============================] - 0s 18us/sample - loss: 0.3869 - acc: 0.8382 - f1: 0.8610 - val_loss: 0.2206 - val_acc: 0.9522 - val_f1: 0.9544\n",
      "Epoch 39/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.3839 - acc: 0.8444 - f1: 0.8659 - val_loss: 0.2275 - val_acc: 0.9513 - val_f1: 0.9524\n",
      "Epoch 40/60\n",
      "12800/12800 [==============================] - 0s 18us/sample - loss: 0.3855 - acc: 0.8418 - f1: 0.8640 - val_loss: 0.2237 - val_acc: 0.9519 - val_f1: 0.9525\n",
      "Epoch 41/60\n",
      "12800/12800 [==============================] - 0s 18us/sample - loss: 0.3874 - acc: 0.8393 - f1: 0.8621 - val_loss: 0.2166 - val_acc: 0.9578 - val_f1: 0.9590\n",
      "Epoch 42/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.3864 - acc: 0.8430 - f1: 0.8650 - val_loss: 0.2208 - val_acc: 0.9528 - val_f1: 0.9552\n",
      "Epoch 43/60\n",
      "12800/12800 [==============================] - 0s 20us/sample - loss: 0.3811 - acc: 0.8460 - f1: 0.8679 - val_loss: 0.2227 - val_acc: 0.9531 - val_f1: 0.9536\n",
      "Epoch 44/60\n",
      "12800/12800 [==============================] - 0s 23us/sample - loss: 0.3765 - acc: 0.8474 - f1: 0.8686 - val_loss: 0.2209 - val_acc: 0.9547 - val_f1: 0.9568\n",
      "Epoch 45/60\n",
      "12800/12800 [==============================] - 0s 21us/sample - loss: 0.3735 - acc: 0.8479 - f1: 0.8683 - val_loss: 0.2162 - val_acc: 0.9541 - val_f1: 0.9564\n",
      "Epoch 46/60\n",
      "12800/12800 [==============================] - 0s 22us/sample - loss: 0.3795 - acc: 0.8465 - f1: 0.8677 - val_loss: 0.2231 - val_acc: 0.9538 - val_f1: 0.9562\n",
      "Epoch 47/60\n",
      "12800/12800 [==============================] - 0s 23us/sample - loss: 0.3782 - acc: 0.8452 - f1: 0.8664 - val_loss: 0.2186 - val_acc: 0.9550 - val_f1: 0.9572\n",
      "Epoch 48/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.3787 - acc: 0.8445 - f1: 0.8658 - val_loss: 0.2172 - val_acc: 0.9563 - val_f1: 0.9585\n",
      "Epoch 49/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.3840 - acc: 0.8435 - f1: 0.8646 - val_loss: 0.2316 - val_acc: 0.9491 - val_f1: 0.9520\n",
      "Epoch 50/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.3751 - acc: 0.8493 - f1: 0.8700 - val_loss: 0.2173 - val_acc: 0.9563 - val_f1: 0.9586\n",
      "Epoch 51/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.3802 - acc: 0.8452 - f1: 0.8668 - val_loss: 0.2134 - val_acc: 0.9569 - val_f1: 0.9581\n",
      "Epoch 52/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.3753 - acc: 0.8469 - f1: 0.8679 - val_loss: 0.2177 - val_acc: 0.9544 - val_f1: 0.9566\n",
      "Epoch 53/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.3747 - acc: 0.8502 - f1: 0.8702 - val_loss: 0.2154 - val_acc: 0.9559 - val_f1: 0.9586\n",
      "Epoch 54/60\n",
      "12800/12800 [==============================] - 0s 18us/sample - loss: 0.3749 - acc: 0.8476 - f1: 0.8691 - val_loss: 0.2174 - val_acc: 0.9556 - val_f1: 0.9575\n",
      "Epoch 55/60\n",
      "12800/12800 [==============================] - 0s 20us/sample - loss: 0.3827 - acc: 0.8439 - f1: 0.8658 - val_loss: 0.2259 - val_acc: 0.9519 - val_f1: 0.9542\n",
      "Epoch 56/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.3802 - acc: 0.8451 - f1: 0.8666 - val_loss: 0.2192 - val_acc: 0.9544 - val_f1: 0.9568\n",
      "Epoch 57/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.3778 - acc: 0.8461 - f1: 0.8676 - val_loss: 0.2162 - val_acc: 0.9541 - val_f1: 0.9562\n",
      "Epoch 58/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.3792 - acc: 0.8445 - f1: 0.8661 - val_loss: 0.2139 - val_acc: 0.9566 - val_f1: 0.9586\n",
      "Epoch 59/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.3843 - acc: 0.8445 - f1: 0.8662 - val_loss: 0.2479 - val_acc: 0.9394 - val_f1: 0.9437\n",
      "Epoch 60/60\n",
      "12800/12800 [==============================] - 0s 19us/sample - loss: 0.3761 - acc: 0.8471 - f1: 0.8683 - val_loss: 0.2140 - val_acc: 0.9584 - val_f1: 0.9602\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/60\n",
      "12800/12800 [==============================] - 1s 91us/sample - loss: 0.6961 - acc: 0.5202 - f1: 0.6512 - val_loss: 0.6928 - val_acc: 0.5191 - val_f1: 0.6821\n",
      "Epoch 2/60\n",
      "12800/12800 [==============================] - 0s 28us/sample - loss: 0.6929 - acc: 0.5204 - f1: 0.6772 - val_loss: 0.6926 - val_acc: 0.5191 - val_f1: 0.6821\n",
      "Epoch 3/60\n",
      "12800/12800 [==============================] - 0s 28us/sample - loss: 0.6927 - acc: 0.5202 - f1: 0.6792 - val_loss: 0.6925 - val_acc: 0.5191 - val_f1: 0.6816\n",
      "Epoch 4/60\n",
      "12800/12800 [==============================] - 0s 29us/sample - loss: 0.6925 - acc: 0.5197 - f1: 0.6808 - val_loss: 0.6925 - val_acc: 0.5191 - val_f1: 0.6819\n",
      "Epoch 5/60\n",
      "12800/12800 [==============================] - 0s 28us/sample - loss: 0.6923 - acc: 0.5202 - f1: 0.6810 - val_loss: 0.6924 - val_acc: 0.5191 - val_f1: 0.6818\n",
      "Epoch 6/60\n",
      "12800/12800 [==============================] - 0s 28us/sample - loss: 0.6920 - acc: 0.5223 - f1: 0.6774 - val_loss: 0.6924 - val_acc: 0.5191 - val_f1: 0.6825\n",
      "Epoch 7/60\n",
      "12800/12800 [==============================] - 0s 29us/sample - loss: 0.6917 - acc: 0.5276 - f1: 0.6780 - val_loss: 0.6924 - val_acc: 0.5191 - val_f1: 0.6817\n",
      "Epoch 8/60\n",
      "12800/12800 [==============================] - 0s 29us/sample - loss: 0.6526 - acc: 0.6195 - f1: 0.7059 - val_loss: 0.5656 - val_acc: 0.8644 - val_f1: 0.8783\n",
      "Epoch 9/60\n",
      "12800/12800 [==============================] - 0s 29us/sample - loss: 0.5973 - acc: 0.6858 - f1: 0.7527 - val_loss: 0.4498 - val_acc: 0.9053 - val_f1: 0.9084\n",
      "Epoch 10/60\n",
      "12800/12800 [==============================] - 0s 28us/sample - loss: 0.5744 - acc: 0.7038 - f1: 0.7688 - val_loss: 0.4466 - val_acc: 0.9125 - val_f1: 0.9198\n",
      "Epoch 11/60\n",
      "12800/12800 [==============================] - 0s 29us/sample - loss: 0.5622 - acc: 0.7113 - f1: 0.7743 - val_loss: 0.4367 - val_acc: 0.9209 - val_f1: 0.9271\n",
      "Epoch 12/60\n",
      "12800/12800 [==============================] - 0s 28us/sample - loss: 0.5568 - acc: 0.7101 - f1: 0.7745 - val_loss: 0.3757 - val_acc: 0.9222 - val_f1: 0.9245\n",
      "Epoch 13/60\n",
      "12800/12800 [==============================] - 0s 29us/sample - loss: 0.5432 - acc: 0.7222 - f1: 0.7820 - val_loss: 0.4654 - val_acc: 0.8719 - val_f1: 0.8889\n",
      "Epoch 14/60\n",
      "12800/12800 [==============================] - 0s 29us/sample - loss: 0.5436 - acc: 0.7184 - f1: 0.7803 - val_loss: 0.3672 - val_acc: 0.9347 - val_f1: 0.9379\n",
      "Epoch 15/60\n",
      "12800/12800 [==============================] - 0s 29us/sample - loss: 0.5423 - acc: 0.7169 - f1: 0.7799 - val_loss: 0.3754 - val_acc: 0.9284 - val_f1: 0.9329\n",
      "Epoch 16/60\n",
      "12800/12800 [==============================] - 0s 28us/sample - loss: 0.5388 - acc: 0.7247 - f1: 0.7843 - val_loss: 0.3733 - val_acc: 0.9316 - val_f1: 0.9357\n",
      "Epoch 17/60\n",
      "12800/12800 [==============================] - 0s 29us/sample - loss: 0.5358 - acc: 0.7231 - f1: 0.7841 - val_loss: 0.3820 - val_acc: 0.9284 - val_f1: 0.9332\n",
      "Epoch 18/60\n",
      "12800/12800 [==============================] - 0s 29us/sample - loss: 0.5310 - acc: 0.7242 - f1: 0.7846 - val_loss: 0.3489 - val_acc: 0.9388 - val_f1: 0.9411\n",
      "Epoch 19/60\n",
      "12800/12800 [==============================] - 0s 28us/sample - loss: 0.5364 - acc: 0.7214 - f1: 0.7826 - val_loss: 0.4229 - val_acc: 0.8988 - val_f1: 0.9102\n",
      "Epoch 20/60\n",
      "12800/12800 [==============================] - 0s 28us/sample - loss: 0.5332 - acc: 0.7250 - f1: 0.7848 - val_loss: 0.3517 - val_acc: 0.9375 - val_f1: 0.9410\n",
      "Epoch 21/60\n",
      "12800/12800 [==============================] - 0s 28us/sample - loss: 0.5332 - acc: 0.7227 - f1: 0.7839 - val_loss: 0.3972 - val_acc: 0.9062 - val_f1: 0.9160\n",
      "Epoch 22/60\n",
      "12800/12800 [==============================] - 0s 28us/sample - loss: 0.5318 - acc: 0.7241 - f1: 0.7853 - val_loss: 0.3486 - val_acc: 0.9362 - val_f1: 0.9398\n",
      "Epoch 23/60\n",
      "12800/12800 [==============================] - 0s 29us/sample - loss: 0.5279 - acc: 0.7265 - f1: 0.7861 - val_loss: 0.3342 - val_acc: 0.9409 - val_f1: 0.9443\n",
      "Epoch 24/60\n",
      "12800/12800 [==============================] - 0s 28us/sample - loss: 0.5307 - acc: 0.7259 - f1: 0.7854 - val_loss: 0.3438 - val_acc: 0.9375 - val_f1: 0.9406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/60\n",
      "12800/12800 [==============================] - 0s 28us/sample - loss: 0.5287 - acc: 0.7259 - f1: 0.7857 - val_loss: 0.3444 - val_acc: 0.9372 - val_f1: 0.9408\n",
      "Epoch 26/60\n",
      "12800/12800 [==============================] - 0s 27us/sample - loss: 0.5243 - acc: 0.7303 - f1: 0.7884 - val_loss: 0.3826 - val_acc: 0.9259 - val_f1: 0.9325\n",
      "Epoch 27/60\n",
      "12800/12800 [==============================] - 0s 27us/sample - loss: 0.5258 - acc: 0.7291 - f1: 0.7882 - val_loss: 0.3278 - val_acc: 0.9425 - val_f1: 0.9454\n",
      "Epoch 28/60\n",
      "12800/12800 [==============================] - 0s 27us/sample - loss: 0.5251 - acc: 0.7270 - f1: 0.7870 - val_loss: 0.3137 - val_acc: 0.9438 - val_f1: 0.9465\n",
      "Epoch 29/60\n",
      "12800/12800 [==============================] - 0s 28us/sample - loss: 0.5326 - acc: 0.7207 - f1: 0.7824 - val_loss: 0.3718 - val_acc: 0.9281 - val_f1: 0.9347\n",
      "Epoch 30/60\n",
      "12800/12800 [==============================] - 0s 26us/sample - loss: 0.5188 - acc: 0.7345 - f1: 0.7916 - val_loss: 0.3437 - val_acc: 0.9416 - val_f1: 0.9448\n",
      "Epoch 31/60\n",
      "12800/12800 [==============================] - 0s 28us/sample - loss: 0.5152 - acc: 0.7361 - f1: 0.7924 - val_loss: 0.3122 - val_acc: 0.9466 - val_f1: 0.9484\n",
      "Epoch 32/60\n",
      "12800/12800 [==============================] - 0s 27us/sample - loss: 0.5231 - acc: 0.7328 - f1: 0.7905 - val_loss: 0.3597 - val_acc: 0.9344 - val_f1: 0.9395\n",
      "Epoch 33/60\n",
      "12800/12800 [==============================] - 0s 27us/sample - loss: 0.5217 - acc: 0.7312 - f1: 0.7896 - val_loss: 0.3127 - val_acc: 0.9466 - val_f1: 0.9480\n",
      "Epoch 34/60\n",
      "12800/12800 [==============================] - 0s 27us/sample - loss: 0.5188 - acc: 0.7318 - f1: 0.7903 - val_loss: 0.3580 - val_acc: 0.9350 - val_f1: 0.9402\n",
      "Epoch 35/60\n",
      "12800/12800 [==============================] - 0s 28us/sample - loss: 0.5208 - acc: 0.7341 - f1: 0.7914 - val_loss: 0.3215 - val_acc: 0.9500 - val_f1: 0.9516\n",
      "Epoch 36/60\n",
      "12800/12800 [==============================] - 0s 27us/sample - loss: 0.5207 - acc: 0.7300 - f1: 0.7884 - val_loss: 0.3114 - val_acc: 0.9456 - val_f1: 0.9476\n",
      "Epoch 37/60\n",
      "12800/12800 [==============================] - 0s 28us/sample - loss: 0.5158 - acc: 0.7344 - f1: 0.7923 - val_loss: 0.3061 - val_acc: 0.9506 - val_f1: 0.9524\n",
      "Epoch 38/60\n",
      "12800/12800 [==============================] - 0s 28us/sample - loss: 0.5251 - acc: 0.7255 - f1: 0.7856 - val_loss: 0.3468 - val_acc: 0.9434 - val_f1: 0.9472\n",
      "Epoch 39/60\n",
      "12800/12800 [==============================] - 0s 28us/sample - loss: 0.5167 - acc: 0.7350 - f1: 0.7922 - val_loss: 0.3601 - val_acc: 0.9291 - val_f1: 0.9344\n",
      "Epoch 40/60\n",
      "12800/12800 [==============================] - 0s 30us/sample - loss: 0.5178 - acc: 0.7329 - f1: 0.7911 - val_loss: 0.3414 - val_acc: 0.9444 - val_f1: 0.9481\n",
      "Epoch 41/60\n",
      "12800/12800 [==============================] - 0s 31us/sample - loss: 0.5153 - acc: 0.7352 - f1: 0.7923 - val_loss: 0.3719 - val_acc: 0.9200 - val_f1: 0.9272\n",
      "Epoch 42/60\n",
      "12800/12800 [==============================] - 0s 29us/sample - loss: 0.5203 - acc: 0.7327 - f1: 0.7907 - val_loss: 0.3036 - val_acc: 0.9509 - val_f1: 0.9517\n",
      "Epoch 43/60\n",
      "12800/12800 [==============================] - 0s 30us/sample - loss: 0.5226 - acc: 0.7305 - f1: 0.7884 - val_loss: 0.3444 - val_acc: 0.9447 - val_f1: 0.9487\n",
      "Epoch 44/60\n",
      "12800/12800 [==============================] - 0s 28us/sample - loss: 0.5194 - acc: 0.7323 - f1: 0.7900 - val_loss: 0.3271 - val_acc: 0.9488 - val_f1: 0.9503\n",
      "Epoch 45/60\n",
      "12800/12800 [==============================] - 0s 31us/sample - loss: 0.5161 - acc: 0.7360 - f1: 0.7927 - val_loss: 0.3309 - val_acc: 0.9400 - val_f1: 0.9436\n",
      "Epoch 46/60\n",
      "12800/12800 [==============================] - 0s 32us/sample - loss: 0.5098 - acc: 0.7410 - f1: 0.7957 - val_loss: 0.3329 - val_acc: 0.9472 - val_f1: 0.9505\n",
      "Epoch 47/60\n",
      "12800/12800 [==============================] - 0s 28us/sample - loss: 0.5182 - acc: 0.7362 - f1: 0.7928 - val_loss: 0.3071 - val_acc: 0.9522 - val_f1: 0.9539\n",
      "Epoch 48/60\n",
      "12800/12800 [==============================] - 0s 29us/sample - loss: 0.5236 - acc: 0.7279 - f1: 0.7874 - val_loss: 0.3856 - val_acc: 0.9275 - val_f1: 0.9336\n",
      "Epoch 49/60\n",
      "12800/12800 [==============================] - 0s 35us/sample - loss: 0.5135 - acc: 0.7355 - f1: 0.7919 - val_loss: 0.3201 - val_acc: 0.9509 - val_f1: 0.9533\n",
      "Epoch 50/60\n",
      "12800/12800 [==============================] - 0s 35us/sample - loss: 0.5246 - acc: 0.7248 - f1: 0.7861 - val_loss: 0.3415 - val_acc: 0.9431 - val_f1: 0.9471\n",
      "Epoch 51/60\n",
      "12800/12800 [==============================] - 0s 34us/sample - loss: 0.5186 - acc: 0.7323 - f1: 0.7905 - val_loss: 0.4025 - val_acc: 0.9103 - val_f1: 0.9200\n",
      "Epoch 52/60\n",
      "12800/12800 [==============================] - 0s 33us/sample - loss: 0.5182 - acc: 0.7334 - f1: 0.7911 - val_loss: 0.3630 - val_acc: 0.9447 - val_f1: 0.9486\n",
      "Epoch 53/60\n",
      "12800/12800 [==============================] - 0s 35us/sample - loss: 0.5163 - acc: 0.7343 - f1: 0.7916 - val_loss: 0.3052 - val_acc: 0.9209 - val_f1: 0.9186\n",
      "Epoch 54/60\n",
      "12800/12800 [==============================] - 0s 37us/sample - loss: 0.5204 - acc: 0.7319 - f1: 0.7894 - val_loss: 0.3206 - val_acc: 0.9553 - val_f1: 0.9577\n",
      "Epoch 55/60\n",
      "12800/12800 [==============================] - 1s 41us/sample - loss: 0.5172 - acc: 0.7340 - f1: 0.7913 - val_loss: 0.3283 - val_acc: 0.9509 - val_f1: 0.9524\n",
      "Epoch 56/60\n",
      "12800/12800 [==============================] - 1s 48us/sample - loss: 0.5166 - acc: 0.7377 - f1: 0.7938 - val_loss: 0.3338 - val_acc: 0.9522 - val_f1: 0.9543\n",
      "Epoch 57/60\n",
      "12800/12800 [==============================] - 0s 37us/sample - loss: 0.5169 - acc: 0.7345 - f1: 0.7914 - val_loss: 0.4471 - val_acc: 0.8959 - val_f1: 0.9084\n",
      "Epoch 58/60\n",
      "12800/12800 [==============================] - 0s 33us/sample - loss: 0.5112 - acc: 0.7384 - f1: 0.7943 - val_loss: 0.3278 - val_acc: 0.9550 - val_f1: 0.9569\n",
      "Epoch 59/60\n",
      "12800/12800 [==============================] - 0s 35us/sample - loss: 0.5165 - acc: 0.7333 - f1: 0.7918 - val_loss: 0.3223 - val_acc: 0.9509 - val_f1: 0.9535\n",
      "Epoch 60/60\n",
      "12800/12800 [==============================] - 0s 38us/sample - loss: 0.5164 - acc: 0.7330 - f1: 0.7903 - val_loss: 0.3055 - val_acc: 0.9500 - val_f1: 0.9500\n"
     ]
    }
   ],
   "source": [
    "training_delay_RN = []\n",
    "predicting_delay_RN = []\n",
    "history_obj = []\n",
    "cpt = 0\n",
    "best_accuracy_RN = 0\n",
    "\n",
    "layer_sizes_range = [[100],[100, 100, 2],[100, 100, 100, 100, 100, 2]]\n",
    "\n",
    "for layer_s in layer_sizes_range:\n",
    "    model = RN_model(layer_s, dropout, learning_rate)\n",
    "    #### Apprentissage                                                                                                                                                               \n",
    "    start = time.time()                                                                                                                   \n",
    "    hist_obj = model.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs, validation_data=(X_test, Y_test))\n",
    "\n",
    "    end = time.time()\n",
    "    training_delay_RN.append(end - start)\n",
    "\n",
    "    history_obj.append( list(hist_obj.history.values()))\n",
    "\n",
    "    #### Prédiction                                                                                                                                                                  \n",
    "    start = time.time()\n",
    "\n",
    "    Y_pred = np.where(model.predict(X_test) > 0.5, 1, 0)\n",
    "\n",
    "    end = time.time()\n",
    "    predicting_delay_RN.append(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layer_sizes_range' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f13cfc91e535>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mho\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mho\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mleg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_sizes_range\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtitre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"RN : HyperParam = number of layer\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layer_sizes_range' is not defined"
     ]
    }
   ],
   "source": [
    "# Mise en forme des données pour l'affichage\n",
    "ho = np.array(history_obj)\n",
    "ho = ho.transpose(1,2,0)\n",
    "\n",
    "leg = [str(i) for i in layer_sizes_range]                                                                                                                                              \n",
    "\n",
    "titre = \"RN : HyperParam = number of layer\"                                                                                                                                         \n",
    "\n",
    "plot_perf(ho, leg, titre ,sub_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Dans notre problème de classification de galaxies, on note que le les valeurs des accuracy et des f1_scores tendent vers les mêmes performances sur les jeux de données de tests qu'il y ait une, trois ou six couches (nb de perceptrons constant par couche). On remarque également que la valeur de perte est moins importante dans le cas où il y le moins de couche. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_delay(training_delay_RN,predicting_delay_RN,titre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Ici, nous remarquons que : plus le nombre de couche est important plus le temps d'entrainement et de prediction sont élevés.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "training_delay_RN = []\n",
    "predicting_delay_RN = []\n",
    "history_obj = []\n",
    "cpt = 0\n",
    "best_accuracy_RN = 0\n",
    "# Faire 1 test à la fois ou réinitialiser les 3 lists   \n",
    "\n",
    "################################## Nombres de couches cachees                                                                                                                        \n",
    "layer_sizes_range = [[100],[100, 100, 2],[100, 100, 100, 100, 100, 2]]\n",
    "\n",
    "for layer_s in layer_sizes_range:\n",
    "    model = RN_model(layer_s, dropout, learning_rate)\n",
    "    #### Apprentissage                                                                                                                                                               \n",
    "    start = time.time()\n",
    "    #model.fit(X_train, Y_train, batch_size = 100, epochs = 60)                                                                                                                      \n",
    "    hist_obj = model.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs, validation_data=(X_test, Y_test))\n",
    "\n",
    "    end = time.time()\n",
    "    training_delay_RN.append(end - start)\n",
    "\n",
    "    history_obj.append( list(hist_obj.history.values()))\n",
    "\n",
    "    #### Prédiction                                                                                                                                                                  \n",
    "    start = time.time()\n",
    "\n",
    "    Y_pred = np.where(model.predict(X_test) > 0.5, 1, 0)\n",
    "\n",
    "    end = time.time()\n",
    "    predicting_delay_RN.append(end - start)\n",
    "################################## Nombres de perceptrons                                                                                                                            \n",
    "# layer_sizes_range = [[5, 4, 4],[100, 100, 2],[500, 500, 500]]                                                                                                                      \n",
    "#                                                                                                                                                                                    \n",
    "# for layer_s in layer_sizes_range:                                                                                                                                                  \n",
    "#     model = RN_model(layer_s, dropout, learning_rate)                                                                                                                              \n",
    "#     #### Apprentissage                                                                                                                                                             \n",
    "#     start = time.time()                                                                                                                                                            \n",
    "#     #model.fit(X_train, Y_train, batch_size = 100, epochs = 60)                                                                                                                    \n",
    "#     hist_obj = model.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs, validation_data=(X_test, Y_test))                                                             \n",
    "#                                                                                                                                                                                    \n",
    "#     end = time.time()                                                                                                                                                              \n",
    "#     training_delay_RN.append(end - start)                                                                                                                                          \n",
    "#                                                                                                                                                                                    \n",
    "#     history_obj.append( list(hist_obj.history.values()))\n",
    "\n",
    "#     #### Prédiction                                                                                                                                                                \n",
    "#     start = time.time()                                                                                                                                                            \n",
    "#                                                                                                                                                                                    \n",
    "#     Y_pred = np.where(model.predict(X_test) > 0.5, 1, 0)                                                                                                                           \n",
    "#                                                                                                                                                                                    \n",
    "#     end = time.time()                                                                                                                                                              \n",
    "#     predicting_delay_RN.append(end - start)                                                                                                                                        \n",
    "\n",
    "################################## Nombres d'iterations                                                                                                                              \n",
    "# epochs_range = [30,60, 120]#[10,60,500]                                                                                                                                            \n",
    "# max_ep = max(epochs_range)                                                                                                                                                         \n",
    "#                                                                                                                                                                                    \n",
    "# for ep in epochs_range:                                                                                                                                                            \n",
    "#     model = RN_model(layer_sizes, dropout, learning_rate)                                                                                                                          \n",
    "#     #### Apprentissage                                                                                                                                                             \n",
    "#     start = time.time()                                                                                                                                                            \n",
    "#     #model.fit(X_train, Y_train, batch_size = 100, epochs = 60)                                                                                                                    \n",
    "#     hist_obj = model.fit(X_train, Y_train, batch_size = batch_size, epochs = ep, validation_data=(X_test, Y_test))                                                                 \n",
    "#                                                                                                                                                                                    \n",
    "#     end = time.time()                                                                                                                                                              \n",
    "#     training_delay_RN.append(end - start)                                                                                                                                          \n",
    "#                                                                                                                                                                                    \n",
    "#     ho_tmp = list(hist_obj.history.values())                                                                                                                                       \n",
    "#     ho_tmp = [i + [np.nan for _ in range(max_ep-ep)] for i in ho_tmp ]                                                                                                             \n",
    "#     history_obj.append(ho_tmp)\n",
    "#     #### Prédiction                                                                                                                                                                \n",
    "#     start = time.time()                                                                                                                                                            \n",
    "#                                                                                                                                                                                    \n",
    "#     Y_pred = np.where(model.predict(X_test) > 0.5, 1, 0)                                                                                                                           \n",
    "#                                                                                                                                                                                    \n",
    "#     end = time.time()                                                                                                                                                              \n",
    "#     predicting_delay_RN.append(end - start) \n",
    "\n",
    "################################## Learning rate                                                                                                                                     \n",
    "\n",
    "#l_rate_range = [0.00001,0.0005,0.1]                                                                                                                                                 \n",
    "#l_rate_range = [0.0005]                                                                                                                                                             \n",
    "\n",
    "\n",
    "# for l_rate in l_rate_range:                                                                                                                                                        \n",
    "#     model = RN_model(layer_sizes, dropout, l_rate)                                                                                                                                 \n",
    "#     #### Apprentissage                                                                                                                                                             \n",
    "#     start = time.time()                                                                                                                                                            \n",
    "#     #model.fit(X_train, Y_train, batch_size = 100, epochs = 60)                                                                                                                    \n",
    "#     hist_obj = model.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs, validation_data=(X_test, Y_test))                                                             \n",
    "#                                                                                                                                                                                    \n",
    "#     end = time.time()                                                                                                                                                              \n",
    "#     training_delay_RN.append(end - start)                                                                                                                                          \n",
    "#                                                                                                                                                                                    \n",
    "#     history_obj.append( list(hist_obj.history.values()))                                                                                                                           \n",
    "#                                                                                                                                                                                    \n",
    "#     #### Prédiction                                                                                                                                                                \n",
    "#     start = time.time()                                                                                                                                                            \n",
    "#                                                                                                                                                                                    \n",
    "#     Y_pred = np.where(model.predict(X_test) > 0.5, 1, 0)                                                                                                                           \n",
    "#                                                                                                                                                                                    \n",
    "#     end = time.time()                                                                                                                                                              \n",
    "#     predicting_delay_RN.append(end - start)   \n",
    "# \n",
    "#                                                                                                                                                            \n",
    "ho = np.array(history_obj)\n",
    "ho = ho.transpose(1,2,0)\n",
    "\n",
    "\n",
    "sub_title = ['loss','acc','f1','val_loss','val_acc', 'val_f1']\n",
    "x_lab = \"epochs\"\n",
    "\n",
    "\n",
    "leg = [str(i) for i in layer_sizes_range]\n",
    "#leg = [str(i) for i in epochs_range]                                                                                                                                                \n",
    "#leg = [str(i) for i in l_rate_range]                                                                                                                                                \n",
    "\n",
    "titre = \"RN : HyperParam = number of layer\"\n",
    "#titre = \"RN : HyperParam = layer size\"                                                                                                                                              \n",
    "#titre = \"RN : HyperParam = number of epochs\"                                                                                                                                        \n",
    "#titre = \"RN : HyperParam = learning rate\"                                                                                                                                           \n",
    "\n",
    "plot_perf(ho, leg, titre ,sub_title)\n",
    "plot_delay(training_delay_RN,predicting_delay_RN,titre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "La fonction de coût que nous avons choisie est \"Binary Cross-Entropy Loss\". En effet, nous avons choisi cette fonction car nos valeurs cibles sont 0 ou 1, soit \"smooth\" ou \"spiral\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Question 4\n",
    "(L’analyse est claire et l’équipe démontre une compréhension du phénomène de sur-apprentissage. Il le phénomène est correctement décrit et montré dans le graphique dans la mesure du possible.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/30\n",
      "12800/12800 [==============================] - 0s 35us/sample - loss: 0.6914 - acc: 0.5196 - f1: 0.5394 - val_loss: 0.6674 - val_acc: 0.6288 - val_f1: 0.7314\n",
      "Epoch 2/30\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.6181 - acc: 0.6126 - f1: 0.6093 - val_loss: 0.4916 - val_acc: 0.8550 - val_f1: 0.8533\n",
      "Epoch 3/30\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.5316 - acc: 0.6757 - f1: 0.5877 - val_loss: 0.3570 - val_acc: 0.9041 - val_f1: 0.9079\n",
      "Epoch 4/30\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4955 - acc: 0.6856 - f1: 0.5981 - val_loss: 0.3395 - val_acc: 0.8966 - val_f1: 0.8922\n",
      "Epoch 5/30\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4658 - acc: 0.6995 - f1: 0.6151 - val_loss: 0.2713 - val_acc: 0.9325 - val_f1: 0.9354\n",
      "Epoch 6/30\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4485 - acc: 0.7016 - f1: 0.6162 - val_loss: 0.2564 - val_acc: 0.9266 - val_f1: 0.9251\n",
      "Epoch 7/30\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4474 - acc: 0.7170 - f1: 0.7582 - val_loss: 0.2378 - val_acc: 0.9413 - val_f1: 0.9441\n",
      "Epoch 8/30\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4422 - acc: 0.7257 - f1: 0.7818 - val_loss: 0.2394 - val_acc: 0.9359 - val_f1: 0.9359\n",
      "Epoch 9/30\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4318 - acc: 0.7313 - f1: 0.7868 - val_loss: 0.2380 - val_acc: 0.9362 - val_f1: 0.9368\n",
      "Epoch 10/30\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4315 - acc: 0.7325 - f1: 0.7877 - val_loss: 0.2053 - val_acc: 0.9469 - val_f1: 0.9481\n",
      "Epoch 11/30\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4235 - acc: 0.7335 - f1: 0.7886 - val_loss: 0.1909 - val_acc: 0.9478 - val_f1: 0.9506\n",
      "Epoch 12/30\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.4229 - acc: 0.7327 - f1: 0.7888 - val_loss: 0.2138 - val_acc: 0.9375 - val_f1: 0.9373\n",
      "Epoch 13/30\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4310 - acc: 0.7237 - f1: 0.7824 - val_loss: 0.2130 - val_acc: 0.9434 - val_f1: 0.9474\n",
      "Epoch 14/30\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.4252 - acc: 0.7325 - f1: 0.7886 - val_loss: 0.1942 - val_acc: 0.9481 - val_f1: 0.9508\n",
      "Epoch 15/30\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.4227 - acc: 0.7320 - f1: 0.7884 - val_loss: 0.2159 - val_acc: 0.9463 - val_f1: 0.9464\n",
      "Epoch 16/30\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4234 - acc: 0.7300 - f1: 0.7874 - val_loss: 0.2117 - val_acc: 0.9278 - val_f1: 0.9254\n",
      "Epoch 17/30\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4217 - acc: 0.7298 - f1: 0.7870 - val_loss: 0.1834 - val_acc: 0.9516 - val_f1: 0.9533\n",
      "Epoch 18/30\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.4222 - acc: 0.7270 - f1: 0.7854 - val_loss: 0.1798 - val_acc: 0.9528 - val_f1: 0.9548\n",
      "Epoch 19/30\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.4129 - acc: 0.7391 - f1: 0.7928 - val_loss: 0.1804 - val_acc: 0.9522 - val_f1: 0.9528\n",
      "Epoch 20/30\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4176 - acc: 0.7307 - f1: 0.7880 - val_loss: 0.1632 - val_acc: 0.9553 - val_f1: 0.9569\n",
      "Epoch 21/30\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4153 - acc: 0.7330 - f1: 0.7896 - val_loss: 0.1730 - val_acc: 0.9494 - val_f1: 0.9520\n",
      "Epoch 22/30\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.4155 - acc: 0.7350 - f1: 0.7906 - val_loss: 0.2071 - val_acc: 0.9337 - val_f1: 0.9322\n",
      "Epoch 23/30\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.4193 - acc: 0.7335 - f1: 0.7892 - val_loss: 0.1755 - val_acc: 0.9506 - val_f1: 0.9525\n",
      "Epoch 24/30\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4141 - acc: 0.7368 - f1: 0.7927 - val_loss: 0.1682 - val_acc: 0.9503 - val_f1: 0.9510\n",
      "Epoch 25/30\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.4122 - acc: 0.7355 - f1: 0.7905 - val_loss: 0.1561 - val_acc: 0.9525 - val_f1: 0.9542\n",
      "Epoch 26/30\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4137 - acc: 0.7309 - f1: 0.7885 - val_loss: 0.1567 - val_acc: 0.9569 - val_f1: 0.9580\n",
      "Epoch 27/30\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4147 - acc: 0.7339 - f1: 0.7911 - val_loss: 0.1680 - val_acc: 0.9544 - val_f1: 0.9551\n",
      "Epoch 28/30\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4083 - acc: 0.7375 - f1: 0.7932 - val_loss: 0.1620 - val_acc: 0.9544 - val_f1: 0.9562\n",
      "Epoch 29/30\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4111 - acc: 0.7351 - f1: 0.7915 - val_loss: 0.1648 - val_acc: 0.9566 - val_f1: 0.9580\n",
      "Epoch 30/30\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4148 - acc: 0.7369 - f1: 0.7924 - val_loss: 0.1572 - val_acc: 0.9563 - val_f1: 0.9577\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/60\n",
      "12800/12800 [==============================] - 0s 36us/sample - loss: 0.6791 - acc: 0.5435 - f1: 0.5851 - val_loss: 0.6254 - val_acc: 0.8259 - val_f1: 0.8365\n",
      "Epoch 2/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.5923 - acc: 0.7173 - f1: 0.7109 - val_loss: 0.5351 - val_acc: 0.8628 - val_f1: 0.8569\n",
      "Epoch 3/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.5307 - acc: 0.7768 - f1: 0.7586 - val_loss: 0.4271 - val_acc: 0.9053 - val_f1: 0.9116\n",
      "Epoch 4/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4985 - acc: 0.7922 - f1: 0.7702 - val_loss: 0.3911 - val_acc: 0.9078 - val_f1: 0.9135\n",
      "Epoch 5/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4779 - acc: 0.8046 - f1: 0.7820 - val_loss: 0.3639 - val_acc: 0.9316 - val_f1: 0.9348\n",
      "Epoch 6/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4637 - acc: 0.8102 - f1: 0.7879 - val_loss: 0.3501 - val_acc: 0.9409 - val_f1: 0.9420\n",
      "Epoch 7/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4511 - acc: 0.8146 - f1: 0.7917 - val_loss: 0.3378 - val_acc: 0.9431 - val_f1: 0.9441\n",
      "Epoch 8/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4471 - acc: 0.8169 - f1: 0.7955 - val_loss: 0.3270 - val_acc: 0.9422 - val_f1: 0.9428\n",
      "Epoch 9/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4342 - acc: 0.8204 - f1: 0.7989 - val_loss: 0.3581 - val_acc: 0.9109 - val_f1: 0.9073\n",
      "Epoch 10/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4238 - acc: 0.8249 - f1: 0.8042 - val_loss: 0.3014 - val_acc: 0.9463 - val_f1: 0.9468\n",
      "Epoch 11/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4206 - acc: 0.8243 - f1: 0.8022 - val_loss: 0.3258 - val_acc: 0.9269 - val_f1: 0.9251\n",
      "Epoch 12/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4239 - acc: 0.8213 - f1: 0.7989 - val_loss: 0.2911 - val_acc: 0.9456 - val_f1: 0.9463\n",
      "Epoch 13/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4278 - acc: 0.8186 - f1: 0.7946 - val_loss: 0.2794 - val_acc: 0.9506 - val_f1: 0.9520\n",
      "Epoch 14/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4140 - acc: 0.8252 - f1: 0.8044 - val_loss: 0.2766 - val_acc: 0.9491 - val_f1: 0.9517\n",
      "Epoch 15/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4120 - acc: 0.8266 - f1: 0.8058 - val_loss: 0.3027 - val_acc: 0.9322 - val_f1: 0.9304\n",
      "Epoch 16/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4037 - acc: 0.8320 - f1: 0.8121 - val_loss: 0.2758 - val_acc: 0.9456 - val_f1: 0.9457\n",
      "Epoch 17/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4036 - acc: 0.8320 - f1: 0.8120 - val_loss: 0.2561 - val_acc: 0.9547 - val_f1: 0.9552\n",
      "Epoch 18/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4057 - acc: 0.8272 - f1: 0.8056 - val_loss: 0.2518 - val_acc: 0.9547 - val_f1: 0.9556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3978 - acc: 0.8338 - f1: 0.8147 - val_loss: 0.2567 - val_acc: 0.9519 - val_f1: 0.9532\n",
      "Epoch 20/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4058 - acc: 0.8241 - f1: 0.8023 - val_loss: 0.2704 - val_acc: 0.9378 - val_f1: 0.9363\n",
      "Epoch 21/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3990 - acc: 0.8291 - f1: 0.8078 - val_loss: 0.2408 - val_acc: 0.9569 - val_f1: 0.9584\n",
      "Epoch 22/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4000 - acc: 0.8280 - f1: 0.8068 - val_loss: 0.2412 - val_acc: 0.9544 - val_f1: 0.9552\n",
      "Epoch 23/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3924 - acc: 0.8316 - f1: 0.8107 - val_loss: 0.2397 - val_acc: 0.9544 - val_f1: 0.9554\n",
      "Epoch 24/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4002 - acc: 0.8291 - f1: 0.8078 - val_loss: 0.2452 - val_acc: 0.9513 - val_f1: 0.9507\n",
      "Epoch 25/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3977 - acc: 0.8305 - f1: 0.8093 - val_loss: 0.2909 - val_acc: 0.9147 - val_f1: 0.9100\n",
      "Epoch 26/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3983 - acc: 0.8305 - f1: 0.8099 - val_loss: 0.2298 - val_acc: 0.9581 - val_f1: 0.9590\n",
      "Epoch 27/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3918 - acc: 0.8316 - f1: 0.8116 - val_loss: 0.2383 - val_acc: 0.9500 - val_f1: 0.9502\n",
      "Epoch 28/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4053 - acc: 0.8255 - f1: 0.8027 - val_loss: 0.2258 - val_acc: 0.9575 - val_f1: 0.9581\n",
      "Epoch 29/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3911 - acc: 0.8341 - f1: 0.8138 - val_loss: 0.2366 - val_acc: 0.9478 - val_f1: 0.9481\n",
      "Epoch 30/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4002 - acc: 0.8279 - f1: 0.8064 - val_loss: 0.2527 - val_acc: 0.9384 - val_f1: 0.9377\n",
      "Epoch 31/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3949 - acc: 0.8297 - f1: 0.8086 - val_loss: 0.2302 - val_acc: 0.9541 - val_f1: 0.9542\n",
      "Epoch 32/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3970 - acc: 0.8302 - f1: 0.8089 - val_loss: 0.2194 - val_acc: 0.9613 - val_f1: 0.9624\n",
      "Epoch 33/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4016 - acc: 0.8258 - f1: 0.8027 - val_loss: 0.2378 - val_acc: 0.9500 - val_f1: 0.9503\n",
      "Epoch 34/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3874 - acc: 0.8355 - f1: 0.8162 - val_loss: 0.2209 - val_acc: 0.9569 - val_f1: 0.9587\n",
      "Epoch 35/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3967 - acc: 0.8283 - f1: 0.8067 - val_loss: 0.2354 - val_acc: 0.9500 - val_f1: 0.9502\n",
      "Epoch 36/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3901 - acc: 0.8344 - f1: 0.8147 - val_loss: 0.2615 - val_acc: 0.9312 - val_f1: 0.9300\n",
      "Epoch 37/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3884 - acc: 0.8330 - f1: 0.8123 - val_loss: 0.2177 - val_acc: 0.9594 - val_f1: 0.9602\n",
      "Epoch 38/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3933 - acc: 0.8352 - f1: 0.8158 - val_loss: 0.2136 - val_acc: 0.9628 - val_f1: 0.9643\n",
      "Epoch 39/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3907 - acc: 0.8338 - f1: 0.8137 - val_loss: 0.2174 - val_acc: 0.9578 - val_f1: 0.9595\n",
      "Epoch 40/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3867 - acc: 0.8353 - f1: 0.8154 - val_loss: 0.2335 - val_acc: 0.9491 - val_f1: 0.9483\n",
      "Epoch 41/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3872 - acc: 0.8354 - f1: 0.8160 - val_loss: 0.2133 - val_acc: 0.9600 - val_f1: 0.9613\n",
      "Epoch 42/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3900 - acc: 0.8340 - f1: 0.8142 - val_loss: 0.2182 - val_acc: 0.9578 - val_f1: 0.9594\n",
      "Epoch 43/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3888 - acc: 0.8341 - f1: 0.8137 - val_loss: 0.2131 - val_acc: 0.9609 - val_f1: 0.9622\n",
      "Epoch 44/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3847 - acc: 0.8383 - f1: 0.8192 - val_loss: 0.2222 - val_acc: 0.9575 - val_f1: 0.9582\n",
      "Epoch 45/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3848 - acc: 0.8363 - f1: 0.8161 - val_loss: 0.2175 - val_acc: 0.9566 - val_f1: 0.9569\n",
      "Epoch 46/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3890 - acc: 0.8338 - f1: 0.8141 - val_loss: 0.2103 - val_acc: 0.9613 - val_f1: 0.9623\n",
      "Epoch 47/60\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3914 - acc: 0.8329 - f1: 0.8125 - val_loss: 0.2128 - val_acc: 0.9609 - val_f1: 0.9616\n",
      "Epoch 48/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3773 - acc: 0.8409 - f1: 0.8233 - val_loss: 0.2119 - val_acc: 0.9609 - val_f1: 0.9623\n",
      "Epoch 49/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3856 - acc: 0.8354 - f1: 0.8152 - val_loss: 0.2095 - val_acc: 0.9625 - val_f1: 0.9638\n",
      "Epoch 50/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3837 - acc: 0.8377 - f1: 0.8170 - val_loss: 0.2298 - val_acc: 0.9475 - val_f1: 0.9479\n",
      "Epoch 51/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3823 - acc: 0.8381 - f1: 0.8185 - val_loss: 0.2094 - val_acc: 0.9659 - val_f1: 0.9669\n",
      "Epoch 52/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3802 - acc: 0.8409 - f1: 0.8236 - val_loss: 0.2090 - val_acc: 0.9591 - val_f1: 0.9598\n",
      "Epoch 53/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3882 - acc: 0.8341 - f1: 0.8134 - val_loss: 0.2727 - val_acc: 0.9244 - val_f1: 0.9221\n",
      "Epoch 54/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3870 - acc: 0.8355 - f1: 0.8152 - val_loss: 0.2119 - val_acc: 0.9616 - val_f1: 0.9622\n",
      "Epoch 55/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3820 - acc: 0.8387 - f1: 0.8201 - val_loss: 0.2115 - val_acc: 0.9606 - val_f1: 0.9612\n",
      "Epoch 56/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3791 - acc: 0.8398 - f1: 0.8214 - val_loss: 0.2079 - val_acc: 0.9631 - val_f1: 0.9642\n",
      "Epoch 57/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3769 - acc: 0.8408 - f1: 0.8219 - val_loss: 0.2156 - val_acc: 0.9575 - val_f1: 0.9580\n",
      "Epoch 58/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3901 - acc: 0.8340 - f1: 0.8138 - val_loss: 0.2057 - val_acc: 0.9638 - val_f1: 0.9645\n",
      "Epoch 59/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3846 - acc: 0.8366 - f1: 0.8163 - val_loss: 0.2338 - val_acc: 0.9472 - val_f1: 0.9471\n",
      "Epoch 60/60\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3884 - acc: 0.8312 - f1: 0.8100 - val_loss: 0.2197 - val_acc: 0.9559 - val_f1: 0.9558\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/5000\n",
      "12800/12800 [==============================] - 0s 38us/sample - loss: 0.6835 - acc: 0.5594 - f1: 0.6350 - val_loss: 0.6208 - val_acc: 0.7309 - val_f1: 0.7025\n",
      "Epoch 2/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.5963 - acc: 0.7306 - f1: 0.7546 - val_loss: 0.5075 - val_acc: 0.8872 - val_f1: 0.8950\n",
      "Epoch 3/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.5336 - acc: 0.7848 - f1: 0.8129 - val_loss: 0.4412 - val_acc: 0.9041 - val_f1: 0.9100\n",
      "Epoch 4/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.5098 - acc: 0.8003 - f1: 0.8279 - val_loss: 0.4109 - val_acc: 0.9131 - val_f1: 0.9173\n",
      "Epoch 5/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4841 - acc: 0.8131 - f1: 0.8399 - val_loss: 0.3830 - val_acc: 0.9241 - val_f1: 0.9286\n",
      "Epoch 6/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4714 - acc: 0.8163 - f1: 0.8430 - val_loss: 0.3656 - val_acc: 0.9225 - val_f1: 0.9233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.4668 - acc: 0.8155 - f1: 0.8425 - val_loss: 0.3501 - val_acc: 0.9362 - val_f1: 0.9390\n",
      "Epoch 8/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4544 - acc: 0.8248 - f1: 0.8497 - val_loss: 0.3380 - val_acc: 0.9344 - val_f1: 0.9373\n",
      "Epoch 9/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4514 - acc: 0.8207 - f1: 0.8469 - val_loss: 0.3306 - val_acc: 0.9362 - val_f1: 0.9384\n",
      "Epoch 10/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4338 - acc: 0.8320 - f1: 0.8548 - val_loss: 0.3279 - val_acc: 0.9375 - val_f1: 0.9416\n",
      "Epoch 11/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4344 - acc: 0.8300 - f1: 0.8540 - val_loss: 0.3064 - val_acc: 0.9394 - val_f1: 0.9415\n",
      "Epoch 12/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.4189 - acc: 0.8359 - f1: 0.8589 - val_loss: 0.3015 - val_acc: 0.9431 - val_f1: 0.9448\n",
      "Epoch 13/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4269 - acc: 0.8279 - f1: 0.8521 - val_loss: 0.3102 - val_acc: 0.9291 - val_f1: 0.9288\n",
      "Epoch 14/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4287 - acc: 0.8257 - f1: 0.8512 - val_loss: 0.2927 - val_acc: 0.9428 - val_f1: 0.9459\n",
      "Epoch 15/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4065 - acc: 0.8405 - f1: 0.8624 - val_loss: 0.2777 - val_acc: 0.9441 - val_f1: 0.9444\n",
      "Epoch 16/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4081 - acc: 0.8370 - f1: 0.8595 - val_loss: 0.2820 - val_acc: 0.9406 - val_f1: 0.9435\n",
      "Epoch 17/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4008 - acc: 0.8424 - f1: 0.8638 - val_loss: 0.2645 - val_acc: 0.9459 - val_f1: 0.9479\n",
      "Epoch 18/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4027 - acc: 0.8384 - f1: 0.8603 - val_loss: 0.2598 - val_acc: 0.9475 - val_f1: 0.9500\n",
      "Epoch 19/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4028 - acc: 0.8377 - f1: 0.8603 - val_loss: 0.2855 - val_acc: 0.9325 - val_f1: 0.9381\n",
      "Epoch 20/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3999 - acc: 0.8370 - f1: 0.8597 - val_loss: 0.2550 - val_acc: 0.9475 - val_f1: 0.9478\n",
      "Epoch 21/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.4015 - acc: 0.8375 - f1: 0.8600 - val_loss: 0.2568 - val_acc: 0.9484 - val_f1: 0.9514\n",
      "Epoch 22/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3898 - acc: 0.8420 - f1: 0.8640 - val_loss: 0.2615 - val_acc: 0.9419 - val_f1: 0.9451\n",
      "Epoch 23/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3966 - acc: 0.8360 - f1: 0.8595 - val_loss: 0.2412 - val_acc: 0.9478 - val_f1: 0.9503\n",
      "Epoch 24/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.4031 - acc: 0.8305 - f1: 0.8545 - val_loss: 0.2500 - val_acc: 0.9428 - val_f1: 0.9464\n",
      "Epoch 25/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.4024 - acc: 0.8327 - f1: 0.8567 - val_loss: 0.2534 - val_acc: 0.9419 - val_f1: 0.9458\n",
      "Epoch 26/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3964 - acc: 0.8367 - f1: 0.8594 - val_loss: 0.2767 - val_acc: 0.9278 - val_f1: 0.9334\n",
      "Epoch 27/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3912 - acc: 0.8438 - f1: 0.8650 - val_loss: 0.2322 - val_acc: 0.9519 - val_f1: 0.9540\n",
      "Epoch 28/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3881 - acc: 0.8403 - f1: 0.8621 - val_loss: 0.2369 - val_acc: 0.9466 - val_f1: 0.9498\n",
      "Epoch 29/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3850 - acc: 0.8440 - f1: 0.8651 - val_loss: 0.2572 - val_acc: 0.9334 - val_f1: 0.9378\n",
      "Epoch 30/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3921 - acc: 0.8379 - f1: 0.8611 - val_loss: 0.2391 - val_acc: 0.9441 - val_f1: 0.9479\n",
      "Epoch 31/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3852 - acc: 0.8439 - f1: 0.8658 - val_loss: 0.2189 - val_acc: 0.9550 - val_f1: 0.9565\n",
      "Epoch 32/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3852 - acc: 0.8423 - f1: 0.8642 - val_loss: 0.2204 - val_acc: 0.9556 - val_f1: 0.9572\n",
      "Epoch 33/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3903 - acc: 0.8408 - f1: 0.8633 - val_loss: 0.2392 - val_acc: 0.9419 - val_f1: 0.9458\n",
      "Epoch 34/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3884 - acc: 0.8405 - f1: 0.8620 - val_loss: 0.2345 - val_acc: 0.9406 - val_f1: 0.9450\n",
      "Epoch 35/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3832 - acc: 0.8410 - f1: 0.8629 - val_loss: 0.2208 - val_acc: 0.9506 - val_f1: 0.9528\n",
      "Epoch 36/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3858 - acc: 0.8405 - f1: 0.8632 - val_loss: 0.2138 - val_acc: 0.9547 - val_f1: 0.9560\n",
      "Epoch 37/5000\n",
      "12800/12800 [==============================] - 0s 17us/sample - loss: 0.3859 - acc: 0.8427 - f1: 0.8649 - val_loss: 0.2200 - val_acc: 0.9544 - val_f1: 0.9562\n",
      "Epoch 38/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3839 - acc: 0.8440 - f1: 0.8654 - val_loss: 0.2147 - val_acc: 0.9547 - val_f1: 0.9571\n",
      "Epoch 39/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3835 - acc: 0.8424 - f1: 0.8644 - val_loss: 0.2118 - val_acc: 0.9572 - val_f1: 0.9593\n",
      "Epoch 40/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3798 - acc: 0.8449 - f1: 0.8667 - val_loss: 0.2181 - val_acc: 0.9528 - val_f1: 0.9561\n",
      "Epoch 41/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3799 - acc: 0.8448 - f1: 0.8660 - val_loss: 0.2280 - val_acc: 0.9434 - val_f1: 0.9475\n",
      "Epoch 42/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3784 - acc: 0.8462 - f1: 0.8675 - val_loss: 0.2160 - val_acc: 0.9538 - val_f1: 0.9569\n",
      "Epoch 43/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3821 - acc: 0.8427 - f1: 0.8645 - val_loss: 0.2072 - val_acc: 0.9563 - val_f1: 0.9569\n",
      "Epoch 44/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3831 - acc: 0.8432 - f1: 0.8649 - val_loss: 0.2121 - val_acc: 0.9575 - val_f1: 0.9596\n",
      "Epoch 45/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3759 - acc: 0.8478 - f1: 0.8685 - val_loss: 0.2090 - val_acc: 0.9553 - val_f1: 0.9565\n",
      "Epoch 46/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3796 - acc: 0.8456 - f1: 0.8673 - val_loss: 0.2067 - val_acc: 0.9572 - val_f1: 0.9587\n",
      "Epoch 47/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3761 - acc: 0.8456 - f1: 0.8667 - val_loss: 0.2039 - val_acc: 0.9597 - val_f1: 0.9611\n",
      "Epoch 48/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3685 - acc: 0.8514 - f1: 0.8714 - val_loss: 0.2046 - val_acc: 0.9581 - val_f1: 0.9587\n",
      "Epoch 49/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3798 - acc: 0.8457 - f1: 0.8673 - val_loss: 0.2266 - val_acc: 0.9425 - val_f1: 0.9463\n",
      "Epoch 50/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3827 - acc: 0.8429 - f1: 0.8654 - val_loss: 0.2062 - val_acc: 0.9578 - val_f1: 0.9593\n",
      "Epoch 51/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3834 - acc: 0.8434 - f1: 0.8651 - val_loss: 0.2125 - val_acc: 0.9525 - val_f1: 0.9550\n",
      "Epoch 52/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3816 - acc: 0.8441 - f1: 0.8651 - val_loss: 0.2121 - val_acc: 0.9516 - val_f1: 0.9519\n",
      "Epoch 53/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3807 - acc: 0.8459 - f1: 0.8675 - val_loss: 0.2320 - val_acc: 0.9375 - val_f1: 0.9422\n",
      "Epoch 54/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3798 - acc: 0.8441 - f1: 0.8658 - val_loss: 0.2118 - val_acc: 0.9559 - val_f1: 0.9580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3771 - acc: 0.8459 - f1: 0.8675 - val_loss: 0.2483 - val_acc: 0.9353 - val_f1: 0.9407\n",
      "Epoch 56/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3681 - acc: 0.8505 - f1: 0.8710 - val_loss: 0.2095 - val_acc: 0.9525 - val_f1: 0.9552\n",
      "Epoch 57/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3770 - acc: 0.8467 - f1: 0.8675 - val_loss: 0.2199 - val_acc: 0.9472 - val_f1: 0.9505\n",
      "Epoch 58/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3778 - acc: 0.8461 - f1: 0.8675 - val_loss: 0.2065 - val_acc: 0.9578 - val_f1: 0.9591\n",
      "Epoch 59/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3802 - acc: 0.8440 - f1: 0.8660 - val_loss: 0.2077 - val_acc: 0.9572 - val_f1: 0.9580\n",
      "Epoch 60/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3676 - acc: 0.8542 - f1: 0.8736 - val_loss: 0.2079 - val_acc: 0.9528 - val_f1: 0.9547\n",
      "Epoch 61/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3691 - acc: 0.8511 - f1: 0.8715 - val_loss: 0.2110 - val_acc: 0.9519 - val_f1: 0.9547\n",
      "Epoch 62/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3770 - acc: 0.8462 - f1: 0.8675 - val_loss: 0.1991 - val_acc: 0.9591 - val_f1: 0.9614\n",
      "Epoch 63/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3714 - acc: 0.8503 - f1: 0.8708 - val_loss: 0.2180 - val_acc: 0.9478 - val_f1: 0.9512\n",
      "Epoch 64/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3711 - acc: 0.8489 - f1: 0.8699 - val_loss: 0.2142 - val_acc: 0.9516 - val_f1: 0.9535\n",
      "Epoch 65/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3700 - acc: 0.8496 - f1: 0.8703 - val_loss: 0.2031 - val_acc: 0.9600 - val_f1: 0.9615\n",
      "Epoch 66/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3750 - acc: 0.8486 - f1: 0.8692 - val_loss: 0.2080 - val_acc: 0.9547 - val_f1: 0.9557\n",
      "Epoch 67/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3716 - acc: 0.8504 - f1: 0.8711 - val_loss: 0.2561 - val_acc: 0.9256 - val_f1: 0.9322\n",
      "Epoch 68/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3728 - acc: 0.8482 - f1: 0.8685 - val_loss: 0.1979 - val_acc: 0.9603 - val_f1: 0.9616\n",
      "Epoch 69/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3835 - acc: 0.8427 - f1: 0.8648 - val_loss: 0.2036 - val_acc: 0.9559 - val_f1: 0.9570\n",
      "Epoch 70/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3759 - acc: 0.8477 - f1: 0.8688 - val_loss: 0.2008 - val_acc: 0.9619 - val_f1: 0.9626\n",
      "Epoch 71/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3779 - acc: 0.8450 - f1: 0.8662 - val_loss: 0.2050 - val_acc: 0.9563 - val_f1: 0.9571\n",
      "Epoch 72/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3714 - acc: 0.8493 - f1: 0.8699 - val_loss: 0.2074 - val_acc: 0.9550 - val_f1: 0.9566\n",
      "Epoch 73/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3697 - acc: 0.8498 - f1: 0.8705 - val_loss: 0.2015 - val_acc: 0.9591 - val_f1: 0.9596\n",
      "Epoch 74/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3708 - acc: 0.8498 - f1: 0.8707 - val_loss: 0.2119 - val_acc: 0.9528 - val_f1: 0.9556\n",
      "Epoch 75/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3736 - acc: 0.8484 - f1: 0.8685 - val_loss: 0.2417 - val_acc: 0.9381 - val_f1: 0.9426\n",
      "Epoch 76/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3757 - acc: 0.8474 - f1: 0.8684 - val_loss: 0.2050 - val_acc: 0.9550 - val_f1: 0.9571\n",
      "Epoch 77/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3690 - acc: 0.8495 - f1: 0.8700 - val_loss: 0.2006 - val_acc: 0.9594 - val_f1: 0.9603\n",
      "Epoch 78/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3713 - acc: 0.8491 - f1: 0.8698 - val_loss: 0.2217 - val_acc: 0.9438 - val_f1: 0.9478\n",
      "Epoch 79/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3690 - acc: 0.8501 - f1: 0.8705 - val_loss: 0.2002 - val_acc: 0.9572 - val_f1: 0.9587\n",
      "Epoch 80/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3792 - acc: 0.8461 - f1: 0.8676 - val_loss: 0.1979 - val_acc: 0.9606 - val_f1: 0.9620\n",
      "Epoch 81/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3759 - acc: 0.8459 - f1: 0.8675 - val_loss: 0.2029 - val_acc: 0.9588 - val_f1: 0.9602\n",
      "Epoch 82/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3735 - acc: 0.8496 - f1: 0.8701 - val_loss: 0.1990 - val_acc: 0.9588 - val_f1: 0.9602\n",
      "Epoch 83/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3739 - acc: 0.8474 - f1: 0.8682 - val_loss: 0.1985 - val_acc: 0.9613 - val_f1: 0.9620\n",
      "Epoch 84/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3740 - acc: 0.8470 - f1: 0.8684 - val_loss: 0.2028 - val_acc: 0.9559 - val_f1: 0.9584\n",
      "Epoch 85/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3697 - acc: 0.8491 - f1: 0.8701 - val_loss: 0.2159 - val_acc: 0.9500 - val_f1: 0.9530\n",
      "Epoch 86/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3712 - acc: 0.8503 - f1: 0.8706 - val_loss: 0.1968 - val_acc: 0.9625 - val_f1: 0.9632\n",
      "Epoch 87/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3683 - acc: 0.8502 - f1: 0.8707 - val_loss: 0.2014 - val_acc: 0.9600 - val_f1: 0.9608\n",
      "Epoch 88/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3637 - acc: 0.8552 - f1: 0.8746 - val_loss: 0.2195 - val_acc: 0.9478 - val_f1: 0.9512\n",
      "Epoch 89/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3575 - acc: 0.8567 - f1: 0.8762 - val_loss: 0.2225 - val_acc: 0.9469 - val_f1: 0.9503\n",
      "Epoch 90/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3690 - acc: 0.8514 - f1: 0.8717 - val_loss: 0.1978 - val_acc: 0.9588 - val_f1: 0.9613\n",
      "Epoch 91/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3672 - acc: 0.8509 - f1: 0.8716 - val_loss: 0.2056 - val_acc: 0.9550 - val_f1: 0.9571\n",
      "Epoch 92/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3631 - acc: 0.8534 - f1: 0.8726 - val_loss: 0.1968 - val_acc: 0.9591 - val_f1: 0.9610\n",
      "Epoch 93/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3643 - acc: 0.8520 - f1: 0.8718 - val_loss: 0.2083 - val_acc: 0.9525 - val_f1: 0.9554\n",
      "Epoch 94/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3751 - acc: 0.8478 - f1: 0.8686 - val_loss: 0.1975 - val_acc: 0.9631 - val_f1: 0.9646\n",
      "Epoch 95/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3693 - acc: 0.8513 - f1: 0.8722 - val_loss: 0.2022 - val_acc: 0.9581 - val_f1: 0.9600\n",
      "Epoch 96/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3683 - acc: 0.8499 - f1: 0.8703 - val_loss: 0.2025 - val_acc: 0.9550 - val_f1: 0.9571\n",
      "Epoch 97/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3635 - acc: 0.8544 - f1: 0.8743 - val_loss: 0.2397 - val_acc: 0.9334 - val_f1: 0.9393\n",
      "Epoch 98/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3651 - acc: 0.8527 - f1: 0.8728 - val_loss: 0.1953 - val_acc: 0.9600 - val_f1: 0.9612\n",
      "Epoch 99/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3703 - acc: 0.8497 - f1: 0.8706 - val_loss: 0.1972 - val_acc: 0.9584 - val_f1: 0.9600\n",
      "Epoch 100/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3619 - acc: 0.8563 - f1: 0.8756 - val_loss: 0.1963 - val_acc: 0.9609 - val_f1: 0.9620\n",
      "Epoch 101/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3673 - acc: 0.8509 - f1: 0.8714 - val_loss: 0.2137 - val_acc: 0.9503 - val_f1: 0.9534\n",
      "Epoch 102/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3758 - acc: 0.8463 - f1: 0.8676 - val_loss: 0.1952 - val_acc: 0.9619 - val_f1: 0.9632\n",
      "Epoch 103/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3635 - acc: 0.8527 - f1: 0.8727 - val_loss: 0.1943 - val_acc: 0.9609 - val_f1: 0.9630\n",
      "Epoch 104/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3718 - acc: 0.8477 - f1: 0.8685 - val_loss: 0.2003 - val_acc: 0.9619 - val_f1: 0.9632\n",
      "Epoch 105/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3661 - acc: 0.8522 - f1: 0.8724 - val_loss: 0.2049 - val_acc: 0.9538 - val_f1: 0.9570\n",
      "Epoch 106/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3646 - acc: 0.8513 - f1: 0.8720 - val_loss: 0.1983 - val_acc: 0.9594 - val_f1: 0.9620\n",
      "Epoch 107/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3662 - acc: 0.8521 - f1: 0.8720 - val_loss: 0.2004 - val_acc: 0.9581 - val_f1: 0.9598\n",
      "Epoch 108/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3624 - acc: 0.8541 - f1: 0.8734 - val_loss: 0.1929 - val_acc: 0.9609 - val_f1: 0.9619\n",
      "Epoch 109/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3594 - acc: 0.8555 - f1: 0.8751 - val_loss: 0.1985 - val_acc: 0.9575 - val_f1: 0.9585\n",
      "Epoch 110/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3639 - acc: 0.8538 - f1: 0.8742 - val_loss: 0.1937 - val_acc: 0.9609 - val_f1: 0.9626\n",
      "Epoch 111/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3641 - acc: 0.8545 - f1: 0.8744 - val_loss: 0.2114 - val_acc: 0.9500 - val_f1: 0.9531\n",
      "Epoch 112/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3591 - acc: 0.8543 - f1: 0.8747 - val_loss: 0.2183 - val_acc: 0.9469 - val_f1: 0.9506\n",
      "Epoch 113/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3636 - acc: 0.8542 - f1: 0.8740 - val_loss: 0.1992 - val_acc: 0.9581 - val_f1: 0.9604\n",
      "Epoch 114/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3621 - acc: 0.8542 - f1: 0.8745 - val_loss: 0.2116 - val_acc: 0.9488 - val_f1: 0.9522\n",
      "Epoch 115/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3690 - acc: 0.8505 - f1: 0.8710 - val_loss: 0.2225 - val_acc: 0.9453 - val_f1: 0.9491\n",
      "Epoch 116/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3693 - acc: 0.8490 - f1: 0.8706 - val_loss: 0.1971 - val_acc: 0.9597 - val_f1: 0.9604\n",
      "Epoch 117/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3667 - acc: 0.8507 - f1: 0.8712 - val_loss: 0.1958 - val_acc: 0.9638 - val_f1: 0.9648\n",
      "Epoch 118/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3637 - acc: 0.8522 - f1: 0.8724 - val_loss: 0.1960 - val_acc: 0.9631 - val_f1: 0.9652\n",
      "Epoch 119/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3616 - acc: 0.8535 - f1: 0.8730 - val_loss: 0.2118 - val_acc: 0.9559 - val_f1: 0.9569\n",
      "Epoch 120/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3745 - acc: 0.8462 - f1: 0.8678 - val_loss: 0.1980 - val_acc: 0.9563 - val_f1: 0.9589\n",
      "Epoch 121/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3638 - acc: 0.8522 - f1: 0.8724 - val_loss: 0.1983 - val_acc: 0.9600 - val_f1: 0.9615\n",
      "Epoch 122/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3672 - acc: 0.8495 - f1: 0.8705 - val_loss: 0.1966 - val_acc: 0.9609 - val_f1: 0.9629\n",
      "Epoch 123/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3622 - acc: 0.8530 - f1: 0.8732 - val_loss: 0.1973 - val_acc: 0.9600 - val_f1: 0.9619\n",
      "Epoch 124/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3648 - acc: 0.8512 - f1: 0.8716 - val_loss: 0.1955 - val_acc: 0.9616 - val_f1: 0.9635\n",
      "Epoch 125/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3628 - acc: 0.8532 - f1: 0.8735 - val_loss: 0.1951 - val_acc: 0.9631 - val_f1: 0.9644\n",
      "Epoch 126/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3631 - acc: 0.8547 - f1: 0.8744 - val_loss: 0.1976 - val_acc: 0.9631 - val_f1: 0.9645\n",
      "Epoch 127/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3611 - acc: 0.8525 - f1: 0.8726 - val_loss: 0.1935 - val_acc: 0.9650 - val_f1: 0.9663\n",
      "Epoch 128/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3570 - acc: 0.8546 - f1: 0.8744 - val_loss: 0.1922 - val_acc: 0.9625 - val_f1: 0.9634\n",
      "Epoch 129/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3571 - acc: 0.8552 - f1: 0.8754 - val_loss: 0.1982 - val_acc: 0.9581 - val_f1: 0.9601\n",
      "Epoch 130/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3558 - acc: 0.8555 - f1: 0.8753 - val_loss: 0.1979 - val_acc: 0.9594 - val_f1: 0.9604\n",
      "Epoch 131/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3642 - acc: 0.8523 - f1: 0.8729 - val_loss: 0.1940 - val_acc: 0.9656 - val_f1: 0.9664\n",
      "Epoch 132/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3665 - acc: 0.8494 - f1: 0.8697 - val_loss: 0.2251 - val_acc: 0.9409 - val_f1: 0.9444\n",
      "Epoch 133/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3652 - acc: 0.8540 - f1: 0.8743 - val_loss: 0.1999 - val_acc: 0.9603 - val_f1: 0.9613\n",
      "Epoch 134/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3640 - acc: 0.8530 - f1: 0.8734 - val_loss: 0.1994 - val_acc: 0.9625 - val_f1: 0.9642\n",
      "Epoch 135/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3631 - acc: 0.8530 - f1: 0.8734 - val_loss: 0.1930 - val_acc: 0.9647 - val_f1: 0.9655\n",
      "Epoch 136/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3702 - acc: 0.8469 - f1: 0.8680 - val_loss: 0.1974 - val_acc: 0.9622 - val_f1: 0.9641\n",
      "Epoch 137/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3505 - acc: 0.8609 - f1: 0.8794 - val_loss: 0.1999 - val_acc: 0.9569 - val_f1: 0.9587\n",
      "Epoch 138/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3649 - acc: 0.8527 - f1: 0.8730 - val_loss: 0.2098 - val_acc: 0.9550 - val_f1: 0.9569\n",
      "Epoch 139/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3688 - acc: 0.8498 - f1: 0.8712 - val_loss: 0.2051 - val_acc: 0.9538 - val_f1: 0.9562\n",
      "Epoch 140/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3569 - acc: 0.8550 - f1: 0.8744 - val_loss: 0.1949 - val_acc: 0.9622 - val_f1: 0.9639\n",
      "Epoch 141/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3688 - acc: 0.8502 - f1: 0.8710 - val_loss: 0.1959 - val_acc: 0.9663 - val_f1: 0.9674\n",
      "Epoch 142/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3707 - acc: 0.8484 - f1: 0.8699 - val_loss: 0.2571 - val_acc: 0.9272 - val_f1: 0.9333\n",
      "Epoch 143/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3682 - acc: 0.8493 - f1: 0.8701 - val_loss: 0.1935 - val_acc: 0.9634 - val_f1: 0.9651\n",
      "Epoch 144/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3631 - acc: 0.8538 - f1: 0.8737 - val_loss: 0.1959 - val_acc: 0.9613 - val_f1: 0.9629\n",
      "Epoch 145/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3585 - acc: 0.8579 - f1: 0.8764 - val_loss: 0.1991 - val_acc: 0.9594 - val_f1: 0.9611\n",
      "Epoch 146/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3598 - acc: 0.8541 - f1: 0.8743 - val_loss: 0.2022 - val_acc: 0.9606 - val_f1: 0.9614\n",
      "Epoch 147/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3636 - acc: 0.8530 - f1: 0.8732 - val_loss: 0.2013 - val_acc: 0.9597 - val_f1: 0.9608\n",
      "Epoch 148/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3720 - acc: 0.8488 - f1: 0.8699 - val_loss: 0.1927 - val_acc: 0.9634 - val_f1: 0.9642\n",
      "Epoch 149/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3609 - acc: 0.8530 - f1: 0.8732 - val_loss: 0.2022 - val_acc: 0.9547 - val_f1: 0.9576\n",
      "Epoch 150/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3676 - acc: 0.8499 - f1: 0.8709 - val_loss: 0.2003 - val_acc: 0.9628 - val_f1: 0.9641\n",
      "Epoch 151/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3573 - acc: 0.8559 - f1: 0.8760 - val_loss: 0.2017 - val_acc: 0.9547 - val_f1: 0.9566\n",
      "Epoch 152/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3598 - acc: 0.8558 - f1: 0.8748 - val_loss: 0.1992 - val_acc: 0.9634 - val_f1: 0.9651\n",
      "Epoch 153/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3551 - acc: 0.8569 - f1: 0.8764 - val_loss: 0.1945 - val_acc: 0.9628 - val_f1: 0.9643\n",
      "Epoch 154/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3613 - acc: 0.8530 - f1: 0.8730 - val_loss: 0.1915 - val_acc: 0.9656 - val_f1: 0.9672\n",
      "Epoch 155/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3540 - acc: 0.8590 - f1: 0.8779 - val_loss: 0.1992 - val_acc: 0.9616 - val_f1: 0.9638\n",
      "Epoch 156/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3662 - acc: 0.8524 - f1: 0.8729 - val_loss: 0.1997 - val_acc: 0.9566 - val_f1: 0.9592\n",
      "Epoch 157/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3621 - acc: 0.8545 - f1: 0.8742 - val_loss: 0.1962 - val_acc: 0.9600 - val_f1: 0.9620\n",
      "Epoch 158/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3625 - acc: 0.8527 - f1: 0.8726 - val_loss: 0.2021 - val_acc: 0.9553 - val_f1: 0.9574\n",
      "Epoch 159/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3644 - acc: 0.8524 - f1: 0.8728 - val_loss: 0.1991 - val_acc: 0.9594 - val_f1: 0.9617\n",
      "Epoch 160/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3553 - acc: 0.8591 - f1: 0.8778 - val_loss: 0.1984 - val_acc: 0.9606 - val_f1: 0.9632\n",
      "Epoch 161/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3595 - acc: 0.8549 - f1: 0.8751 - val_loss: 0.2330 - val_acc: 0.9397 - val_f1: 0.9442\n",
      "Epoch 162/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3683 - acc: 0.8496 - f1: 0.8709 - val_loss: 0.2145 - val_acc: 0.9494 - val_f1: 0.9528\n",
      "Epoch 163/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3579 - acc: 0.8555 - f1: 0.8752 - val_loss: 0.2330 - val_acc: 0.9384 - val_f1: 0.9432\n",
      "Epoch 164/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3571 - acc: 0.8578 - f1: 0.8761 - val_loss: 0.1979 - val_acc: 0.9572 - val_f1: 0.9591\n",
      "Epoch 165/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3572 - acc: 0.8580 - f1: 0.8769 - val_loss: 0.2038 - val_acc: 0.9547 - val_f1: 0.9574\n",
      "Epoch 166/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3651 - acc: 0.8517 - f1: 0.8722 - val_loss: 0.1938 - val_acc: 0.9634 - val_f1: 0.9648\n",
      "Epoch 167/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3630 - acc: 0.8534 - f1: 0.8740 - val_loss: 0.1958 - val_acc: 0.9666 - val_f1: 0.9678\n",
      "Epoch 168/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3490 - acc: 0.8621 - f1: 0.8807 - val_loss: 0.1950 - val_acc: 0.9634 - val_f1: 0.9651\n",
      "Epoch 169/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3560 - acc: 0.8571 - f1: 0.8763 - val_loss: 0.1922 - val_acc: 0.9641 - val_f1: 0.9656\n",
      "Epoch 170/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3544 - acc: 0.8575 - f1: 0.8771 - val_loss: 0.2037 - val_acc: 0.9553 - val_f1: 0.9582\n",
      "Epoch 171/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3656 - acc: 0.8520 - f1: 0.8725 - val_loss: 0.2044 - val_acc: 0.9606 - val_f1: 0.9619\n",
      "Epoch 172/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3616 - acc: 0.8516 - f1: 0.8724 - val_loss: 0.2051 - val_acc: 0.9559 - val_f1: 0.9584\n",
      "Epoch 173/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3626 - acc: 0.8536 - f1: 0.8737 - val_loss: 0.1951 - val_acc: 0.9638 - val_f1: 0.9649\n",
      "Epoch 174/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3596 - acc: 0.8548 - f1: 0.8745 - val_loss: 0.1927 - val_acc: 0.9659 - val_f1: 0.9671\n",
      "Epoch 175/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3607 - acc: 0.8545 - f1: 0.8742 - val_loss: 0.2081 - val_acc: 0.9538 - val_f1: 0.9567\n",
      "Epoch 176/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3625 - acc: 0.8538 - f1: 0.8739 - val_loss: 0.2036 - val_acc: 0.9556 - val_f1: 0.9583\n",
      "Epoch 177/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3533 - acc: 0.8578 - f1: 0.8769 - val_loss: 0.1955 - val_acc: 0.9603 - val_f1: 0.9628\n",
      "Epoch 178/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3503 - acc: 0.8606 - f1: 0.8793 - val_loss: 0.1928 - val_acc: 0.9625 - val_f1: 0.9641\n",
      "Epoch 179/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3600 - acc: 0.8538 - f1: 0.8740 - val_loss: 0.2012 - val_acc: 0.9559 - val_f1: 0.9586\n",
      "Epoch 180/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3533 - acc: 0.8573 - f1: 0.8766 - val_loss: 0.1942 - val_acc: 0.9656 - val_f1: 0.9667\n",
      "Epoch 181/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3555 - acc: 0.8590 - f1: 0.8778 - val_loss: 0.1939 - val_acc: 0.9622 - val_f1: 0.9635\n",
      "Epoch 182/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3565 - acc: 0.8553 - f1: 0.8754 - val_loss: 0.1939 - val_acc: 0.9656 - val_f1: 0.9672\n",
      "Epoch 183/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3623 - acc: 0.8529 - f1: 0.8732 - val_loss: 0.1945 - val_acc: 0.9600 - val_f1: 0.9608\n",
      "Epoch 184/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3602 - acc: 0.8534 - f1: 0.8737 - val_loss: 0.2036 - val_acc: 0.9600 - val_f1: 0.9613\n",
      "Epoch 185/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3577 - acc: 0.8554 - f1: 0.8746 - val_loss: 0.2154 - val_acc: 0.9491 - val_f1: 0.9526\n",
      "Epoch 186/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3609 - acc: 0.8516 - f1: 0.8725 - val_loss: 0.2007 - val_acc: 0.9591 - val_f1: 0.9614\n",
      "Epoch 187/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3491 - acc: 0.8601 - f1: 0.8790 - val_loss: 0.1953 - val_acc: 0.9594 - val_f1: 0.9612\n",
      "Epoch 188/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3605 - acc: 0.8537 - f1: 0.8738 - val_loss: 0.1936 - val_acc: 0.9669 - val_f1: 0.9680\n",
      "Epoch 189/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3600 - acc: 0.8535 - f1: 0.8735 - val_loss: 0.2287 - val_acc: 0.9422 - val_f1: 0.9462\n",
      "Epoch 190/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3623 - acc: 0.8521 - f1: 0.8727 - val_loss: 0.2022 - val_acc: 0.9578 - val_f1: 0.9596\n",
      "Epoch 191/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3554 - acc: 0.8567 - f1: 0.8760 - val_loss: 0.1915 - val_acc: 0.9678 - val_f1: 0.9692\n",
      "Epoch 192/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3604 - acc: 0.8534 - f1: 0.8734 - val_loss: 0.1974 - val_acc: 0.9600 - val_f1: 0.9616\n",
      "Epoch 193/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3631 - acc: 0.8527 - f1: 0.8727 - val_loss: 0.1956 - val_acc: 0.9619 - val_f1: 0.9626\n",
      "Epoch 194/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3618 - acc: 0.8538 - f1: 0.8740 - val_loss: 0.1956 - val_acc: 0.9644 - val_f1: 0.9654\n",
      "Epoch 195/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3535 - acc: 0.8591 - f1: 0.8782 - val_loss: 0.2101 - val_acc: 0.9575 - val_f1: 0.9589\n",
      "Epoch 196/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3515 - acc: 0.8582 - f1: 0.8775 - val_loss: 0.1949 - val_acc: 0.9650 - val_f1: 0.9661\n",
      "Epoch 197/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3539 - acc: 0.8577 - f1: 0.8771 - val_loss: 0.1946 - val_acc: 0.9631 - val_f1: 0.9652\n",
      "Epoch 198/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3565 - acc: 0.8539 - f1: 0.8741 - val_loss: 0.1964 - val_acc: 0.9594 - val_f1: 0.9618\n",
      "Epoch 199/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3635 - acc: 0.8523 - f1: 0.8729 - val_loss: 0.1910 - val_acc: 0.9666 - val_f1: 0.9670\n",
      "Epoch 200/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3536 - acc: 0.8600 - f1: 0.8791 - val_loss: 0.2062 - val_acc: 0.9522 - val_f1: 0.9548\n",
      "Epoch 201/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3563 - acc: 0.8564 - f1: 0.8757 - val_loss: 0.1924 - val_acc: 0.9641 - val_f1: 0.9658\n",
      "Epoch 202/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3602 - acc: 0.8542 - f1: 0.8746 - val_loss: 0.2406 - val_acc: 0.9325 - val_f1: 0.9383\n",
      "Epoch 203/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3603 - acc: 0.8538 - f1: 0.8740 - val_loss: 0.1977 - val_acc: 0.9638 - val_f1: 0.9649\n",
      "Epoch 204/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3654 - acc: 0.8520 - f1: 0.8723 - val_loss: 0.1998 - val_acc: 0.9644 - val_f1: 0.9659\n",
      "Epoch 205/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3585 - acc: 0.8550 - f1: 0.8751 - val_loss: 0.1920 - val_acc: 0.9653 - val_f1: 0.9668\n",
      "Epoch 206/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3627 - acc: 0.8525 - f1: 0.8733 - val_loss: 0.1943 - val_acc: 0.9631 - val_f1: 0.9641\n",
      "Epoch 207/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3510 - acc: 0.8585 - f1: 0.8777 - val_loss: 0.2053 - val_acc: 0.9581 - val_f1: 0.9602\n",
      "Epoch 208/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3605 - acc: 0.8559 - f1: 0.8757 - val_loss: 0.1954 - val_acc: 0.9650 - val_f1: 0.9663\n",
      "Epoch 209/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3618 - acc: 0.8539 - f1: 0.8739 - val_loss: 0.1951 - val_acc: 0.9631 - val_f1: 0.9648\n",
      "Epoch 210/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3517 - acc: 0.8596 - f1: 0.8787 - val_loss: 0.2144 - val_acc: 0.9491 - val_f1: 0.9521\n",
      "Epoch 211/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3579 - acc: 0.8549 - f1: 0.8750 - val_loss: 0.2054 - val_acc: 0.9547 - val_f1: 0.9575\n",
      "Epoch 212/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3584 - acc: 0.8552 - f1: 0.8751 - val_loss: 0.1978 - val_acc: 0.9616 - val_f1: 0.9633\n",
      "Epoch 213/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3548 - acc: 0.8568 - f1: 0.8767 - val_loss: 0.1965 - val_acc: 0.9628 - val_f1: 0.9646\n",
      "Epoch 214/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3579 - acc: 0.8562 - f1: 0.8757 - val_loss: 0.1974 - val_acc: 0.9625 - val_f1: 0.9646\n",
      "Epoch 215/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3487 - acc: 0.8609 - f1: 0.8799 - val_loss: 0.1924 - val_acc: 0.9647 - val_f1: 0.9662\n",
      "Epoch 216/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3560 - acc: 0.8567 - f1: 0.8764 - val_loss: 0.2011 - val_acc: 0.9613 - val_f1: 0.9634\n",
      "Epoch 217/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3707 - acc: 0.8487 - f1: 0.8703 - val_loss: 0.1957 - val_acc: 0.9644 - val_f1: 0.9660\n",
      "Epoch 218/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3521 - acc: 0.8585 - f1: 0.8778 - val_loss: 0.1933 - val_acc: 0.9663 - val_f1: 0.9678\n",
      "Epoch 219/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3551 - acc: 0.8570 - f1: 0.8764 - val_loss: 0.1959 - val_acc: 0.9647 - val_f1: 0.9661\n",
      "Epoch 220/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3514 - acc: 0.8596 - f1: 0.8785 - val_loss: 0.1953 - val_acc: 0.9638 - val_f1: 0.9652\n",
      "Epoch 221/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3637 - acc: 0.8516 - f1: 0.8724 - val_loss: 0.1948 - val_acc: 0.9653 - val_f1: 0.9662\n",
      "Epoch 222/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3528 - acc: 0.8600 - f1: 0.8788 - val_loss: 0.1910 - val_acc: 0.9647 - val_f1: 0.9659\n",
      "Epoch 223/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3637 - acc: 0.8520 - f1: 0.8729 - val_loss: 0.1951 - val_acc: 0.9659 - val_f1: 0.9673\n",
      "Epoch 224/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3550 - acc: 0.8564 - f1: 0.8762 - val_loss: 0.2045 - val_acc: 0.9566 - val_f1: 0.9591\n",
      "Epoch 225/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3540 - acc: 0.8567 - f1: 0.8764 - val_loss: 0.2000 - val_acc: 0.9606 - val_f1: 0.9621\n",
      "Epoch 226/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3668 - acc: 0.8510 - f1: 0.8716 - val_loss: 0.2015 - val_acc: 0.9594 - val_f1: 0.9618\n",
      "Epoch 227/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3545 - acc: 0.8573 - f1: 0.8768 - val_loss: 0.2254 - val_acc: 0.9478 - val_f1: 0.9518\n",
      "Epoch 228/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3527 - acc: 0.8581 - f1: 0.8777 - val_loss: 0.2003 - val_acc: 0.9619 - val_f1: 0.9636\n",
      "Epoch 229/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3573 - acc: 0.8545 - f1: 0.8750 - val_loss: 0.2016 - val_acc: 0.9622 - val_f1: 0.9643\n",
      "Epoch 230/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3527 - acc: 0.8585 - f1: 0.8773 - val_loss: 0.1982 - val_acc: 0.9634 - val_f1: 0.9656\n",
      "Epoch 231/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3547 - acc: 0.8577 - f1: 0.8770 - val_loss: 0.1937 - val_acc: 0.9656 - val_f1: 0.9666\n",
      "Epoch 232/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3613 - acc: 0.8542 - f1: 0.8748 - val_loss: 0.2024 - val_acc: 0.9581 - val_f1: 0.9596\n",
      "Epoch 233/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3546 - acc: 0.8566 - f1: 0.8764 - val_loss: 0.2035 - val_acc: 0.9588 - val_f1: 0.9611\n",
      "Epoch 234/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3689 - acc: 0.8502 - f1: 0.8715 - val_loss: 0.2398 - val_acc: 0.9316 - val_f1: 0.9373\n",
      "Epoch 235/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3633 - acc: 0.8537 - f1: 0.8737 - val_loss: 0.2044 - val_acc: 0.9550 - val_f1: 0.9574\n",
      "Epoch 236/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3525 - acc: 0.8573 - f1: 0.8768 - val_loss: 0.1930 - val_acc: 0.9656 - val_f1: 0.9667\n",
      "Epoch 237/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3527 - acc: 0.8591 - f1: 0.8784 - val_loss: 0.2093 - val_acc: 0.9531 - val_f1: 0.9564\n",
      "Epoch 238/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3502 - acc: 0.8616 - f1: 0.8797 - val_loss: 0.2008 - val_acc: 0.9603 - val_f1: 0.9627\n",
      "Epoch 239/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3656 - acc: 0.8517 - f1: 0.8725 - val_loss: 0.1983 - val_acc: 0.9591 - val_f1: 0.9609\n",
      "Epoch 240/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3574 - acc: 0.8555 - f1: 0.8751 - val_loss: 0.1966 - val_acc: 0.9619 - val_f1: 0.9640\n",
      "Epoch 241/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3566 - acc: 0.8563 - f1: 0.8762 - val_loss: 0.2380 - val_acc: 0.9350 - val_f1: 0.9406\n",
      "Epoch 242/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3533 - acc: 0.8591 - f1: 0.8780 - val_loss: 0.2096 - val_acc: 0.9522 - val_f1: 0.9550\n",
      "Epoch 243/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3627 - acc: 0.8514 - f1: 0.8723 - val_loss: 0.2042 - val_acc: 0.9563 - val_f1: 0.9589\n",
      "Epoch 244/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3547 - acc: 0.8577 - f1: 0.8769 - val_loss: 0.1992 - val_acc: 0.9613 - val_f1: 0.9639\n",
      "Epoch 245/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3460 - acc: 0.8609 - f1: 0.8797 - val_loss: 0.2264 - val_acc: 0.9422 - val_f1: 0.9460\n",
      "Epoch 246/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3501 - acc: 0.8594 - f1: 0.8785 - val_loss: 0.1938 - val_acc: 0.9653 - val_f1: 0.9667\n",
      "Epoch 247/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3482 - acc: 0.8616 - f1: 0.8795 - val_loss: 0.1965 - val_acc: 0.9650 - val_f1: 0.9659\n",
      "Epoch 248/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3535 - acc: 0.8582 - f1: 0.8774 - val_loss: 0.2144 - val_acc: 0.9450 - val_f1: 0.9491\n",
      "Epoch 249/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3571 - acc: 0.8568 - f1: 0.8766 - val_loss: 0.1967 - val_acc: 0.9616 - val_f1: 0.9633\n",
      "Epoch 250/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3599 - acc: 0.8545 - f1: 0.8749 - val_loss: 0.2032 - val_acc: 0.9594 - val_f1: 0.9592\n",
      "Epoch 251/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3486 - acc: 0.8609 - f1: 0.8795 - val_loss: 0.1893 - val_acc: 0.9663 - val_f1: 0.9670\n",
      "Epoch 252/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3488 - acc: 0.8608 - f1: 0.8795 - val_loss: 0.1913 - val_acc: 0.9647 - val_f1: 0.9663\n",
      "Epoch 253/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3559 - acc: 0.8569 - f1: 0.8764 - val_loss: 0.1871 - val_acc: 0.9653 - val_f1: 0.9668\n",
      "Epoch 254/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3542 - acc: 0.8584 - f1: 0.8780 - val_loss: 0.1932 - val_acc: 0.9641 - val_f1: 0.9659\n",
      "Epoch 255/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3534 - acc: 0.8571 - f1: 0.8766 - val_loss: 0.2118 - val_acc: 0.9506 - val_f1: 0.9543\n",
      "Epoch 256/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3636 - acc: 0.8514 - f1: 0.8722 - val_loss: 0.1922 - val_acc: 0.9653 - val_f1: 0.9666\n",
      "Epoch 257/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3502 - acc: 0.8586 - f1: 0.8780 - val_loss: 0.1966 - val_acc: 0.9656 - val_f1: 0.9673\n",
      "Epoch 258/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3572 - acc: 0.8547 - f1: 0.8745 - val_loss: 0.2010 - val_acc: 0.9597 - val_f1: 0.9604\n",
      "Epoch 259/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3498 - acc: 0.8601 - f1: 0.8786 - val_loss: 0.2023 - val_acc: 0.9550 - val_f1: 0.9580\n",
      "Epoch 260/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3583 - acc: 0.8548 - f1: 0.8748 - val_loss: 0.1953 - val_acc: 0.9684 - val_f1: 0.9697\n",
      "Epoch 261/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3537 - acc: 0.8562 - f1: 0.8756 - val_loss: 0.1912 - val_acc: 0.9669 - val_f1: 0.9680\n",
      "Epoch 262/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3574 - acc: 0.8546 - f1: 0.8746 - val_loss: 0.2043 - val_acc: 0.9597 - val_f1: 0.9616\n",
      "Epoch 263/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3561 - acc: 0.8571 - f1: 0.8769 - val_loss: 0.1948 - val_acc: 0.9638 - val_f1: 0.9654\n",
      "Epoch 264/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3567 - acc: 0.8560 - f1: 0.8755 - val_loss: 0.1990 - val_acc: 0.9603 - val_f1: 0.9622\n",
      "Epoch 265/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3541 - acc: 0.8577 - f1: 0.8768 - val_loss: 0.2037 - val_acc: 0.9600 - val_f1: 0.9619\n",
      "Epoch 266/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3600 - acc: 0.8535 - f1: 0.8744 - val_loss: 0.2014 - val_acc: 0.9616 - val_f1: 0.9628\n",
      "Epoch 267/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3590 - acc: 0.8553 - f1: 0.8753 - val_loss: 0.1965 - val_acc: 0.9638 - val_f1: 0.9647\n",
      "Epoch 268/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3606 - acc: 0.8530 - f1: 0.8729 - val_loss: 0.1957 - val_acc: 0.9641 - val_f1: 0.9653\n",
      "Epoch 269/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3566 - acc: 0.8557 - f1: 0.8758 - val_loss: 0.2070 - val_acc: 0.9588 - val_f1: 0.9607\n",
      "Epoch 270/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3476 - acc: 0.8602 - f1: 0.8788 - val_loss: 0.1975 - val_acc: 0.9638 - val_f1: 0.9661\n",
      "Epoch 271/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3540 - acc: 0.8574 - f1: 0.8768 - val_loss: 0.2003 - val_acc: 0.9603 - val_f1: 0.9627\n",
      "Epoch 272/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3517 - acc: 0.8584 - f1: 0.8773 - val_loss: 0.1966 - val_acc: 0.9647 - val_f1: 0.9656\n",
      "Epoch 273/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3572 - acc: 0.8562 - f1: 0.8764 - val_loss: 0.2154 - val_acc: 0.9584 - val_f1: 0.9597\n",
      "Epoch 274/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3616 - acc: 0.8531 - f1: 0.8733 - val_loss: 0.2032 - val_acc: 0.9622 - val_f1: 0.9633\n",
      "Epoch 275/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3656 - acc: 0.8518 - f1: 0.8724 - val_loss: 0.1956 - val_acc: 0.9628 - val_f1: 0.9643\n",
      "Epoch 276/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3578 - acc: 0.8552 - f1: 0.8755 - val_loss: 0.1942 - val_acc: 0.9656 - val_f1: 0.9675\n",
      "Epoch 277/5000\n",
      "12800/12800 [==============================] - 0s 17us/sample - loss: 0.3485 - acc: 0.8608 - f1: 0.8794 - val_loss: 0.2089 - val_acc: 0.9528 - val_f1: 0.9559\n",
      "Epoch 278/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3560 - acc: 0.8573 - f1: 0.8766 - val_loss: 0.1949 - val_acc: 0.9653 - val_f1: 0.9675\n",
      "Epoch 279/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3524 - acc: 0.8587 - f1: 0.8779 - val_loss: 0.1919 - val_acc: 0.9653 - val_f1: 0.9671\n",
      "Epoch 280/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3513 - acc: 0.8593 - f1: 0.8784 - val_loss: 0.1941 - val_acc: 0.9666 - val_f1: 0.9680\n",
      "Epoch 281/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3565 - acc: 0.8570 - f1: 0.8769 - val_loss: 0.2006 - val_acc: 0.9634 - val_f1: 0.9652\n",
      "Epoch 282/5000\n",
      "12800/12800 [==============================] - 0s 18us/sample - loss: 0.3470 - acc: 0.8625 - f1: 0.8807 - val_loss: 0.1956 - val_acc: 0.9653 - val_f1: 0.9667\n",
      "Epoch 283/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3521 - acc: 0.8582 - f1: 0.8768 - val_loss: 0.1939 - val_acc: 0.9678 - val_f1: 0.9690\n",
      "Epoch 284/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3612 - acc: 0.8527 - f1: 0.8737 - val_loss: 0.1926 - val_acc: 0.9678 - val_f1: 0.9694\n",
      "Epoch 285/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3590 - acc: 0.8537 - f1: 0.8740 - val_loss: 0.2080 - val_acc: 0.9519 - val_f1: 0.9554\n",
      "Epoch 286/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3547 - acc: 0.8570 - f1: 0.8764 - val_loss: 0.1975 - val_acc: 0.9634 - val_f1: 0.9648\n",
      "Epoch 287/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3542 - acc: 0.8573 - f1: 0.8764 - val_loss: 0.1975 - val_acc: 0.9669 - val_f1: 0.9681\n",
      "Epoch 288/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3519 - acc: 0.8573 - f1: 0.8765 - val_loss: 0.2046 - val_acc: 0.9588 - val_f1: 0.9617\n",
      "Epoch 289/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3500 - acc: 0.8600 - f1: 0.8792 - val_loss: 0.1953 - val_acc: 0.9647 - val_f1: 0.9655\n",
      "Epoch 290/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3520 - acc: 0.8587 - f1: 0.8781 - val_loss: 0.1988 - val_acc: 0.9606 - val_f1: 0.9620\n",
      "Epoch 291/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3470 - acc: 0.8602 - f1: 0.8789 - val_loss: 0.1943 - val_acc: 0.9634 - val_f1: 0.9653\n",
      "Epoch 292/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3570 - acc: 0.8546 - f1: 0.8742 - val_loss: 0.2023 - val_acc: 0.9588 - val_f1: 0.9605\n",
      "Epoch 293/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3651 - acc: 0.8517 - f1: 0.8725 - val_loss: 0.1973 - val_acc: 0.9641 - val_f1: 0.9650\n",
      "Epoch 294/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3521 - acc: 0.8577 - f1: 0.8774 - val_loss: 0.2019 - val_acc: 0.9597 - val_f1: 0.9616\n",
      "Epoch 295/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3479 - acc: 0.8615 - f1: 0.8802 - val_loss: 0.1970 - val_acc: 0.9638 - val_f1: 0.9650\n",
      "Epoch 296/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3462 - acc: 0.8620 - f1: 0.8803 - val_loss: 0.1956 - val_acc: 0.9666 - val_f1: 0.9683\n",
      "Epoch 297/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3513 - acc: 0.8587 - f1: 0.8781 - val_loss: 0.2058 - val_acc: 0.9522 - val_f1: 0.9548\n",
      "Epoch 298/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3568 - acc: 0.8562 - f1: 0.8761 - val_loss: 0.1998 - val_acc: 0.9647 - val_f1: 0.9660\n",
      "Epoch 299/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3466 - acc: 0.8615 - f1: 0.8801 - val_loss: 0.1975 - val_acc: 0.9647 - val_f1: 0.9655\n",
      "Epoch 300/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3616 - acc: 0.8523 - f1: 0.8729 - val_loss: 0.2118 - val_acc: 0.9506 - val_f1: 0.9539\n",
      "Epoch 301/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3492 - acc: 0.8609 - f1: 0.8797 - val_loss: 0.2068 - val_acc: 0.9606 - val_f1: 0.9610\n",
      "Epoch 302/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3533 - acc: 0.8577 - f1: 0.8769 - val_loss: 0.1991 - val_acc: 0.9659 - val_f1: 0.9676\n",
      "Epoch 303/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3553 - acc: 0.8582 - f1: 0.8780 - val_loss: 0.1916 - val_acc: 0.9659 - val_f1: 0.9668\n",
      "Epoch 304/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3557 - acc: 0.8569 - f1: 0.8761 - val_loss: 0.1981 - val_acc: 0.9644 - val_f1: 0.9653\n",
      "Epoch 305/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3551 - acc: 0.8556 - f1: 0.8757 - val_loss: 0.2030 - val_acc: 0.9622 - val_f1: 0.9629\n",
      "Epoch 306/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3520 - acc: 0.8594 - f1: 0.8789 - val_loss: 0.1954 - val_acc: 0.9653 - val_f1: 0.9668\n",
      "Epoch 307/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3484 - acc: 0.8610 - f1: 0.8797 - val_loss: 0.1979 - val_acc: 0.9628 - val_f1: 0.9648\n",
      "Epoch 308/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3455 - acc: 0.8614 - f1: 0.8800 - val_loss: 0.2108 - val_acc: 0.9613 - val_f1: 0.9623\n",
      "Epoch 309/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3495 - acc: 0.8589 - f1: 0.8780 - val_loss: 0.1973 - val_acc: 0.9638 - val_f1: 0.9651\n",
      "Epoch 310/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3508 - acc: 0.8586 - f1: 0.8778 - val_loss: 0.1992 - val_acc: 0.9644 - val_f1: 0.9654\n",
      "Epoch 311/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3515 - acc: 0.8609 - f1: 0.8796 - val_loss: 0.2013 - val_acc: 0.9619 - val_f1: 0.9638\n",
      "Epoch 312/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3479 - acc: 0.8599 - f1: 0.8790 - val_loss: 0.2078 - val_acc: 0.9541 - val_f1: 0.9572\n",
      "Epoch 313/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3481 - acc: 0.8615 - f1: 0.8802 - val_loss: 0.1964 - val_acc: 0.9656 - val_f1: 0.9671\n",
      "Epoch 314/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3550 - acc: 0.8555 - f1: 0.8754 - val_loss: 0.2027 - val_acc: 0.9603 - val_f1: 0.9611\n",
      "Epoch 315/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3567 - acc: 0.8559 - f1: 0.8751 - val_loss: 0.2140 - val_acc: 0.9497 - val_f1: 0.9532\n",
      "Epoch 316/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3582 - acc: 0.8545 - f1: 0.8749 - val_loss: 0.2001 - val_acc: 0.9663 - val_f1: 0.9675\n",
      "Epoch 317/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3586 - acc: 0.8572 - f1: 0.8765 - val_loss: 0.1966 - val_acc: 0.9647 - val_f1: 0.9660\n",
      "Epoch 318/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3582 - acc: 0.8562 - f1: 0.8760 - val_loss: 0.1962 - val_acc: 0.9691 - val_f1: 0.9704\n",
      "Epoch 319/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3512 - acc: 0.8585 - f1: 0.8782 - val_loss: 0.2013 - val_acc: 0.9663 - val_f1: 0.9669\n",
      "Epoch 320/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3444 - acc: 0.8646 - f1: 0.8823 - val_loss: 0.2010 - val_acc: 0.9659 - val_f1: 0.9670\n",
      "Epoch 321/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3501 - acc: 0.8572 - f1: 0.8766 - val_loss: 0.1982 - val_acc: 0.9663 - val_f1: 0.9672\n",
      "Epoch 322/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3487 - acc: 0.8582 - f1: 0.8782 - val_loss: 0.2072 - val_acc: 0.9619 - val_f1: 0.9632\n",
      "Epoch 323/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3507 - acc: 0.8581 - f1: 0.8773 - val_loss: 0.2186 - val_acc: 0.9469 - val_f1: 0.9504\n",
      "Epoch 324/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3532 - acc: 0.8587 - f1: 0.8779 - val_loss: 0.2011 - val_acc: 0.9600 - val_f1: 0.9615\n",
      "Epoch 325/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3507 - acc: 0.8596 - f1: 0.8779 - val_loss: 0.1957 - val_acc: 0.9653 - val_f1: 0.9667\n",
      "Epoch 326/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3550 - acc: 0.8570 - f1: 0.8769 - val_loss: 0.2155 - val_acc: 0.9531 - val_f1: 0.9563\n",
      "Epoch 327/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3430 - acc: 0.8641 - f1: 0.8817 - val_loss: 0.1979 - val_acc: 0.9644 - val_f1: 0.9656\n",
      "Epoch 328/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3497 - acc: 0.8592 - f1: 0.8787 - val_loss: 0.1989 - val_acc: 0.9641 - val_f1: 0.9657\n",
      "Epoch 329/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3498 - acc: 0.8588 - f1: 0.8781 - val_loss: 0.2039 - val_acc: 0.9616 - val_f1: 0.9634\n",
      "Epoch 330/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3529 - acc: 0.8584 - f1: 0.8779 - val_loss: 0.2010 - val_acc: 0.9600 - val_f1: 0.9620\n",
      "Epoch 331/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3567 - acc: 0.8550 - f1: 0.8749 - val_loss: 0.2158 - val_acc: 0.9634 - val_f1: 0.9649\n",
      "Epoch 332/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3520 - acc: 0.8575 - f1: 0.8767 - val_loss: 0.2015 - val_acc: 0.9634 - val_f1: 0.9644\n",
      "Epoch 333/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3540 - acc: 0.8575 - f1: 0.8765 - val_loss: 0.2078 - val_acc: 0.9638 - val_f1: 0.9653\n",
      "Epoch 334/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3523 - acc: 0.8587 - f1: 0.8783 - val_loss: 0.1983 - val_acc: 0.9616 - val_f1: 0.9633\n",
      "Epoch 335/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3544 - acc: 0.8562 - f1: 0.8764 - val_loss: 0.2037 - val_acc: 0.9606 - val_f1: 0.9628\n",
      "Epoch 336/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3519 - acc: 0.8602 - f1: 0.8791 - val_loss: 0.2017 - val_acc: 0.9641 - val_f1: 0.9651\n",
      "Epoch 337/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3594 - acc: 0.8540 - f1: 0.8741 - val_loss: 0.2015 - val_acc: 0.9619 - val_f1: 0.9639\n",
      "Epoch 338/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3537 - acc: 0.8590 - f1: 0.8787 - val_loss: 0.1952 - val_acc: 0.9613 - val_f1: 0.9629\n",
      "Epoch 339/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3483 - acc: 0.8598 - f1: 0.8787 - val_loss: 0.2082 - val_acc: 0.9644 - val_f1: 0.9654\n",
      "Epoch 340/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3452 - acc: 0.8619 - f1: 0.8802 - val_loss: 0.2034 - val_acc: 0.9631 - val_f1: 0.9644\n",
      "Epoch 341/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3522 - acc: 0.8548 - f1: 0.8751 - val_loss: 0.2038 - val_acc: 0.9619 - val_f1: 0.9634\n",
      "Epoch 342/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3390 - acc: 0.8651 - f1: 0.8831 - val_loss: 0.1966 - val_acc: 0.9631 - val_f1: 0.9650\n",
      "Epoch 343/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3599 - acc: 0.8550 - f1: 0.8752 - val_loss: 0.2008 - val_acc: 0.9656 - val_f1: 0.9669\n",
      "Epoch 344/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3526 - acc: 0.8570 - f1: 0.8765 - val_loss: 0.2052 - val_acc: 0.9591 - val_f1: 0.9611\n",
      "Epoch 345/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3505 - acc: 0.8579 - f1: 0.8773 - val_loss: 0.2027 - val_acc: 0.9575 - val_f1: 0.9590\n",
      "Epoch 346/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3539 - acc: 0.8570 - f1: 0.8771 - val_loss: 0.2008 - val_acc: 0.9613 - val_f1: 0.9631\n",
      "Epoch 347/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3485 - acc: 0.8591 - f1: 0.8784 - val_loss: 0.1981 - val_acc: 0.9628 - val_f1: 0.9643\n",
      "Epoch 348/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3460 - acc: 0.8612 - f1: 0.8798 - val_loss: 0.1965 - val_acc: 0.9666 - val_f1: 0.9679\n",
      "Epoch 349/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3559 - acc: 0.8551 - f1: 0.8751 - val_loss: 0.2153 - val_acc: 0.9506 - val_f1: 0.9539\n",
      "Epoch 350/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3528 - acc: 0.8584 - f1: 0.8779 - val_loss: 0.2009 - val_acc: 0.9656 - val_f1: 0.9666\n",
      "Epoch 351/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3514 - acc: 0.8591 - f1: 0.8784 - val_loss: 0.2195 - val_acc: 0.9453 - val_f1: 0.9499\n",
      "Epoch 352/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3551 - acc: 0.8560 - f1: 0.8760 - val_loss: 0.2395 - val_acc: 0.9362 - val_f1: 0.9416\n",
      "Epoch 353/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3529 - acc: 0.8586 - f1: 0.8783 - val_loss: 0.2024 - val_acc: 0.9606 - val_f1: 0.9627\n",
      "Epoch 354/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3531 - acc: 0.8589 - f1: 0.8774 - val_loss: 0.1994 - val_acc: 0.9600 - val_f1: 0.9618\n",
      "Epoch 355/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3453 - acc: 0.8608 - f1: 0.8795 - val_loss: 0.1975 - val_acc: 0.9678 - val_f1: 0.9686\n",
      "Epoch 356/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3456 - acc: 0.8634 - f1: 0.8817 - val_loss: 0.1958 - val_acc: 0.9653 - val_f1: 0.9671\n",
      "Epoch 357/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3541 - acc: 0.8572 - f1: 0.8767 - val_loss: 0.1986 - val_acc: 0.9616 - val_f1: 0.9635\n",
      "Epoch 358/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3446 - acc: 0.8623 - f1: 0.8810 - val_loss: 0.2013 - val_acc: 0.9622 - val_f1: 0.9626\n",
      "Epoch 359/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3542 - acc: 0.8581 - f1: 0.8775 - val_loss: 0.1959 - val_acc: 0.9606 - val_f1: 0.9628\n",
      "Epoch 360/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3532 - acc: 0.8569 - f1: 0.8766 - val_loss: 0.1932 - val_acc: 0.9656 - val_f1: 0.9669\n",
      "Epoch 361/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3534 - acc: 0.8572 - f1: 0.8772 - val_loss: 0.2014 - val_acc: 0.9647 - val_f1: 0.9663\n",
      "Epoch 362/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3547 - acc: 0.8561 - f1: 0.8756 - val_loss: 0.2026 - val_acc: 0.9591 - val_f1: 0.9615\n",
      "Epoch 363/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3556 - acc: 0.8577 - f1: 0.8769 - val_loss: 0.1954 - val_acc: 0.9650 - val_f1: 0.9664\n",
      "Epoch 364/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3399 - acc: 0.8664 - f1: 0.8842 - val_loss: 0.1986 - val_acc: 0.9634 - val_f1: 0.9648\n",
      "Epoch 365/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3451 - acc: 0.8601 - f1: 0.8788 - val_loss: 0.1977 - val_acc: 0.9644 - val_f1: 0.9653\n",
      "Epoch 366/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3504 - acc: 0.8589 - f1: 0.8784 - val_loss: 0.2021 - val_acc: 0.9634 - val_f1: 0.9647\n",
      "Epoch 367/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3562 - acc: 0.8564 - f1: 0.8762 - val_loss: 0.2154 - val_acc: 0.9513 - val_f1: 0.9545\n",
      "Epoch 368/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3604 - acc: 0.8555 - f1: 0.8756 - val_loss: 0.2056 - val_acc: 0.9631 - val_f1: 0.9640\n",
      "Epoch 369/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3538 - acc: 0.8572 - f1: 0.8769 - val_loss: 0.2010 - val_acc: 0.9631 - val_f1: 0.9647\n",
      "Epoch 370/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3484 - acc: 0.8609 - f1: 0.8795 - val_loss: 0.1954 - val_acc: 0.9644 - val_f1: 0.9659\n",
      "Epoch 371/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3501 - acc: 0.8591 - f1: 0.8783 - val_loss: 0.2113 - val_acc: 0.9538 - val_f1: 0.9563\n",
      "Epoch 372/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3357 - acc: 0.8667 - f1: 0.8841 - val_loss: 0.2029 - val_acc: 0.9609 - val_f1: 0.9628\n",
      "Epoch 373/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3578 - acc: 0.8552 - f1: 0.8756 - val_loss: 0.2061 - val_acc: 0.9519 - val_f1: 0.9545\n",
      "Epoch 374/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3488 - acc: 0.8620 - f1: 0.8803 - val_loss: 0.2025 - val_acc: 0.9628 - val_f1: 0.9642\n",
      "Epoch 375/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3551 - acc: 0.8564 - f1: 0.8763 - val_loss: 0.1999 - val_acc: 0.9588 - val_f1: 0.9607\n",
      "Epoch 376/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3493 - acc: 0.8606 - f1: 0.8791 - val_loss: 0.2240 - val_acc: 0.9453 - val_f1: 0.9489\n",
      "Epoch 377/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3447 - acc: 0.8620 - f1: 0.8804 - val_loss: 0.2032 - val_acc: 0.9581 - val_f1: 0.9607\n",
      "Epoch 378/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3426 - acc: 0.8650 - f1: 0.8832 - val_loss: 0.1953 - val_acc: 0.9663 - val_f1: 0.9677\n",
      "Epoch 379/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3522 - acc: 0.8583 - f1: 0.8775 - val_loss: 0.1953 - val_acc: 0.9622 - val_f1: 0.9637\n",
      "Epoch 380/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3423 - acc: 0.8637 - f1: 0.8818 - val_loss: 0.2075 - val_acc: 0.9544 - val_f1: 0.9568\n",
      "Epoch 381/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3506 - acc: 0.8578 - f1: 0.8778 - val_loss: 0.1932 - val_acc: 0.9675 - val_f1: 0.9691\n",
      "Epoch 382/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3496 - acc: 0.8591 - f1: 0.8781 - val_loss: 0.2084 - val_acc: 0.9563 - val_f1: 0.9585\n",
      "Epoch 383/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3512 - acc: 0.8612 - f1: 0.8801 - val_loss: 0.1971 - val_acc: 0.9647 - val_f1: 0.9659\n",
      "Epoch 384/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3479 - acc: 0.8611 - f1: 0.8802 - val_loss: 0.1998 - val_acc: 0.9613 - val_f1: 0.9632\n",
      "Epoch 385/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3396 - acc: 0.8657 - f1: 0.8838 - val_loss: 0.2070 - val_acc: 0.9569 - val_f1: 0.9594\n",
      "Epoch 386/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3389 - acc: 0.8683 - f1: 0.8856 - val_loss: 0.1938 - val_acc: 0.9659 - val_f1: 0.9681\n",
      "Epoch 387/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3506 - acc: 0.8610 - f1: 0.8796 - val_loss: 0.2109 - val_acc: 0.9572 - val_f1: 0.9592\n",
      "Epoch 388/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3531 - acc: 0.8575 - f1: 0.8773 - val_loss: 0.2180 - val_acc: 0.9513 - val_f1: 0.9546\n",
      "Epoch 389/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3613 - acc: 0.8533 - f1: 0.8739 - val_loss: 0.1977 - val_acc: 0.9684 - val_f1: 0.9695\n",
      "Epoch 390/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3463 - acc: 0.8614 - f1: 0.8801 - val_loss: 0.1976 - val_acc: 0.9638 - val_f1: 0.9656\n",
      "Epoch 391/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3566 - acc: 0.8559 - f1: 0.8757 - val_loss: 0.1984 - val_acc: 0.9641 - val_f1: 0.9648\n",
      "Epoch 392/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3363 - acc: 0.8662 - f1: 0.8837 - val_loss: 0.1989 - val_acc: 0.9641 - val_f1: 0.9661\n",
      "Epoch 393/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3504 - acc: 0.8584 - f1: 0.8772 - val_loss: 0.2089 - val_acc: 0.9575 - val_f1: 0.9602\n",
      "Epoch 394/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3547 - acc: 0.8574 - f1: 0.8765 - val_loss: 0.1981 - val_acc: 0.9641 - val_f1: 0.9651\n",
      "Epoch 395/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3472 - acc: 0.8598 - f1: 0.8789 - val_loss: 0.2036 - val_acc: 0.9619 - val_f1: 0.9627\n",
      "Epoch 396/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3659 - acc: 0.8505 - f1: 0.8712 - val_loss: 0.2004 - val_acc: 0.9650 - val_f1: 0.9663\n",
      "Epoch 397/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3585 - acc: 0.8559 - f1: 0.8754 - val_loss: 0.1972 - val_acc: 0.9638 - val_f1: 0.9653\n",
      "Epoch 398/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3536 - acc: 0.8579 - f1: 0.8772 - val_loss: 0.1956 - val_acc: 0.9684 - val_f1: 0.9699\n",
      "Epoch 399/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3430 - acc: 0.8641 - f1: 0.8822 - val_loss: 0.1955 - val_acc: 0.9659 - val_f1: 0.9670\n",
      "Epoch 400/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3455 - acc: 0.8611 - f1: 0.8800 - val_loss: 0.1964 - val_acc: 0.9669 - val_f1: 0.9685\n",
      "Epoch 401/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3488 - acc: 0.8598 - f1: 0.8790 - val_loss: 0.2079 - val_acc: 0.9613 - val_f1: 0.9629\n",
      "Epoch 402/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3458 - acc: 0.8628 - f1: 0.8811 - val_loss: 0.1996 - val_acc: 0.9631 - val_f1: 0.9652\n",
      "Epoch 403/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3499 - acc: 0.8604 - f1: 0.8794 - val_loss: 0.1952 - val_acc: 0.9691 - val_f1: 0.9705\n",
      "Epoch 404/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3537 - acc: 0.8590 - f1: 0.8780 - val_loss: 0.1966 - val_acc: 0.9650 - val_f1: 0.9669\n",
      "Epoch 405/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3475 - acc: 0.8594 - f1: 0.8777 - val_loss: 0.2066 - val_acc: 0.9563 - val_f1: 0.9588\n",
      "Epoch 406/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3398 - acc: 0.8636 - f1: 0.8818 - val_loss: 0.1997 - val_acc: 0.9659 - val_f1: 0.9673\n",
      "Epoch 407/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3517 - acc: 0.8595 - f1: 0.8788 - val_loss: 0.2028 - val_acc: 0.9619 - val_f1: 0.9639\n",
      "Epoch 408/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3526 - acc: 0.8583 - f1: 0.8775 - val_loss: 0.1998 - val_acc: 0.9609 - val_f1: 0.9631\n",
      "Epoch 409/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3555 - acc: 0.8557 - f1: 0.8756 - val_loss: 0.2010 - val_acc: 0.9628 - val_f1: 0.9644\n",
      "Epoch 410/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3487 - acc: 0.8579 - f1: 0.8779 - val_loss: 0.2112 - val_acc: 0.9631 - val_f1: 0.9644\n",
      "Epoch 411/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3495 - acc: 0.8587 - f1: 0.8779 - val_loss: 0.2031 - val_acc: 0.9619 - val_f1: 0.9634\n",
      "Epoch 412/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3475 - acc: 0.8591 - f1: 0.8780 - val_loss: 0.2012 - val_acc: 0.9609 - val_f1: 0.9622\n",
      "Epoch 413/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3492 - acc: 0.8605 - f1: 0.8795 - val_loss: 0.2008 - val_acc: 0.9659 - val_f1: 0.9676\n",
      "Epoch 414/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3520 - acc: 0.8572 - f1: 0.8770 - val_loss: 0.1989 - val_acc: 0.9619 - val_f1: 0.9637\n",
      "Epoch 415/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3455 - acc: 0.8612 - f1: 0.8794 - val_loss: 0.1989 - val_acc: 0.9666 - val_f1: 0.9676\n",
      "Epoch 416/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3515 - acc: 0.8574 - f1: 0.8769 - val_loss: 0.2002 - val_acc: 0.9644 - val_f1: 0.9656\n",
      "Epoch 417/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3468 - acc: 0.8614 - f1: 0.8799 - val_loss: 0.2083 - val_acc: 0.9575 - val_f1: 0.9594\n",
      "Epoch 418/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3425 - acc: 0.8623 - f1: 0.8809 - val_loss: 0.1999 - val_acc: 0.9625 - val_f1: 0.9646\n",
      "Epoch 419/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3508 - acc: 0.8596 - f1: 0.8791 - val_loss: 0.1941 - val_acc: 0.9634 - val_f1: 0.9656\n",
      "Epoch 420/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3463 - acc: 0.8614 - f1: 0.8799 - val_loss: 0.1986 - val_acc: 0.9631 - val_f1: 0.9648\n",
      "Epoch 421/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3489 - acc: 0.8598 - f1: 0.8790 - val_loss: 0.2185 - val_acc: 0.9613 - val_f1: 0.9620\n",
      "Epoch 422/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3437 - acc: 0.8623 - f1: 0.8807 - val_loss: 0.1989 - val_acc: 0.9659 - val_f1: 0.9674\n",
      "Epoch 423/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3547 - acc: 0.8555 - f1: 0.8759 - val_loss: 0.2022 - val_acc: 0.9653 - val_f1: 0.9667\n",
      "Epoch 424/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3536 - acc: 0.8590 - f1: 0.8786 - val_loss: 0.2065 - val_acc: 0.9606 - val_f1: 0.9626\n",
      "Epoch 425/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3532 - acc: 0.8577 - f1: 0.8775 - val_loss: 0.1992 - val_acc: 0.9672 - val_f1: 0.9688\n",
      "Epoch 426/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3448 - acc: 0.8601 - f1: 0.8793 - val_loss: 0.2036 - val_acc: 0.9641 - val_f1: 0.9648\n",
      "Epoch 427/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3460 - acc: 0.8616 - f1: 0.8802 - val_loss: 0.1969 - val_acc: 0.9613 - val_f1: 0.9630\n",
      "Epoch 428/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3473 - acc: 0.8593 - f1: 0.8788 - val_loss: 0.1946 - val_acc: 0.9656 - val_f1: 0.9669\n",
      "Epoch 429/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3466 - acc: 0.8623 - f1: 0.8811 - val_loss: 0.2006 - val_acc: 0.9666 - val_f1: 0.9679\n",
      "Epoch 430/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3486 - acc: 0.8616 - f1: 0.8805 - val_loss: 0.2026 - val_acc: 0.9653 - val_f1: 0.9663\n",
      "Epoch 431/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3483 - acc: 0.8589 - f1: 0.8786 - val_loss: 0.1964 - val_acc: 0.9641 - val_f1: 0.9652\n",
      "Epoch 432/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3432 - acc: 0.8623 - f1: 0.8805 - val_loss: 0.2115 - val_acc: 0.9613 - val_f1: 0.9629\n",
      "Epoch 433/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3513 - acc: 0.8579 - f1: 0.8771 - val_loss: 0.2042 - val_acc: 0.9634 - val_f1: 0.9652\n",
      "Epoch 434/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3441 - acc: 0.8627 - f1: 0.8808 - val_loss: 0.2059 - val_acc: 0.9641 - val_f1: 0.9653\n",
      "Epoch 435/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3513 - acc: 0.8587 - f1: 0.8776 - val_loss: 0.2047 - val_acc: 0.9622 - val_f1: 0.9638\n",
      "Epoch 436/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3515 - acc: 0.8580 - f1: 0.8772 - val_loss: 0.2024 - val_acc: 0.9625 - val_f1: 0.9641\n",
      "Epoch 437/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3392 - acc: 0.8646 - f1: 0.8824 - val_loss: 0.2053 - val_acc: 0.9669 - val_f1: 0.9682\n",
      "Epoch 438/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3519 - acc: 0.8584 - f1: 0.8779 - val_loss: 0.2126 - val_acc: 0.9591 - val_f1: 0.9613\n",
      "Epoch 439/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3429 - acc: 0.8644 - f1: 0.8828 - val_loss: 0.2084 - val_acc: 0.9663 - val_f1: 0.9672\n",
      "Epoch 440/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3458 - acc: 0.8612 - f1: 0.8792 - val_loss: 0.2349 - val_acc: 0.9403 - val_f1: 0.9451\n",
      "Epoch 441/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3444 - acc: 0.8616 - f1: 0.8805 - val_loss: 0.2032 - val_acc: 0.9656 - val_f1: 0.9667\n",
      "Epoch 442/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3495 - acc: 0.8595 - f1: 0.8786 - val_loss: 0.2012 - val_acc: 0.9606 - val_f1: 0.9625\n",
      "Epoch 443/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3472 - acc: 0.8607 - f1: 0.8794 - val_loss: 0.1998 - val_acc: 0.9634 - val_f1: 0.9644\n",
      "Epoch 444/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3418 - acc: 0.8643 - f1: 0.8824 - val_loss: 0.2110 - val_acc: 0.9566 - val_f1: 0.9585\n",
      "Epoch 445/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3476 - acc: 0.8619 - f1: 0.8806 - val_loss: 0.2083 - val_acc: 0.9625 - val_f1: 0.9642\n",
      "Epoch 446/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3479 - acc: 0.8611 - f1: 0.8795 - val_loss: 0.1992 - val_acc: 0.9625 - val_f1: 0.9640\n",
      "Epoch 447/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3497 - acc: 0.8588 - f1: 0.8779 - val_loss: 0.1966 - val_acc: 0.9672 - val_f1: 0.9682\n",
      "Epoch 448/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3446 - acc: 0.8623 - f1: 0.8806 - val_loss: 0.2007 - val_acc: 0.9606 - val_f1: 0.9635\n",
      "Epoch 449/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3527 - acc: 0.8576 - f1: 0.8770 - val_loss: 0.1946 - val_acc: 0.9631 - val_f1: 0.9653\n",
      "Epoch 450/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3500 - acc: 0.8586 - f1: 0.8778 - val_loss: 0.1935 - val_acc: 0.9653 - val_f1: 0.9671\n",
      "Epoch 451/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3432 - acc: 0.8625 - f1: 0.8811 - val_loss: 0.1989 - val_acc: 0.9663 - val_f1: 0.9676\n",
      "Epoch 452/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3492 - acc: 0.8587 - f1: 0.8784 - val_loss: 0.2059 - val_acc: 0.9588 - val_f1: 0.9614\n",
      "Epoch 453/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3545 - acc: 0.8571 - f1: 0.8769 - val_loss: 0.2146 - val_acc: 0.9522 - val_f1: 0.9547\n",
      "Epoch 454/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3499 - acc: 0.8584 - f1: 0.8780 - val_loss: 0.1980 - val_acc: 0.9641 - val_f1: 0.9661\n",
      "Epoch 455/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3440 - acc: 0.8624 - f1: 0.8807 - val_loss: 0.1935 - val_acc: 0.9656 - val_f1: 0.9670\n",
      "Epoch 456/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3516 - acc: 0.8580 - f1: 0.8776 - val_loss: 0.2096 - val_acc: 0.9628 - val_f1: 0.9638\n",
      "Epoch 457/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3478 - acc: 0.8599 - f1: 0.8788 - val_loss: 0.2154 - val_acc: 0.9613 - val_f1: 0.9621\n",
      "Epoch 458/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3476 - acc: 0.8605 - f1: 0.8797 - val_loss: 0.2109 - val_acc: 0.9588 - val_f1: 0.9603\n",
      "Epoch 459/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3531 - acc: 0.8570 - f1: 0.8766 - val_loss: 0.2008 - val_acc: 0.9650 - val_f1: 0.9665\n",
      "Epoch 460/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3416 - acc: 0.8650 - f1: 0.8828 - val_loss: 0.2016 - val_acc: 0.9647 - val_f1: 0.9662\n",
      "Epoch 461/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3533 - acc: 0.8542 - f1: 0.8745 - val_loss: 0.2080 - val_acc: 0.9569 - val_f1: 0.9596\n",
      "Epoch 462/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3443 - acc: 0.8616 - f1: 0.8803 - val_loss: 0.2076 - val_acc: 0.9656 - val_f1: 0.9666\n",
      "Epoch 463/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3445 - acc: 0.8612 - f1: 0.8801 - val_loss: 0.2031 - val_acc: 0.9641 - val_f1: 0.9655\n",
      "Epoch 464/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3386 - acc: 0.8645 - f1: 0.8832 - val_loss: 0.2049 - val_acc: 0.9628 - val_f1: 0.9649\n",
      "Epoch 465/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3456 - acc: 0.8617 - f1: 0.8808 - val_loss: 0.2013 - val_acc: 0.9622 - val_f1: 0.9641\n",
      "Epoch 466/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3466 - acc: 0.8613 - f1: 0.8800 - val_loss: 0.2052 - val_acc: 0.9603 - val_f1: 0.9627\n",
      "Epoch 467/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3462 - acc: 0.8612 - f1: 0.8796 - val_loss: 0.2074 - val_acc: 0.9563 - val_f1: 0.9585\n",
      "Epoch 468/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3449 - acc: 0.8613 - f1: 0.8801 - val_loss: 0.2172 - val_acc: 0.9609 - val_f1: 0.9621\n",
      "Epoch 469/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3441 - acc: 0.8611 - f1: 0.8799 - val_loss: 0.2010 - val_acc: 0.9653 - val_f1: 0.9661\n",
      "Epoch 470/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3461 - acc: 0.8608 - f1: 0.8792 - val_loss: 0.2047 - val_acc: 0.9600 - val_f1: 0.9621\n",
      "Epoch 471/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3555 - acc: 0.8549 - f1: 0.8750 - val_loss: 0.2018 - val_acc: 0.9613 - val_f1: 0.9635\n",
      "Epoch 472/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3471 - acc: 0.8593 - f1: 0.8784 - val_loss: 0.2042 - val_acc: 0.9678 - val_f1: 0.9689\n",
      "Epoch 473/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3371 - acc: 0.8658 - f1: 0.8832 - val_loss: 0.1973 - val_acc: 0.9656 - val_f1: 0.9669\n",
      "Epoch 474/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3523 - acc: 0.8576 - f1: 0.8771 - val_loss: 0.2175 - val_acc: 0.9644 - val_f1: 0.9653\n",
      "Epoch 475/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3459 - acc: 0.8602 - f1: 0.8788 - val_loss: 0.2049 - val_acc: 0.9578 - val_f1: 0.9601\n",
      "Epoch 476/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3440 - acc: 0.8634 - f1: 0.8820 - val_loss: 0.1957 - val_acc: 0.9613 - val_f1: 0.9625\n",
      "Epoch 477/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3484 - acc: 0.8600 - f1: 0.8789 - val_loss: 0.2041 - val_acc: 0.9653 - val_f1: 0.9669\n",
      "Epoch 478/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3444 - acc: 0.8633 - f1: 0.8809 - val_loss: 0.2113 - val_acc: 0.9600 - val_f1: 0.9617\n",
      "Epoch 479/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3499 - acc: 0.8602 - f1: 0.8795 - val_loss: 0.1948 - val_acc: 0.9675 - val_f1: 0.9689\n",
      "Epoch 480/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3433 - acc: 0.8619 - f1: 0.8806 - val_loss: 0.2118 - val_acc: 0.9613 - val_f1: 0.9624\n",
      "Epoch 481/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3434 - acc: 0.8625 - f1: 0.8811 - val_loss: 0.2016 - val_acc: 0.9622 - val_f1: 0.9637\n",
      "Epoch 482/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3464 - acc: 0.8641 - f1: 0.8819 - val_loss: 0.2008 - val_acc: 0.9631 - val_f1: 0.9648\n",
      "Epoch 483/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3343 - acc: 0.8678 - f1: 0.8854 - val_loss: 0.2019 - val_acc: 0.9656 - val_f1: 0.9666\n",
      "Epoch 484/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3434 - acc: 0.8620 - f1: 0.8812 - val_loss: 0.2003 - val_acc: 0.9650 - val_f1: 0.9666\n",
      "Epoch 485/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3339 - acc: 0.8683 - f1: 0.8856 - val_loss: 0.2038 - val_acc: 0.9656 - val_f1: 0.9670\n",
      "Epoch 486/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3401 - acc: 0.8634 - f1: 0.8818 - val_loss: 0.2101 - val_acc: 0.9578 - val_f1: 0.9600\n",
      "Epoch 487/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3412 - acc: 0.8645 - f1: 0.8830 - val_loss: 0.2001 - val_acc: 0.9647 - val_f1: 0.9659\n",
      "Epoch 488/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3470 - acc: 0.8612 - f1: 0.8802 - val_loss: 0.2075 - val_acc: 0.9634 - val_f1: 0.9641\n",
      "Epoch 489/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3426 - acc: 0.8639 - f1: 0.8825 - val_loss: 0.2051 - val_acc: 0.9641 - val_f1: 0.9650\n",
      "Epoch 490/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3464 - acc: 0.8614 - f1: 0.8803 - val_loss: 0.2002 - val_acc: 0.9669 - val_f1: 0.9688\n",
      "Epoch 491/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3484 - acc: 0.8612 - f1: 0.8798 - val_loss: 0.2093 - val_acc: 0.9647 - val_f1: 0.9662\n",
      "Epoch 492/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3521 - acc: 0.8580 - f1: 0.8772 - val_loss: 0.2113 - val_acc: 0.9594 - val_f1: 0.9614\n",
      "Epoch 493/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3526 - acc: 0.8554 - f1: 0.8753 - val_loss: 0.2056 - val_acc: 0.9616 - val_f1: 0.9637\n",
      "Epoch 494/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3504 - acc: 0.8588 - f1: 0.8786 - val_loss: 0.2068 - val_acc: 0.9631 - val_f1: 0.9651\n",
      "Epoch 495/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3435 - acc: 0.8626 - f1: 0.8815 - val_loss: 0.2139 - val_acc: 0.9575 - val_f1: 0.9601\n",
      "Epoch 496/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3523 - acc: 0.8569 - f1: 0.8764 - val_loss: 0.2068 - val_acc: 0.9653 - val_f1: 0.9664\n",
      "Epoch 497/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3471 - acc: 0.8605 - f1: 0.8797 - val_loss: 0.2216 - val_acc: 0.9631 - val_f1: 0.9642\n",
      "Epoch 498/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3452 - acc: 0.8617 - f1: 0.8804 - val_loss: 0.2105 - val_acc: 0.9609 - val_f1: 0.9622\n",
      "Epoch 499/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3425 - acc: 0.8632 - f1: 0.8812 - val_loss: 0.2116 - val_acc: 0.9631 - val_f1: 0.9640\n",
      "Epoch 500/5000\n",
      "12800/12800 [==============================] - 0s 22us/sample - loss: 0.3439 - acc: 0.8609 - f1: 0.8801 - val_loss: 0.2090 - val_acc: 0.9672 - val_f1: 0.9686\n",
      "Epoch 501/5000\n",
      "12800/12800 [==============================] - 0s 20us/sample - loss: 0.3478 - acc: 0.8584 - f1: 0.8780 - val_loss: 0.2111 - val_acc: 0.9556 - val_f1: 0.9583\n",
      "Epoch 502/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3544 - acc: 0.8565 - f1: 0.8759 - val_loss: 0.2189 - val_acc: 0.9503 - val_f1: 0.9537\n",
      "Epoch 503/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3425 - acc: 0.8627 - f1: 0.8813 - val_loss: 0.2060 - val_acc: 0.9631 - val_f1: 0.9649\n",
      "Epoch 504/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3380 - acc: 0.8660 - f1: 0.8833 - val_loss: 0.2015 - val_acc: 0.9653 - val_f1: 0.9669\n",
      "Epoch 505/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3457 - acc: 0.8608 - f1: 0.8797 - val_loss: 0.2121 - val_acc: 0.9591 - val_f1: 0.9608\n",
      "Epoch 506/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3455 - acc: 0.8605 - f1: 0.8796 - val_loss: 0.2090 - val_acc: 0.9613 - val_f1: 0.9635\n",
      "Epoch 507/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3445 - acc: 0.8607 - f1: 0.8794 - val_loss: 0.2160 - val_acc: 0.9606 - val_f1: 0.9622\n",
      "Epoch 508/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3492 - acc: 0.8605 - f1: 0.8798 - val_loss: 0.2045 - val_acc: 0.9656 - val_f1: 0.9669\n",
      "Epoch 509/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3490 - acc: 0.8590 - f1: 0.8785 - val_loss: 0.2040 - val_acc: 0.9672 - val_f1: 0.9689\n",
      "Epoch 510/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3442 - acc: 0.8619 - f1: 0.8806 - val_loss: 0.2040 - val_acc: 0.9600 - val_f1: 0.9623\n",
      "Epoch 511/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3361 - acc: 0.8680 - f1: 0.8862 - val_loss: 0.2031 - val_acc: 0.9659 - val_f1: 0.9668\n",
      "Epoch 512/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3405 - acc: 0.8644 - f1: 0.8826 - val_loss: 0.2039 - val_acc: 0.9653 - val_f1: 0.9669\n",
      "Epoch 513/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3483 - acc: 0.8598 - f1: 0.8792 - val_loss: 0.2109 - val_acc: 0.9553 - val_f1: 0.9578\n",
      "Epoch 514/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3517 - acc: 0.8596 - f1: 0.8786 - val_loss: 0.2077 - val_acc: 0.9591 - val_f1: 0.9616\n",
      "Epoch 515/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3487 - acc: 0.8597 - f1: 0.8788 - val_loss: 0.2026 - val_acc: 0.9634 - val_f1: 0.9652\n",
      "Epoch 516/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3415 - acc: 0.8659 - f1: 0.8832 - val_loss: 0.2062 - val_acc: 0.9625 - val_f1: 0.9639\n",
      "Epoch 517/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3518 - acc: 0.8566 - f1: 0.8761 - val_loss: 0.2031 - val_acc: 0.9663 - val_f1: 0.9680\n",
      "Epoch 518/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3488 - acc: 0.8620 - f1: 0.8809 - val_loss: 0.2143 - val_acc: 0.9600 - val_f1: 0.9612\n",
      "Epoch 519/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3435 - acc: 0.8621 - f1: 0.8811 - val_loss: 0.2132 - val_acc: 0.9531 - val_f1: 0.9557\n",
      "Epoch 520/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3457 - acc: 0.8610 - f1: 0.8802 - val_loss: 0.2189 - val_acc: 0.9550 - val_f1: 0.9568\n",
      "Epoch 521/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3377 - acc: 0.8651 - f1: 0.8826 - val_loss: 0.2039 - val_acc: 0.9678 - val_f1: 0.9691\n",
      "Epoch 522/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3430 - acc: 0.8627 - f1: 0.8807 - val_loss: 0.2058 - val_acc: 0.9672 - val_f1: 0.9689\n",
      "Epoch 523/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3534 - acc: 0.8572 - f1: 0.8772 - val_loss: 0.2015 - val_acc: 0.9663 - val_f1: 0.9677\n",
      "Epoch 524/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3468 - acc: 0.8615 - f1: 0.8801 - val_loss: 0.2177 - val_acc: 0.9541 - val_f1: 0.9563\n",
      "Epoch 525/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3492 - acc: 0.8598 - f1: 0.8789 - val_loss: 0.2025 - val_acc: 0.9656 - val_f1: 0.9670\n",
      "Epoch 526/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3598 - acc: 0.8547 - f1: 0.8747 - val_loss: 0.2074 - val_acc: 0.9641 - val_f1: 0.9657\n",
      "Epoch 527/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3442 - acc: 0.8622 - f1: 0.8811 - val_loss: 0.2019 - val_acc: 0.9650 - val_f1: 0.9663\n",
      "Epoch 528/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3432 - acc: 0.8630 - f1: 0.8817 - val_loss: 0.2049 - val_acc: 0.9669 - val_f1: 0.9680\n",
      "Epoch 529/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3437 - acc: 0.8624 - f1: 0.8813 - val_loss: 0.2188 - val_acc: 0.9506 - val_f1: 0.9535\n",
      "Epoch 530/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3442 - acc: 0.8618 - f1: 0.8807 - val_loss: 0.2052 - val_acc: 0.9647 - val_f1: 0.9659\n",
      "Epoch 531/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3516 - acc: 0.8571 - f1: 0.8770 - val_loss: 0.1971 - val_acc: 0.9659 - val_f1: 0.9673\n",
      "Epoch 532/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3398 - acc: 0.8647 - f1: 0.8827 - val_loss: 0.2015 - val_acc: 0.9678 - val_f1: 0.9692\n",
      "Epoch 533/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3412 - acc: 0.8646 - f1: 0.8830 - val_loss: 0.2027 - val_acc: 0.9631 - val_f1: 0.9648\n",
      "Epoch 534/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3401 - acc: 0.8649 - f1: 0.8831 - val_loss: 0.2199 - val_acc: 0.9491 - val_f1: 0.9526\n",
      "Epoch 535/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3475 - acc: 0.8603 - f1: 0.8795 - val_loss: 0.1990 - val_acc: 0.9631 - val_f1: 0.9654\n",
      "Epoch 536/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3453 - acc: 0.8606 - f1: 0.8798 - val_loss: 0.2063 - val_acc: 0.9681 - val_f1: 0.9693\n",
      "Epoch 537/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3494 - acc: 0.8592 - f1: 0.8788 - val_loss: 0.2045 - val_acc: 0.9625 - val_f1: 0.9646\n",
      "Epoch 538/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3438 - acc: 0.8634 - f1: 0.8818 - val_loss: 0.2063 - val_acc: 0.9622 - val_f1: 0.9643\n",
      "Epoch 539/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3452 - acc: 0.8600 - f1: 0.8789 - val_loss: 0.2049 - val_acc: 0.9675 - val_f1: 0.9687\n",
      "Epoch 540/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3405 - acc: 0.8645 - f1: 0.8825 - val_loss: 0.2201 - val_acc: 0.9613 - val_f1: 0.9620\n",
      "Epoch 541/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3426 - acc: 0.8627 - f1: 0.8811 - val_loss: 0.2110 - val_acc: 0.9641 - val_f1: 0.9655\n",
      "Epoch 542/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3461 - acc: 0.8618 - f1: 0.8804 - val_loss: 0.2094 - val_acc: 0.9578 - val_f1: 0.9600\n",
      "Epoch 543/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3400 - acc: 0.8650 - f1: 0.8827 - val_loss: 0.2057 - val_acc: 0.9622 - val_f1: 0.9639\n",
      "Epoch 544/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3498 - acc: 0.8575 - f1: 0.8772 - val_loss: 0.2201 - val_acc: 0.9631 - val_f1: 0.9641\n",
      "Epoch 545/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3577 - acc: 0.8556 - f1: 0.8755 - val_loss: 0.2020 - val_acc: 0.9644 - val_f1: 0.9665\n",
      "Epoch 546/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3524 - acc: 0.8571 - f1: 0.8768 - val_loss: 0.2058 - val_acc: 0.9650 - val_f1: 0.9667\n",
      "Epoch 547/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3334 - acc: 0.8680 - f1: 0.8853 - val_loss: 0.2086 - val_acc: 0.9559 - val_f1: 0.9581\n",
      "Epoch 548/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3453 - acc: 0.8618 - f1: 0.8806 - val_loss: 0.2069 - val_acc: 0.9625 - val_f1: 0.9643\n",
      "Epoch 549/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3393 - acc: 0.8644 - f1: 0.8824 - val_loss: 0.2031 - val_acc: 0.9647 - val_f1: 0.9663\n",
      "Epoch 550/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3427 - acc: 0.8633 - f1: 0.8819 - val_loss: 0.2202 - val_acc: 0.9534 - val_f1: 0.9569\n",
      "Epoch 551/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3542 - acc: 0.8579 - f1: 0.8776 - val_loss: 0.2009 - val_acc: 0.9622 - val_f1: 0.9642\n",
      "Epoch 552/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3464 - acc: 0.8607 - f1: 0.8799 - val_loss: 0.2132 - val_acc: 0.9631 - val_f1: 0.9649\n",
      "Epoch 553/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3431 - acc: 0.8628 - f1: 0.8815 - val_loss: 0.2056 - val_acc: 0.9659 - val_f1: 0.9671\n",
      "Epoch 554/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3524 - acc: 0.8584 - f1: 0.8781 - val_loss: 0.2008 - val_acc: 0.9647 - val_f1: 0.9665\n",
      "Epoch 555/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3469 - acc: 0.8611 - f1: 0.8803 - val_loss: 0.2107 - val_acc: 0.9591 - val_f1: 0.9610\n",
      "Epoch 556/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3415 - acc: 0.8631 - f1: 0.8816 - val_loss: 0.2142 - val_acc: 0.9638 - val_f1: 0.9651\n",
      "Epoch 557/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3430 - acc: 0.8627 - f1: 0.8813 - val_loss: 0.2049 - val_acc: 0.9625 - val_f1: 0.9641\n",
      "Epoch 558/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3508 - acc: 0.8578 - f1: 0.8769 - val_loss: 0.2080 - val_acc: 0.9609 - val_f1: 0.9630\n",
      "Epoch 559/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3446 - acc: 0.8612 - f1: 0.8797 - val_loss: 0.2189 - val_acc: 0.9641 - val_f1: 0.9654\n",
      "Epoch 560/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3393 - acc: 0.8659 - f1: 0.8835 - val_loss: 0.2036 - val_acc: 0.9663 - val_f1: 0.9672\n",
      "Epoch 561/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3456 - acc: 0.8609 - f1: 0.8796 - val_loss: 0.2057 - val_acc: 0.9606 - val_f1: 0.9625\n",
      "Epoch 562/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3522 - acc: 0.8581 - f1: 0.8771 - val_loss: 0.2082 - val_acc: 0.9681 - val_f1: 0.9699\n",
      "Epoch 563/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3426 - acc: 0.8625 - f1: 0.8808 - val_loss: 0.2112 - val_acc: 0.9622 - val_f1: 0.9638\n",
      "Epoch 564/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3459 - acc: 0.8607 - f1: 0.8792 - val_loss: 0.2103 - val_acc: 0.9644 - val_f1: 0.9662\n",
      "Epoch 565/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3491 - acc: 0.8588 - f1: 0.8788 - val_loss: 0.2126 - val_acc: 0.9622 - val_f1: 0.9638\n",
      "Epoch 566/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3541 - acc: 0.8555 - f1: 0.8751 - val_loss: 0.2026 - val_acc: 0.9691 - val_f1: 0.9709\n",
      "Epoch 567/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3479 - acc: 0.8609 - f1: 0.8799 - val_loss: 0.2154 - val_acc: 0.9544 - val_f1: 0.9572\n",
      "Epoch 568/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3450 - acc: 0.8601 - f1: 0.8796 - val_loss: 0.2026 - val_acc: 0.9656 - val_f1: 0.9665\n",
      "Epoch 569/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3486 - acc: 0.8613 - f1: 0.8803 - val_loss: 0.2060 - val_acc: 0.9641 - val_f1: 0.9657\n",
      "Epoch 570/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3375 - acc: 0.8664 - f1: 0.8841 - val_loss: 0.2095 - val_acc: 0.9634 - val_f1: 0.9644\n",
      "Epoch 571/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3515 - acc: 0.8573 - f1: 0.8769 - val_loss: 0.2081 - val_acc: 0.9609 - val_f1: 0.9632\n",
      "Epoch 572/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3445 - acc: 0.8613 - f1: 0.8802 - val_loss: 0.2088 - val_acc: 0.9678 - val_f1: 0.9687\n",
      "Epoch 573/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3391 - acc: 0.8645 - f1: 0.8826 - val_loss: 0.2153 - val_acc: 0.9634 - val_f1: 0.9655\n",
      "Epoch 574/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3495 - acc: 0.8609 - f1: 0.8801 - val_loss: 0.2102 - val_acc: 0.9653 - val_f1: 0.9663\n",
      "Epoch 575/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3543 - acc: 0.8557 - f1: 0.8760 - val_loss: 0.1987 - val_acc: 0.9644 - val_f1: 0.9659\n",
      "Epoch 576/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3434 - acc: 0.8620 - f1: 0.8807 - val_loss: 0.2018 - val_acc: 0.9631 - val_f1: 0.9653\n",
      "Epoch 577/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3431 - acc: 0.8623 - f1: 0.8811 - val_loss: 0.2080 - val_acc: 0.9675 - val_f1: 0.9687\n",
      "Epoch 578/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3508 - acc: 0.8590 - f1: 0.8787 - val_loss: 0.2098 - val_acc: 0.9597 - val_f1: 0.9617\n",
      "Epoch 579/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3437 - acc: 0.8626 - f1: 0.8811 - val_loss: 0.2064 - val_acc: 0.9606 - val_f1: 0.9624\n",
      "Epoch 580/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3423 - acc: 0.8629 - f1: 0.8811 - val_loss: 0.1959 - val_acc: 0.9666 - val_f1: 0.9681\n",
      "Epoch 581/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3449 - acc: 0.8616 - f1: 0.8810 - val_loss: 0.2075 - val_acc: 0.9566 - val_f1: 0.9589\n",
      "Epoch 582/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3458 - acc: 0.8609 - f1: 0.8798 - val_loss: 0.2078 - val_acc: 0.9631 - val_f1: 0.9644\n",
      "Epoch 583/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3343 - acc: 0.8672 - f1: 0.8847 - val_loss: 0.2204 - val_acc: 0.9603 - val_f1: 0.9618\n",
      "Epoch 584/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3522 - acc: 0.8572 - f1: 0.8771 - val_loss: 0.2025 - val_acc: 0.9669 - val_f1: 0.9683\n",
      "Epoch 585/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3369 - acc: 0.8656 - f1: 0.8837 - val_loss: 0.2077 - val_acc: 0.9628 - val_f1: 0.9643\n",
      "Epoch 586/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3416 - acc: 0.8621 - f1: 0.8811 - val_loss: 0.2150 - val_acc: 0.9591 - val_f1: 0.9615\n",
      "Epoch 587/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3493 - acc: 0.8581 - f1: 0.8777 - val_loss: 0.2049 - val_acc: 0.9650 - val_f1: 0.9665\n",
      "Epoch 588/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3491 - acc: 0.8583 - f1: 0.8779 - val_loss: 0.2027 - val_acc: 0.9672 - val_f1: 0.9683\n",
      "Epoch 589/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3466 - acc: 0.8608 - f1: 0.8800 - val_loss: 0.2020 - val_acc: 0.9666 - val_f1: 0.9680\n",
      "Epoch 590/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3476 - acc: 0.8597 - f1: 0.8785 - val_loss: 0.2018 - val_acc: 0.9622 - val_f1: 0.9633\n",
      "Epoch 591/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3399 - acc: 0.8641 - f1: 0.8821 - val_loss: 0.2160 - val_acc: 0.9500 - val_f1: 0.9527\n",
      "Epoch 592/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3500 - acc: 0.8575 - f1: 0.8774 - val_loss: 0.2175 - val_acc: 0.9544 - val_f1: 0.9571\n",
      "Epoch 593/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3412 - acc: 0.8639 - f1: 0.8826 - val_loss: 0.2176 - val_acc: 0.9653 - val_f1: 0.9666\n",
      "Epoch 594/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3482 - acc: 0.8588 - f1: 0.8782 - val_loss: 0.2063 - val_acc: 0.9666 - val_f1: 0.9673\n",
      "Epoch 595/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3475 - acc: 0.8595 - f1: 0.8793 - val_loss: 0.2049 - val_acc: 0.9666 - val_f1: 0.9681\n",
      "Epoch 596/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3432 - acc: 0.8623 - f1: 0.8809 - val_loss: 0.2066 - val_acc: 0.9603 - val_f1: 0.9624\n",
      "Epoch 597/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3509 - acc: 0.8563 - f1: 0.8763 - val_loss: 0.2093 - val_acc: 0.9650 - val_f1: 0.9663\n",
      "Epoch 598/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3433 - acc: 0.8617 - f1: 0.8806 - val_loss: 0.2128 - val_acc: 0.9594 - val_f1: 0.9614\n",
      "Epoch 599/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3403 - acc: 0.8652 - f1: 0.8832 - val_loss: 0.2123 - val_acc: 0.9625 - val_f1: 0.9640\n",
      "Epoch 600/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3394 - acc: 0.8651 - f1: 0.8835 - val_loss: 0.2201 - val_acc: 0.9641 - val_f1: 0.9654\n",
      "Epoch 601/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3463 - acc: 0.8611 - f1: 0.8796 - val_loss: 0.2020 - val_acc: 0.9656 - val_f1: 0.9671\n",
      "Epoch 602/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3408 - acc: 0.8627 - f1: 0.8812 - val_loss: 0.2145 - val_acc: 0.9559 - val_f1: 0.9581\n",
      "Epoch 603/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3351 - acc: 0.8682 - f1: 0.8860 - val_loss: 0.2090 - val_acc: 0.9622 - val_f1: 0.9646\n",
      "Epoch 604/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3455 - acc: 0.8605 - f1: 0.8796 - val_loss: 0.2032 - val_acc: 0.9616 - val_f1: 0.9638\n",
      "Epoch 605/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3384 - acc: 0.8643 - f1: 0.8823 - val_loss: 0.2001 - val_acc: 0.9663 - val_f1: 0.9675\n",
      "Epoch 606/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3436 - acc: 0.8609 - f1: 0.8802 - val_loss: 0.2128 - val_acc: 0.9588 - val_f1: 0.9602\n",
      "Epoch 607/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3556 - acc: 0.8575 - f1: 0.8772 - val_loss: 0.2096 - val_acc: 0.9616 - val_f1: 0.9630\n",
      "Epoch 608/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3502 - acc: 0.8585 - f1: 0.8779 - val_loss: 0.2016 - val_acc: 0.9672 - val_f1: 0.9684\n",
      "Epoch 609/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3349 - acc: 0.8674 - f1: 0.8849 - val_loss: 0.1988 - val_acc: 0.9634 - val_f1: 0.9649\n",
      "Epoch 610/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3395 - acc: 0.8654 - f1: 0.8834 - val_loss: 0.2065 - val_acc: 0.9603 - val_f1: 0.9622\n",
      "Epoch 611/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3362 - acc: 0.8673 - f1: 0.8845 - val_loss: 0.2080 - val_acc: 0.9644 - val_f1: 0.9653\n",
      "Epoch 612/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3439 - acc: 0.8617 - f1: 0.8806 - val_loss: 0.2001 - val_acc: 0.9656 - val_f1: 0.9674\n",
      "Epoch 613/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3416 - acc: 0.8627 - f1: 0.8811 - val_loss: 0.2049 - val_acc: 0.9613 - val_f1: 0.9632\n",
      "Epoch 614/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3364 - acc: 0.8673 - f1: 0.8841 - val_loss: 0.2108 - val_acc: 0.9525 - val_f1: 0.9554\n",
      "Epoch 615/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3447 - acc: 0.8613 - f1: 0.8802 - val_loss: 0.1988 - val_acc: 0.9644 - val_f1: 0.9659\n",
      "Epoch 616/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3379 - acc: 0.8642 - f1: 0.8825 - val_loss: 0.2109 - val_acc: 0.9541 - val_f1: 0.9574\n",
      "Epoch 617/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3472 - acc: 0.8610 - f1: 0.8802 - val_loss: 0.2082 - val_acc: 0.9647 - val_f1: 0.9664\n",
      "Epoch 618/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3392 - acc: 0.8650 - f1: 0.8829 - val_loss: 0.2112 - val_acc: 0.9581 - val_f1: 0.9603\n",
      "Epoch 619/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3417 - acc: 0.8636 - f1: 0.8821 - val_loss: 0.2054 - val_acc: 0.9584 - val_f1: 0.9605\n",
      "Epoch 620/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3475 - acc: 0.8599 - f1: 0.8795 - val_loss: 0.2136 - val_acc: 0.9581 - val_f1: 0.9607\n",
      "Epoch 621/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3454 - acc: 0.8612 - f1: 0.8800 - val_loss: 0.2098 - val_acc: 0.9631 - val_f1: 0.9644\n",
      "Epoch 622/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3390 - acc: 0.8650 - f1: 0.8830 - val_loss: 0.2060 - val_acc: 0.9656 - val_f1: 0.9669\n",
      "Epoch 623/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3469 - acc: 0.8598 - f1: 0.8791 - val_loss: 0.2014 - val_acc: 0.9653 - val_f1: 0.9668\n",
      "Epoch 624/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3376 - acc: 0.8655 - f1: 0.8835 - val_loss: 0.2140 - val_acc: 0.9653 - val_f1: 0.9663\n",
      "Epoch 625/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3499 - acc: 0.8605 - f1: 0.8795 - val_loss: 0.2090 - val_acc: 0.9644 - val_f1: 0.9662\n",
      "Epoch 626/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3381 - acc: 0.8645 - f1: 0.8827 - val_loss: 0.2127 - val_acc: 0.9675 - val_f1: 0.9685\n",
      "Epoch 627/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3547 - acc: 0.8560 - f1: 0.8760 - val_loss: 0.2078 - val_acc: 0.9591 - val_f1: 0.9614\n",
      "Epoch 628/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3422 - acc: 0.8628 - f1: 0.8818 - val_loss: 0.2063 - val_acc: 0.9619 - val_f1: 0.9633\n",
      "Epoch 629/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3454 - acc: 0.8613 - f1: 0.8805 - val_loss: 0.2118 - val_acc: 0.9619 - val_f1: 0.9635\n",
      "Epoch 630/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3390 - acc: 0.8643 - f1: 0.8826 - val_loss: 0.2108 - val_acc: 0.9581 - val_f1: 0.9597\n",
      "Epoch 631/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3442 - acc: 0.8619 - f1: 0.8806 - val_loss: 0.2102 - val_acc: 0.9619 - val_f1: 0.9634\n",
      "Epoch 632/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3411 - acc: 0.8644 - f1: 0.8827 - val_loss: 0.2183 - val_acc: 0.9678 - val_f1: 0.9690\n",
      "Epoch 633/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3524 - acc: 0.8569 - f1: 0.8767 - val_loss: 0.2106 - val_acc: 0.9656 - val_f1: 0.9669\n",
      "Epoch 634/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3380 - acc: 0.8648 - f1: 0.8828 - val_loss: 0.2122 - val_acc: 0.9578 - val_f1: 0.9601\n",
      "Epoch 635/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3473 - acc: 0.8583 - f1: 0.8777 - val_loss: 0.2080 - val_acc: 0.9638 - val_f1: 0.9652\n",
      "Epoch 636/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3455 - acc: 0.8610 - f1: 0.8797 - val_loss: 0.2053 - val_acc: 0.9672 - val_f1: 0.9681\n",
      "Epoch 637/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3365 - acc: 0.8661 - f1: 0.8837 - val_loss: 0.2012 - val_acc: 0.9672 - val_f1: 0.9687\n",
      "Epoch 638/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3348 - acc: 0.8664 - f1: 0.8839 - val_loss: 0.2056 - val_acc: 0.9678 - val_f1: 0.9692\n",
      "Epoch 639/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3407 - acc: 0.8648 - f1: 0.8831 - val_loss: 0.2093 - val_acc: 0.9597 - val_f1: 0.9621\n",
      "Epoch 640/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3428 - acc: 0.8637 - f1: 0.8819 - val_loss: 0.2113 - val_acc: 0.9644 - val_f1: 0.9655\n",
      "Epoch 641/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3436 - acc: 0.8611 - f1: 0.8800 - val_loss: 0.2075 - val_acc: 0.9663 - val_f1: 0.9677\n",
      "Epoch 642/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3444 - acc: 0.8609 - f1: 0.8798 - val_loss: 0.2157 - val_acc: 0.9556 - val_f1: 0.9579\n",
      "Epoch 643/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3355 - acc: 0.8680 - f1: 0.8852 - val_loss: 0.2103 - val_acc: 0.9569 - val_f1: 0.9595\n",
      "Epoch 644/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3442 - acc: 0.8605 - f1: 0.8800 - val_loss: 0.2059 - val_acc: 0.9663 - val_f1: 0.9682\n",
      "Epoch 645/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3440 - acc: 0.8621 - f1: 0.8811 - val_loss: 0.2052 - val_acc: 0.9634 - val_f1: 0.9651\n",
      "Epoch 646/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3400 - acc: 0.8631 - f1: 0.8817 - val_loss: 0.2182 - val_acc: 0.9641 - val_f1: 0.9655\n",
      "Epoch 647/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3474 - acc: 0.8595 - f1: 0.8788 - val_loss: 0.2089 - val_acc: 0.9628 - val_f1: 0.9649\n",
      "Epoch 648/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3362 - acc: 0.8656 - f1: 0.8838 - val_loss: 0.2093 - val_acc: 0.9566 - val_f1: 0.9594\n",
      "Epoch 649/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3413 - acc: 0.8634 - f1: 0.8809 - val_loss: 0.2094 - val_acc: 0.9609 - val_f1: 0.9624\n",
      "Epoch 650/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3424 - acc: 0.8624 - f1: 0.8815 - val_loss: 0.2016 - val_acc: 0.9584 - val_f1: 0.9604\n",
      "Epoch 651/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3505 - acc: 0.8571 - f1: 0.8770 - val_loss: 0.2094 - val_acc: 0.9678 - val_f1: 0.9693\n",
      "Epoch 652/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3463 - acc: 0.8613 - f1: 0.8802 - val_loss: 0.2180 - val_acc: 0.9647 - val_f1: 0.9661\n",
      "Epoch 653/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3420 - acc: 0.8630 - f1: 0.8820 - val_loss: 0.2090 - val_acc: 0.9663 - val_f1: 0.9674\n",
      "Epoch 654/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3465 - acc: 0.8595 - f1: 0.8784 - val_loss: 0.2023 - val_acc: 0.9684 - val_f1: 0.9696\n",
      "Epoch 655/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3465 - acc: 0.8618 - f1: 0.8802 - val_loss: 0.2176 - val_acc: 0.9625 - val_f1: 0.9644\n",
      "Epoch 656/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3430 - acc: 0.8625 - f1: 0.8811 - val_loss: 0.2065 - val_acc: 0.9672 - val_f1: 0.9680\n",
      "Epoch 657/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3307 - acc: 0.8695 - f1: 0.8868 - val_loss: 0.2076 - val_acc: 0.9681 - val_f1: 0.9690\n",
      "Epoch 658/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3382 - acc: 0.8653 - f1: 0.8837 - val_loss: 0.2153 - val_acc: 0.9641 - val_f1: 0.9649\n",
      "Epoch 659/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3509 - acc: 0.8579 - f1: 0.8776 - val_loss: 0.2004 - val_acc: 0.9691 - val_f1: 0.9705\n",
      "Epoch 660/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3431 - acc: 0.8613 - f1: 0.8802 - val_loss: 0.2033 - val_acc: 0.9650 - val_f1: 0.9663\n",
      "Epoch 661/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3436 - acc: 0.8617 - f1: 0.8804 - val_loss: 0.2092 - val_acc: 0.9634 - val_f1: 0.9651\n",
      "Epoch 662/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3502 - acc: 0.8579 - f1: 0.8776 - val_loss: 0.2066 - val_acc: 0.9631 - val_f1: 0.9644\n",
      "Epoch 663/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3403 - acc: 0.8634 - f1: 0.8815 - val_loss: 0.2229 - val_acc: 0.9631 - val_f1: 0.9649\n",
      "Epoch 664/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3336 - acc: 0.8679 - f1: 0.8852 - val_loss: 0.2098 - val_acc: 0.9644 - val_f1: 0.9656\n",
      "Epoch 665/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3444 - acc: 0.8613 - f1: 0.8807 - val_loss: 0.2049 - val_acc: 0.9588 - val_f1: 0.9605\n",
      "Epoch 666/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 0s 17us/sample - loss: 0.3454 - acc: 0.8637 - f1: 0.8820 - val_loss: 0.2056 - val_acc: 0.9638 - val_f1: 0.9657\n",
      "Epoch 667/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3424 - acc: 0.8623 - f1: 0.8809 - val_loss: 0.2041 - val_acc: 0.9616 - val_f1: 0.9633\n",
      "Epoch 668/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3435 - acc: 0.8605 - f1: 0.8795 - val_loss: 0.2101 - val_acc: 0.9644 - val_f1: 0.9658\n",
      "Epoch 669/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3468 - acc: 0.8598 - f1: 0.8789 - val_loss: 0.2064 - val_acc: 0.9644 - val_f1: 0.9659\n",
      "Epoch 670/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3410 - acc: 0.8648 - f1: 0.8828 - val_loss: 0.2063 - val_acc: 0.9666 - val_f1: 0.9679\n",
      "Epoch 671/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3471 - acc: 0.8607 - f1: 0.8795 - val_loss: 0.2123 - val_acc: 0.9625 - val_f1: 0.9638\n",
      "Epoch 672/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3442 - acc: 0.8620 - f1: 0.8802 - val_loss: 0.2204 - val_acc: 0.9547 - val_f1: 0.9575\n",
      "Epoch 673/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3483 - acc: 0.8575 - f1: 0.8769 - val_loss: 0.2173 - val_acc: 0.9659 - val_f1: 0.9677\n",
      "Epoch 674/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3497 - acc: 0.8576 - f1: 0.8772 - val_loss: 0.2086 - val_acc: 0.9616 - val_f1: 0.9637\n",
      "Epoch 675/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3454 - acc: 0.8600 - f1: 0.8795 - val_loss: 0.2181 - val_acc: 0.9656 - val_f1: 0.9666\n",
      "Epoch 676/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3476 - acc: 0.8609 - f1: 0.8800 - val_loss: 0.2124 - val_acc: 0.9563 - val_f1: 0.9584\n",
      "Epoch 677/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3391 - acc: 0.8642 - f1: 0.8823 - val_loss: 0.2179 - val_acc: 0.9528 - val_f1: 0.9553\n",
      "Epoch 678/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3570 - acc: 0.8548 - f1: 0.8750 - val_loss: 0.2123 - val_acc: 0.9606 - val_f1: 0.9626\n",
      "Epoch 679/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3364 - acc: 0.8660 - f1: 0.8838 - val_loss: 0.2081 - val_acc: 0.9650 - val_f1: 0.9669\n",
      "Epoch 680/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3472 - acc: 0.8587 - f1: 0.8783 - val_loss: 0.2117 - val_acc: 0.9622 - val_f1: 0.9628\n",
      "Epoch 681/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3349 - acc: 0.8668 - f1: 0.8844 - val_loss: 0.2094 - val_acc: 0.9622 - val_f1: 0.9645\n",
      "Epoch 682/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3305 - acc: 0.8689 - f1: 0.8858 - val_loss: 0.2081 - val_acc: 0.9575 - val_f1: 0.9597\n",
      "Epoch 683/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3578 - acc: 0.8533 - f1: 0.8747 - val_loss: 0.2173 - val_acc: 0.9641 - val_f1: 0.9654\n",
      "Epoch 684/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3469 - acc: 0.8594 - f1: 0.8791 - val_loss: 0.2300 - val_acc: 0.9628 - val_f1: 0.9639\n",
      "Epoch 685/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3442 - acc: 0.8614 - f1: 0.8805 - val_loss: 0.2108 - val_acc: 0.9672 - val_f1: 0.9681\n",
      "Epoch 686/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3331 - acc: 0.8684 - f1: 0.8859 - val_loss: 0.2322 - val_acc: 0.9631 - val_f1: 0.9646\n",
      "Epoch 687/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3406 - acc: 0.8636 - f1: 0.8819 - val_loss: 0.2027 - val_acc: 0.9644 - val_f1: 0.9661\n",
      "Epoch 688/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3456 - acc: 0.8605 - f1: 0.8798 - val_loss: 0.2026 - val_acc: 0.9647 - val_f1: 0.9663\n",
      "Epoch 689/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3491 - acc: 0.8598 - f1: 0.8790 - val_loss: 0.2111 - val_acc: 0.9584 - val_f1: 0.9607\n",
      "Epoch 690/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3445 - acc: 0.8605 - f1: 0.8796 - val_loss: 0.2281 - val_acc: 0.9606 - val_f1: 0.9611\n",
      "Epoch 691/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3403 - acc: 0.8642 - f1: 0.8828 - val_loss: 0.2058 - val_acc: 0.9647 - val_f1: 0.9664\n",
      "Epoch 692/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3438 - acc: 0.8620 - f1: 0.8807 - val_loss: 0.2043 - val_acc: 0.9653 - val_f1: 0.9669\n",
      "Epoch 693/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3461 - acc: 0.8593 - f1: 0.8779 - val_loss: 0.2106 - val_acc: 0.9644 - val_f1: 0.9661\n",
      "Epoch 694/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3448 - acc: 0.8611 - f1: 0.8798 - val_loss: 0.2048 - val_acc: 0.9647 - val_f1: 0.9656\n",
      "Epoch 695/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3399 - acc: 0.8642 - f1: 0.8823 - val_loss: 0.2227 - val_acc: 0.9672 - val_f1: 0.9681\n",
      "Epoch 696/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3451 - acc: 0.8612 - f1: 0.8806 - val_loss: 0.2143 - val_acc: 0.9544 - val_f1: 0.9568\n",
      "Epoch 697/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3390 - acc: 0.8641 - f1: 0.8821 - val_loss: 0.2110 - val_acc: 0.9647 - val_f1: 0.9661\n",
      "Epoch 698/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3500 - acc: 0.8580 - f1: 0.8779 - val_loss: 0.2070 - val_acc: 0.9613 - val_f1: 0.9627\n",
      "Epoch 699/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3359 - acc: 0.8658 - f1: 0.8835 - val_loss: 0.2074 - val_acc: 0.9613 - val_f1: 0.9633\n",
      "Epoch 700/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3428 - acc: 0.8619 - f1: 0.8811 - val_loss: 0.2116 - val_acc: 0.9616 - val_f1: 0.9624\n",
      "Epoch 701/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3387 - acc: 0.8657 - f1: 0.8838 - val_loss: 0.2120 - val_acc: 0.9659 - val_f1: 0.9667\n",
      "Epoch 702/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3444 - acc: 0.8606 - f1: 0.8802 - val_loss: 0.2116 - val_acc: 0.9638 - val_f1: 0.9651\n",
      "Epoch 703/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3462 - acc: 0.8609 - f1: 0.8804 - val_loss: 0.2088 - val_acc: 0.9650 - val_f1: 0.9665\n",
      "Epoch 704/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3521 - acc: 0.8564 - f1: 0.8765 - val_loss: 0.2055 - val_acc: 0.9634 - val_f1: 0.9654\n",
      "Epoch 705/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3441 - acc: 0.8609 - f1: 0.8806 - val_loss: 0.2096 - val_acc: 0.9672 - val_f1: 0.9680\n",
      "Epoch 706/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3414 - acc: 0.8621 - f1: 0.8808 - val_loss: 0.2129 - val_acc: 0.9594 - val_f1: 0.9612\n",
      "Epoch 707/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3315 - acc: 0.8691 - f1: 0.8863 - val_loss: 0.2059 - val_acc: 0.9675 - val_f1: 0.9685\n",
      "Epoch 708/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3376 - acc: 0.8658 - f1: 0.8838 - val_loss: 0.2105 - val_acc: 0.9616 - val_f1: 0.9634\n",
      "Epoch 709/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3380 - acc: 0.8652 - f1: 0.8834 - val_loss: 0.2103 - val_acc: 0.9584 - val_f1: 0.9609\n",
      "Epoch 710/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3406 - acc: 0.8634 - f1: 0.8820 - val_loss: 0.2087 - val_acc: 0.9653 - val_f1: 0.9667\n",
      "Epoch 711/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3397 - acc: 0.8635 - f1: 0.8818 - val_loss: 0.2162 - val_acc: 0.9663 - val_f1: 0.9676\n",
      "Epoch 712/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3458 - acc: 0.8591 - f1: 0.8783 - val_loss: 0.2061 - val_acc: 0.9672 - val_f1: 0.9685\n",
      "Epoch 713/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3423 - acc: 0.8632 - f1: 0.8812 - val_loss: 0.2132 - val_acc: 0.9603 - val_f1: 0.9619\n",
      "Epoch 714/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3382 - acc: 0.8645 - f1: 0.8829 - val_loss: 0.2092 - val_acc: 0.9663 - val_f1: 0.9677\n",
      "Epoch 715/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3427 - acc: 0.8612 - f1: 0.8800 - val_loss: 0.2099 - val_acc: 0.9578 - val_f1: 0.9607\n",
      "Epoch 716/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3488 - acc: 0.8598 - f1: 0.8794 - val_loss: 0.2114 - val_acc: 0.9600 - val_f1: 0.9622\n",
      "Epoch 717/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3441 - acc: 0.8616 - f1: 0.8807 - val_loss: 0.2126 - val_acc: 0.9616 - val_f1: 0.9631\n",
      "Epoch 718/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3468 - acc: 0.8594 - f1: 0.8787 - val_loss: 0.2161 - val_acc: 0.9619 - val_f1: 0.9636\n",
      "Epoch 719/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3357 - acc: 0.8663 - f1: 0.8842 - val_loss: 0.2010 - val_acc: 0.9678 - val_f1: 0.9692\n",
      "Epoch 720/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3373 - acc: 0.8660 - f1: 0.8841 - val_loss: 0.2086 - val_acc: 0.9600 - val_f1: 0.9620\n",
      "Epoch 721/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3386 - acc: 0.8627 - f1: 0.8811 - val_loss: 0.2229 - val_acc: 0.9650 - val_f1: 0.9665\n",
      "Epoch 722/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3398 - acc: 0.8631 - f1: 0.8818 - val_loss: 0.2143 - val_acc: 0.9609 - val_f1: 0.9634\n",
      "Epoch 723/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3507 - acc: 0.8567 - f1: 0.8771 - val_loss: 0.2132 - val_acc: 0.9625 - val_f1: 0.9642\n",
      "Epoch 724/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3483 - acc: 0.8595 - f1: 0.8783 - val_loss: 0.2040 - val_acc: 0.9628 - val_f1: 0.9651\n",
      "Epoch 725/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3382 - acc: 0.8657 - f1: 0.8839 - val_loss: 0.2056 - val_acc: 0.9656 - val_f1: 0.9677\n",
      "Epoch 726/5000\n",
      "12800/12800 [==============================] - ETA: 0s - loss: 0.3358 - acc: 0.8646 - f1: 0.88 - 0s 16us/sample - loss: 0.3361 - acc: 0.8646 - f1: 0.8831 - val_loss: 0.2123 - val_acc: 0.9663 - val_f1: 0.9669\n",
      "Epoch 727/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3379 - acc: 0.8649 - f1: 0.8828 - val_loss: 0.2113 - val_acc: 0.9650 - val_f1: 0.9665\n",
      "Epoch 728/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3466 - acc: 0.8602 - f1: 0.8791 - val_loss: 0.2064 - val_acc: 0.9650 - val_f1: 0.9666\n",
      "Epoch 729/5000\n",
      "12800/12800 [==============================] - ETA: 0s - loss: 0.3473 - acc: 0.8599 - f1: 0.88 - 0s 16us/sample - loss: 0.3463 - acc: 0.8598 - f1: 0.8795 - val_loss: 0.2076 - val_acc: 0.9631 - val_f1: 0.9645\n",
      "Epoch 730/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3394 - acc: 0.8641 - f1: 0.8828 - val_loss: 0.2140 - val_acc: 0.9659 - val_f1: 0.9678\n",
      "Epoch 731/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3401 - acc: 0.8628 - f1: 0.8816 - val_loss: 0.2138 - val_acc: 0.9594 - val_f1: 0.9620\n",
      "Epoch 732/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3395 - acc: 0.8650 - f1: 0.8829 - val_loss: 0.2074 - val_acc: 0.9631 - val_f1: 0.9650\n",
      "Epoch 733/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3421 - acc: 0.8629 - f1: 0.8817 - val_loss: 0.2089 - val_acc: 0.9553 - val_f1: 0.9575\n",
      "Epoch 734/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3456 - acc: 0.8620 - f1: 0.8811 - val_loss: 0.2130 - val_acc: 0.9663 - val_f1: 0.9664\n",
      "Epoch 735/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3448 - acc: 0.8608 - f1: 0.8797 - val_loss: 0.2120 - val_acc: 0.9619 - val_f1: 0.9630\n",
      "Epoch 736/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3388 - acc: 0.8645 - f1: 0.8833 - val_loss: 0.2150 - val_acc: 0.9669 - val_f1: 0.9681\n",
      "Epoch 737/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3505 - acc: 0.8591 - f1: 0.8785 - val_loss: 0.2220 - val_acc: 0.9669 - val_f1: 0.9681\n",
      "Epoch 738/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3430 - acc: 0.8628 - f1: 0.8814 - val_loss: 0.2072 - val_acc: 0.9613 - val_f1: 0.9626\n",
      "Epoch 739/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3433 - acc: 0.8613 - f1: 0.8801 - val_loss: 0.2090 - val_acc: 0.9644 - val_f1: 0.9658\n",
      "Epoch 740/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3383 - acc: 0.8648 - f1: 0.8830 - val_loss: 0.2016 - val_acc: 0.9606 - val_f1: 0.9632\n",
      "Epoch 741/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3420 - acc: 0.8630 - f1: 0.8813 - val_loss: 0.2057 - val_acc: 0.9634 - val_f1: 0.9656\n",
      "Epoch 742/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3516 - acc: 0.8570 - f1: 0.8772 - val_loss: 0.2088 - val_acc: 0.9659 - val_f1: 0.9673\n",
      "Epoch 743/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3493 - acc: 0.8577 - f1: 0.8772 - val_loss: 0.2099 - val_acc: 0.9638 - val_f1: 0.9660\n",
      "Epoch 744/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3414 - acc: 0.8621 - f1: 0.8810 - val_loss: 0.2114 - val_acc: 0.9669 - val_f1: 0.9682\n",
      "Epoch 745/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3369 - acc: 0.8645 - f1: 0.8825 - val_loss: 0.2222 - val_acc: 0.9628 - val_f1: 0.9641\n",
      "Epoch 746/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3398 - acc: 0.8653 - f1: 0.8829 - val_loss: 0.2226 - val_acc: 0.9622 - val_f1: 0.9630\n",
      "Epoch 747/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3384 - acc: 0.8639 - f1: 0.8825 - val_loss: 0.2126 - val_acc: 0.9672 - val_f1: 0.9683\n",
      "Epoch 748/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3412 - acc: 0.8639 - f1: 0.8825 - val_loss: 0.2273 - val_acc: 0.9447 - val_f1: 0.9485\n",
      "Epoch 749/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3399 - acc: 0.8652 - f1: 0.8826 - val_loss: 0.2140 - val_acc: 0.9631 - val_f1: 0.9643\n",
      "Epoch 750/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3457 - acc: 0.8607 - f1: 0.8797 - val_loss: 0.2130 - val_acc: 0.9634 - val_f1: 0.9640\n",
      "Epoch 751/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3393 - acc: 0.8641 - f1: 0.8822 - val_loss: 0.2094 - val_acc: 0.9647 - val_f1: 0.9659\n",
      "Epoch 752/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3358 - acc: 0.8677 - f1: 0.8854 - val_loss: 0.2082 - val_acc: 0.9634 - val_f1: 0.9650\n",
      "Epoch 753/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3468 - acc: 0.8609 - f1: 0.8797 - val_loss: 0.2090 - val_acc: 0.9650 - val_f1: 0.9663\n",
      "Epoch 754/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3368 - acc: 0.8649 - f1: 0.8830 - val_loss: 0.2142 - val_acc: 0.9606 - val_f1: 0.9618\n",
      "Epoch 755/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3362 - acc: 0.8662 - f1: 0.8841 - val_loss: 0.2098 - val_acc: 0.9688 - val_f1: 0.9706\n",
      "Epoch 756/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3372 - acc: 0.8660 - f1: 0.8836 - val_loss: 0.2222 - val_acc: 0.9531 - val_f1: 0.9560\n",
      "Epoch 757/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3486 - acc: 0.8590 - f1: 0.8787 - val_loss: 0.2150 - val_acc: 0.9578 - val_f1: 0.9603\n",
      "Epoch 758/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3536 - acc: 0.8563 - f1: 0.8762 - val_loss: 0.2097 - val_acc: 0.9625 - val_f1: 0.9643\n",
      "Epoch 759/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3386 - acc: 0.8641 - f1: 0.8821 - val_loss: 0.2199 - val_acc: 0.9522 - val_f1: 0.9548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 760/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3415 - acc: 0.8642 - f1: 0.8824 - val_loss: 0.2088 - val_acc: 0.9619 - val_f1: 0.9638\n",
      "Epoch 761/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3380 - acc: 0.8659 - f1: 0.8840 - val_loss: 0.2097 - val_acc: 0.9631 - val_f1: 0.9646\n",
      "Epoch 762/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3370 - acc: 0.8660 - f1: 0.8841 - val_loss: 0.2050 - val_acc: 0.9638 - val_f1: 0.9654\n",
      "Epoch 763/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3415 - acc: 0.8625 - f1: 0.8814 - val_loss: 0.2122 - val_acc: 0.9613 - val_f1: 0.9629\n",
      "Epoch 764/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3384 - acc: 0.8647 - f1: 0.8832 - val_loss: 0.2053 - val_acc: 0.9650 - val_f1: 0.9664\n",
      "Epoch 765/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3384 - acc: 0.8645 - f1: 0.8829 - val_loss: 0.2059 - val_acc: 0.9584 - val_f1: 0.9600\n",
      "Epoch 766/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3412 - acc: 0.8630 - f1: 0.8818 - val_loss: 0.2144 - val_acc: 0.9634 - val_f1: 0.9645\n",
      "Epoch 767/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3432 - acc: 0.8639 - f1: 0.8828 - val_loss: 0.2116 - val_acc: 0.9659 - val_f1: 0.9669\n",
      "Epoch 768/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3445 - acc: 0.8611 - f1: 0.8801 - val_loss: 0.2278 - val_acc: 0.9556 - val_f1: 0.9575\n",
      "Epoch 769/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3406 - acc: 0.8638 - f1: 0.8819 - val_loss: 0.2096 - val_acc: 0.9644 - val_f1: 0.9657\n",
      "Epoch 770/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3445 - acc: 0.8593 - f1: 0.8791 - val_loss: 0.2141 - val_acc: 0.9653 - val_f1: 0.9671\n",
      "Epoch 771/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3398 - acc: 0.8639 - f1: 0.8815 - val_loss: 0.2209 - val_acc: 0.9641 - val_f1: 0.9654\n",
      "Epoch 772/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3348 - acc: 0.8669 - f1: 0.8850 - val_loss: 0.2400 - val_acc: 0.9606 - val_f1: 0.9614\n",
      "Epoch 773/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3333 - acc: 0.8692 - f1: 0.8862 - val_loss: 0.2163 - val_acc: 0.9634 - val_f1: 0.9645\n",
      "Epoch 774/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3362 - acc: 0.8658 - f1: 0.8837 - val_loss: 0.2201 - val_acc: 0.9553 - val_f1: 0.9577\n",
      "Epoch 775/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3411 - acc: 0.8639 - f1: 0.8821 - val_loss: 0.2156 - val_acc: 0.9522 - val_f1: 0.9550\n",
      "Epoch 776/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3457 - acc: 0.8615 - f1: 0.8803 - val_loss: 0.2063 - val_acc: 0.9619 - val_f1: 0.9637\n",
      "Epoch 777/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3460 - acc: 0.8602 - f1: 0.8794 - val_loss: 0.2038 - val_acc: 0.9669 - val_f1: 0.9688\n",
      "Epoch 778/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3430 - acc: 0.8609 - f1: 0.8799 - val_loss: 0.2095 - val_acc: 0.9678 - val_f1: 0.9682\n",
      "Epoch 779/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3372 - acc: 0.8651 - f1: 0.8836 - val_loss: 0.2147 - val_acc: 0.9650 - val_f1: 0.9660\n",
      "Epoch 780/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3456 - acc: 0.8607 - f1: 0.8798 - val_loss: 0.2081 - val_acc: 0.9597 - val_f1: 0.9616\n",
      "Epoch 781/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3427 - acc: 0.8625 - f1: 0.8812 - val_loss: 0.2057 - val_acc: 0.9653 - val_f1: 0.9668\n",
      "Epoch 782/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3345 - acc: 0.8677 - f1: 0.8857 - val_loss: 0.2053 - val_acc: 0.9678 - val_f1: 0.9688\n",
      "Epoch 783/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3377 - acc: 0.8647 - f1: 0.8828 - val_loss: 0.2080 - val_acc: 0.9625 - val_f1: 0.9644\n",
      "Epoch 784/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3349 - acc: 0.8682 - f1: 0.8856 - val_loss: 0.2075 - val_acc: 0.9656 - val_f1: 0.9673\n",
      "Epoch 785/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3402 - acc: 0.8628 - f1: 0.8817 - val_loss: 0.2137 - val_acc: 0.9634 - val_f1: 0.9651\n",
      "Epoch 786/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3423 - acc: 0.8643 - f1: 0.8826 - val_loss: 0.2162 - val_acc: 0.9556 - val_f1: 0.9576\n",
      "Epoch 787/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3467 - acc: 0.8603 - f1: 0.8797 - val_loss: 0.2202 - val_acc: 0.9578 - val_f1: 0.9601\n",
      "Epoch 788/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3358 - acc: 0.8670 - f1: 0.8850 - val_loss: 0.2110 - val_acc: 0.9650 - val_f1: 0.9667\n",
      "Epoch 789/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3328 - acc: 0.8677 - f1: 0.8852 - val_loss: 0.2072 - val_acc: 0.9619 - val_f1: 0.9637\n",
      "Epoch 790/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3503 - acc: 0.8570 - f1: 0.8772 - val_loss: 0.2230 - val_acc: 0.9641 - val_f1: 0.9650\n",
      "Epoch 791/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3425 - acc: 0.8612 - f1: 0.8802 - val_loss: 0.2142 - val_acc: 0.9678 - val_f1: 0.9694\n",
      "Epoch 792/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3458 - acc: 0.8618 - f1: 0.8809 - val_loss: 0.2155 - val_acc: 0.9606 - val_f1: 0.9623\n",
      "Epoch 793/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3530 - acc: 0.8580 - f1: 0.8771 - val_loss: 0.2197 - val_acc: 0.9663 - val_f1: 0.9668\n",
      "Epoch 794/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3418 - acc: 0.8625 - f1: 0.8814 - val_loss: 0.2045 - val_acc: 0.9653 - val_f1: 0.9670\n",
      "Epoch 795/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3387 - acc: 0.8650 - f1: 0.8834 - val_loss: 0.2081 - val_acc: 0.9616 - val_f1: 0.9634\n",
      "Epoch 796/5000\n",
      "12800/12800 [==============================] - ETA: 0s - loss: 0.3406 - acc: 0.8624 - f1: 0.88 - 0s 16us/sample - loss: 0.3411 - acc: 0.8625 - f1: 0.8815 - val_loss: 0.2230 - val_acc: 0.9625 - val_f1: 0.9635\n",
      "Epoch 797/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3393 - acc: 0.8642 - f1: 0.8821 - val_loss: 0.2250 - val_acc: 0.9609 - val_f1: 0.9622\n",
      "Epoch 798/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3457 - acc: 0.8612 - f1: 0.8806 - val_loss: 0.2124 - val_acc: 0.9650 - val_f1: 0.9666\n",
      "Epoch 799/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3485 - acc: 0.8587 - f1: 0.8783 - val_loss: 0.2104 - val_acc: 0.9675 - val_f1: 0.9682\n",
      "Epoch 800/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3477 - acc: 0.8592 - f1: 0.8787 - val_loss: 0.2077 - val_acc: 0.9688 - val_f1: 0.9699\n",
      "Epoch 801/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3440 - acc: 0.8618 - f1: 0.8802 - val_loss: 0.2086 - val_acc: 0.9666 - val_f1: 0.9679\n",
      "Epoch 802/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3386 - acc: 0.8654 - f1: 0.8827 - val_loss: 0.2181 - val_acc: 0.9644 - val_f1: 0.9657\n",
      "Epoch 803/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3434 - acc: 0.8616 - f1: 0.8807 - val_loss: 0.2177 - val_acc: 0.9606 - val_f1: 0.9625\n",
      "Epoch 804/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3420 - acc: 0.8622 - f1: 0.8815 - val_loss: 0.2085 - val_acc: 0.9663 - val_f1: 0.9675\n",
      "Epoch 805/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3367 - acc: 0.8652 - f1: 0.8833 - val_loss: 0.2235 - val_acc: 0.9484 - val_f1: 0.9518\n",
      "Epoch 806/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3424 - acc: 0.8616 - f1: 0.8807 - val_loss: 0.2035 - val_acc: 0.9653 - val_f1: 0.9668\n",
      "Epoch 807/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3477 - acc: 0.8590 - f1: 0.8788 - val_loss: 0.2070 - val_acc: 0.9644 - val_f1: 0.9654\n",
      "Epoch 808/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3416 - acc: 0.8630 - f1: 0.8817 - val_loss: 0.2040 - val_acc: 0.9672 - val_f1: 0.9691\n",
      "Epoch 809/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3408 - acc: 0.8627 - f1: 0.8813 - val_loss: 0.2304 - val_acc: 0.9616 - val_f1: 0.9626\n",
      "Epoch 810/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3375 - acc: 0.8659 - f1: 0.8838 - val_loss: 0.2091 - val_acc: 0.9609 - val_f1: 0.9625\n",
      "Epoch 811/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3382 - acc: 0.8640 - f1: 0.8822 - val_loss: 0.2075 - val_acc: 0.9641 - val_f1: 0.9655\n",
      "Epoch 812/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3390 - acc: 0.8655 - f1: 0.8831 - val_loss: 0.2062 - val_acc: 0.9666 - val_f1: 0.9678\n",
      "Epoch 813/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3437 - acc: 0.8620 - f1: 0.8810 - val_loss: 0.2082 - val_acc: 0.9675 - val_f1: 0.9691\n",
      "Epoch 814/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3458 - acc: 0.8597 - f1: 0.8790 - val_loss: 0.2084 - val_acc: 0.9631 - val_f1: 0.9646\n",
      "Epoch 815/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3375 - acc: 0.8652 - f1: 0.8834 - val_loss: 0.2139 - val_acc: 0.9641 - val_f1: 0.9651\n",
      "Epoch 816/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3285 - acc: 0.8703 - f1: 0.8874 - val_loss: 0.2156 - val_acc: 0.9522 - val_f1: 0.9553\n",
      "Epoch 817/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3423 - acc: 0.8652 - f1: 0.8833 - val_loss: 0.2122 - val_acc: 0.9675 - val_f1: 0.9686\n",
      "Epoch 818/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3372 - acc: 0.8653 - f1: 0.8832 - val_loss: 0.2247 - val_acc: 0.9634 - val_f1: 0.9643\n",
      "Epoch 819/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3434 - acc: 0.8624 - f1: 0.8812 - val_loss: 0.2135 - val_acc: 0.9656 - val_f1: 0.9676\n",
      "Epoch 820/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3287 - acc: 0.8698 - f1: 0.8871 - val_loss: 0.2096 - val_acc: 0.9644 - val_f1: 0.9655\n",
      "Epoch 821/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3531 - acc: 0.8562 - f1: 0.8766 - val_loss: 0.2279 - val_acc: 0.9631 - val_f1: 0.9637\n",
      "Epoch 822/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3395 - acc: 0.8623 - f1: 0.8812 - val_loss: 0.2076 - val_acc: 0.9641 - val_f1: 0.9657\n",
      "Epoch 823/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3354 - acc: 0.8656 - f1: 0.8839 - val_loss: 0.2027 - val_acc: 0.9622 - val_f1: 0.9634\n",
      "Epoch 824/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3358 - acc: 0.8675 - f1: 0.8851 - val_loss: 0.2136 - val_acc: 0.9650 - val_f1: 0.9667\n",
      "Epoch 825/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3348 - acc: 0.8665 - f1: 0.8843 - val_loss: 0.2218 - val_acc: 0.9625 - val_f1: 0.9639\n",
      "Epoch 826/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3440 - acc: 0.8616 - f1: 0.8807 - val_loss: 0.2100 - val_acc: 0.9653 - val_f1: 0.9664\n",
      "Epoch 827/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3361 - acc: 0.8668 - f1: 0.8846 - val_loss: 0.2113 - val_acc: 0.9647 - val_f1: 0.9665\n",
      "Epoch 828/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3376 - acc: 0.8645 - f1: 0.8832 - val_loss: 0.2112 - val_acc: 0.9653 - val_f1: 0.9670\n",
      "Epoch 829/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3432 - acc: 0.8610 - f1: 0.8801 - val_loss: 0.2111 - val_acc: 0.9616 - val_f1: 0.9637\n",
      "Epoch 830/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3379 - acc: 0.8651 - f1: 0.8835 - val_loss: 0.2076 - val_acc: 0.9653 - val_f1: 0.9670\n",
      "Epoch 831/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3430 - acc: 0.8637 - f1: 0.8821 - val_loss: 0.2091 - val_acc: 0.9634 - val_f1: 0.9649\n",
      "Epoch 832/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3506 - acc: 0.8580 - f1: 0.8776 - val_loss: 0.2191 - val_acc: 0.9491 - val_f1: 0.9523\n",
      "Epoch 833/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3511 - acc: 0.8587 - f1: 0.8781 - val_loss: 0.2082 - val_acc: 0.9647 - val_f1: 0.9663\n",
      "Epoch 834/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3461 - acc: 0.8605 - f1: 0.8797 - val_loss: 0.2126 - val_acc: 0.9631 - val_f1: 0.9646\n",
      "Epoch 835/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3476 - acc: 0.8595 - f1: 0.8784 - val_loss: 0.2131 - val_acc: 0.9672 - val_f1: 0.9684\n",
      "Epoch 836/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3379 - acc: 0.8663 - f1: 0.8843 - val_loss: 0.2081 - val_acc: 0.9638 - val_f1: 0.9650\n",
      "Epoch 837/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3381 - acc: 0.8644 - f1: 0.8828 - val_loss: 0.2064 - val_acc: 0.9644 - val_f1: 0.9664\n",
      "Epoch 838/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3368 - acc: 0.8658 - f1: 0.8835 - val_loss: 0.2091 - val_acc: 0.9656 - val_f1: 0.9672\n",
      "Epoch 839/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3341 - acc: 0.8660 - f1: 0.8839 - val_loss: 0.2087 - val_acc: 0.9634 - val_f1: 0.9652\n",
      "Epoch 840/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3373 - acc: 0.8659 - f1: 0.8838 - val_loss: 0.2195 - val_acc: 0.9559 - val_f1: 0.9584\n",
      "Epoch 841/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3408 - acc: 0.8628 - f1: 0.8816 - val_loss: 0.2085 - val_acc: 0.9650 - val_f1: 0.9659\n",
      "Epoch 842/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3290 - acc: 0.8697 - f1: 0.8875 - val_loss: 0.2164 - val_acc: 0.9666 - val_f1: 0.9676\n",
      "Epoch 843/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3370 - acc: 0.8653 - f1: 0.8832 - val_loss: 0.2078 - val_acc: 0.9659 - val_f1: 0.9673\n",
      "Epoch 844/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3288 - acc: 0.8696 - f1: 0.8865 - val_loss: 0.2097 - val_acc: 0.9613 - val_f1: 0.9626\n",
      "Epoch 845/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3458 - acc: 0.8601 - f1: 0.8793 - val_loss: 0.2131 - val_acc: 0.9638 - val_f1: 0.9654\n",
      "Epoch 846/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3324 - acc: 0.8685 - f1: 0.8859 - val_loss: 0.2106 - val_acc: 0.9691 - val_f1: 0.9699\n",
      "Epoch 847/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3401 - acc: 0.8636 - f1: 0.8823 - val_loss: 0.2289 - val_acc: 0.9647 - val_f1: 0.9655\n",
      "Epoch 848/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3394 - acc: 0.8641 - f1: 0.8832 - val_loss: 0.2114 - val_acc: 0.9669 - val_f1: 0.9681\n",
      "Epoch 849/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3414 - acc: 0.8631 - f1: 0.8819 - val_loss: 0.2048 - val_acc: 0.9619 - val_f1: 0.9635\n",
      "Epoch 850/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3319 - acc: 0.8676 - f1: 0.8852 - val_loss: 0.2151 - val_acc: 0.9650 - val_f1: 0.9661\n",
      "Epoch 851/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3428 - acc: 0.8624 - f1: 0.8814 - val_loss: 0.2127 - val_acc: 0.9656 - val_f1: 0.9666\n",
      "Epoch 852/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3396 - acc: 0.8655 - f1: 0.8836 - val_loss: 0.2320 - val_acc: 0.9416 - val_f1: 0.9453\n",
      "Epoch 853/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3464 - acc: 0.8622 - f1: 0.8808 - val_loss: 0.2040 - val_acc: 0.9653 - val_f1: 0.9671\n",
      "Epoch 854/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3373 - acc: 0.8658 - f1: 0.8840 - val_loss: 0.2001 - val_acc: 0.9619 - val_f1: 0.9641\n",
      "Epoch 855/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3401 - acc: 0.8634 - f1: 0.8818 - val_loss: 0.2069 - val_acc: 0.9613 - val_f1: 0.9633\n",
      "Epoch 856/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3395 - acc: 0.8642 - f1: 0.8827 - val_loss: 0.2059 - val_acc: 0.9638 - val_f1: 0.9650\n",
      "Epoch 857/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3299 - acc: 0.8702 - f1: 0.8868 - val_loss: 0.2129 - val_acc: 0.9681 - val_f1: 0.9693\n",
      "Epoch 858/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3324 - acc: 0.8678 - f1: 0.8853 - val_loss: 0.2077 - val_acc: 0.9650 - val_f1: 0.9661\n",
      "Epoch 859/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3307 - acc: 0.8691 - f1: 0.8864 - val_loss: 0.2136 - val_acc: 0.9584 - val_f1: 0.9607\n",
      "Epoch 860/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3587 - acc: 0.8528 - f1: 0.8736 - val_loss: 0.2071 - val_acc: 0.9584 - val_f1: 0.9607\n",
      "Epoch 861/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3409 - acc: 0.8636 - f1: 0.8818 - val_loss: 0.2133 - val_acc: 0.9547 - val_f1: 0.9572\n",
      "Epoch 862/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3395 - acc: 0.8637 - f1: 0.8821 - val_loss: 0.2138 - val_acc: 0.9634 - val_f1: 0.9643\n",
      "Epoch 863/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3385 - acc: 0.8639 - f1: 0.8825 - val_loss: 0.2092 - val_acc: 0.9663 - val_f1: 0.9676\n",
      "Epoch 864/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3391 - acc: 0.8651 - f1: 0.8830 - val_loss: 0.2166 - val_acc: 0.9619 - val_f1: 0.9638\n",
      "Epoch 865/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3314 - acc: 0.8686 - f1: 0.8865 - val_loss: 0.2090 - val_acc: 0.9597 - val_f1: 0.9615\n",
      "Epoch 866/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3492 - acc: 0.8576 - f1: 0.8776 - val_loss: 0.2155 - val_acc: 0.9622 - val_f1: 0.9634\n",
      "Epoch 867/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3450 - acc: 0.8610 - f1: 0.8803 - val_loss: 0.2127 - val_acc: 0.9566 - val_f1: 0.9592\n",
      "Epoch 868/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3361 - acc: 0.8664 - f1: 0.8845 - val_loss: 0.2094 - val_acc: 0.9669 - val_f1: 0.9678\n",
      "Epoch 869/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3387 - acc: 0.8652 - f1: 0.8831 - val_loss: 0.2103 - val_acc: 0.9625 - val_f1: 0.9640\n",
      "Epoch 870/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3301 - acc: 0.8689 - f1: 0.8864 - val_loss: 0.2135 - val_acc: 0.9684 - val_f1: 0.9699\n",
      "Epoch 871/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3447 - acc: 0.8621 - f1: 0.8810 - val_loss: 0.2117 - val_acc: 0.9550 - val_f1: 0.9572\n",
      "Epoch 872/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3299 - acc: 0.8702 - f1: 0.8872 - val_loss: 0.2116 - val_acc: 0.9659 - val_f1: 0.9672\n",
      "Epoch 873/5000\n",
      "12800/12800 [==============================] - 0s 15us/sample - loss: 0.3390 - acc: 0.8641 - f1: 0.8824 - val_loss: 0.2041 - val_acc: 0.9631 - val_f1: 0.9643\n",
      "Epoch 874/5000\n",
      "12800/12800 [==============================] - 0s 16us/sample - loss: 0.3358 - acc: 0.8662 - f1: 0.8843 - val_loss: 0.2158 - val_acc: 0.9591 - val_f1: 0.9610\n",
      "Epoch 875/5000\n",
      " 7500/12800 [================>.............] - ETA: 0s - loss: 0.3358 - acc: 0.8675 - f1: 0.8849"
     ]
    }
   ],
   "source": [
    "training_delay_RN = []\n",
    "predicting_delay_RN = []\n",
    "history_obj = []\n",
    "cpt = 0\n",
    "best_accuracy_RN = 0\n",
    "\n",
    "epochs_range = [30,60, 5000]                                                                                                                                            \n",
    "max_ep = max(epochs_range)                                                                                                                                                         \n",
    "\n",
    "for ep in epochs_range:                                                                                                                                                            \n",
    "    model = RN_model(layer_sizes, dropout, learning_rate)                                                                                                                          \n",
    "    #### Apprentissage                                                                                                                                                             \n",
    "    start = time.time()                                                                                                                                                            \n",
    "    #model.fit(X_train, Y_train, batch_size = 100, epochs = 60)                                                                                                                    \n",
    "    hist_obj = model.fit(X_train, Y_train, batch_size = batch_size, epochs = ep, validation_data=(X_test, Y_test))                                                                 \n",
    "\n",
    "    end = time.time()                                                                                                                                                              \n",
    "    training_delay_RN.append(end - start)                                                                                                                                          \n",
    "\n",
    "    ho_tmp = list(hist_obj.history.values())                                                                                                                                       \n",
    "    ho_tmp = [i + [np.nan for _ in range(max_ep-ep)] for i in ho_tmp ]                                                                                                             \n",
    "    history_obj.append(ho_tmp)\n",
    "    #### Prédiction                                                                                                                                                                \n",
    "    start = time.time()                                                                                                                                                            \n",
    "\n",
    "    Y_pred = np.where(model.predict(X_test) > 0.5, 1, 0)                                                                                                                           \n",
    "\n",
    "    end = time.time()                                                                                                                                                              \n",
    "    predicting_delay_RN.append(end - start) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ho = np.array(history_obj)\n",
    "ho = ho.transpose(1,2,0)\n",
    "\n",
    "leg = [str(i) for i in epochs_range]                                                                                                                                                \n",
    "                                                                                                                                       \n",
    "titre = \"RN : HyperParam = number of epochs\"                                                                                                                                          \n",
    "\n",
    "plot_perf(ho, leg, titre ,sub_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "EXPLICATION A REVOIR CAR JE NE VOIS PAS DE SURAPPRENTISSAGE : \n",
    "Nous remarquons que les performances (accuracy et f1_score) tendent vers les mêmes taux qu'il y ait 30, 60 ou 120 itérations. \n",
    "\n",
    "(graphe time - itérations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_delay(training_delay_RN,predicting_delay_RN,titre)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "Notons ici que le temps d'apprentissage et de prédiction croissent avec le nombre d'itération."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Question 5\n",
    "(1 - La démarche de comparaison des hyperparamètres est sérieuse. Les résultats sont présentés de façon correcte et concise dans un tableau et un graphique.\n",
    "2 - Les explications montrant les différences sont claires, concises et plausibles.\n",
    "3 - La configuration matérielle sur laquelle les expérimentations est présenteainsi que le temps d’exécution requis pour compléter les expérimentations)\n",
    "\n",
    "1&2 : (alterner graphe et explication (cf q3) ou mettre tous les graphes (en q3) puis explication ici en q5.\n",
    "\n",
    "3 : regarder les specs du PC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Question 6\n",
    "(1 - La méthode est décrite et un lien avec l’implémentation est fait afin d’expliquer comment l’équipe a trouvé le meilleur modèle SVM. \n",
    "2- Les résultats sont présentés de façon correcte et concise dans un TABLEAU ET un GRAPHIQUE. \n",
    "3 - La configuration matérielle sur laquelle les expérimentations est présente ainsi que le temps d’exécution requis pour compléter les expérimentations.)\n",
    "\n",
    "1 - explication implémentation SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "source": [
    "## Question 7\n",
    "(L’impact de la taille de l’ensemble d’apprentissage sur les performances est présent et décrit convenablement.)\n",
    "\n",
    "Dans le cas du MLP, on remarque que pour 1600 images, le temps d'apprentissage se situe autour de 5 secondes. Pour 16000, le temps d'apprentissage est d'environ 32 secondes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Question 8\n",
    "(Un classificateur est recommandé en se basant sur l’expérimentation précédemment effectuée.)\n",
    "\n",
    "SVM ou MLP en fonction des résultats (perf + time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Question 9\n",
    "(Des pistes d’amélioration sont proposées.)\n",
    "\n",
    "Bonne question : investiguer davantages les études d'hyperparamètres pour trouver une solution optimale ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Conlusion\n",
    "\n",
    "(1 - Un court résumé du problème est présent.\n",
    "2 - Un rappel des résultats est présent.\n",
    "3 - Des pistes pour de possibles améliorations sont présentes.)\n",
    "\n",
    "Ce troisième laboratoire nous a permis de comprendre davantage le fonctionnment de deux nouvelles méthodes de classification : SVM et MLP. Dans les deux cas, les temps d'apprentissage sont bien plus long que les premiers laboratoires. \n",
    "Avantages et incovénients entre les deux méthodes:\n",
    "MLP : nb de sorties non limitée != svm => MLP plus évolutif si plus de deux catégories. \n",
    "\n",
    "Les résultats sont meilleurs dans le cas de ..... (MLP,SVM ?).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
