{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratoire 4 : Développement d'un système intelligent\n",
    "#### Département du génie logiciel et des technologies de l’information\n",
    "\n",
    "| Étudiants             | Alexandre Bleau — BLEA14058906 / David Létinaud  — LETD05129708 / Thomas Lioret   — LIOT20069605|\n",
    "|-----------------------|---------------------------------------------------------|\n",
    "| Cours                 | GTI770 - Systèmes intelligents et apprentissage machine |\n",
    "| Session               | Automne 2019                                            |\n",
    "| Groupe                | 1                                                       |\n",
    "| Numéro du laboratoire | 4                                                       |\n",
    "| Professeur            | Prof. LOMBAERT                                          |\n",
    "| Chargé de laboratoire | Pierre-Luc Delisle                                      |\n",
    "| Date                  | 18/12/2019                                              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction et revue de la littérature\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour choisir correctement les modèles à associés aux ensembles de features, il est important d'étudier ce qui a déjà été fait.\n",
    "Ainsi, on s'est notamment appuyé sur l'étude \"FACILITATING COMPREHENSIVE BENCHMARKING EXPERIMENTS\n",
    "ON THE MILLION SONG DATASET\" réalisée par l'ISMIR (International Society for Music Information Retrieval) en 2012.\n",
    "\n",
    "La combinaison la plus performante a permis d'obtenir une précision de 27,41 % en appliquant un modèle SVM sur l'ensemble de features \"Statistical Spectrum Descriptor\"(SSD). Sur ces caractéristiques, l'algorithme KNN a produit une précision comparable de 27,07 %. Les autres modèles testés, baïes naïf, l'arbre de décision et \"random forest\" ont tous donné un taux d'exactitude entre 14 et 20 % environ. Outre le SSD, d'autres ensembles ont également fait l'objet d'essais, mais tous ont donné des résultats d'une précision inférieur, à une près. \n",
    "Les ensembles \"MFCC\" et \"Spectral Derivates\" offrent tout de même des performances comparables bien qu'inférieures à celle que peut offrir SSD.\n",
    "\n",
    "Toutefois, cette étude n'analyse pas les performances que peut offrir un réseau de neurones. En effet, ce modèle de classification n'était pas encore très développé. On a tout de même trouvé une étude plus récente sur la classification de genre de musique. Celle-ci utilise une autre base de donnée appelée : \"Free Music Archive\" (FMA) constitué de 161 genres. Cette étude, \"FMA: A DATASET FOR MUSIC ANALYS\" est disponible ici : harxiv.org/pdf/1612.01840.pdf. Bien que le dataset soit différent, il est intéressant de noter que le modèle MLP (\"MultiLayer Perceptron\") performe très bien avec l'ensemble de features MFCC. Il atteint même une précision de 53%, ce qui en fait pottentiellement un très bon candidat pour notre dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme expliqué précedemment, le modèle MLP est un bon candidat. Toutefois, nous n'avons de données sur sa performance avec les ensembles de features du dataset MSD. Nous allons donc créé un modèle MLP est le tester sur tout les ensembles de features disponible pour comparer ses performances. Nous irons plus en détails dans l'ajustement des hyperparamètres par la suite.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\David\\Anaconda3\\envs\\GTI 770-BIS\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from RN_model import RN_model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import time\n",
    "from functions import get_data, plot_perf_epochs,plot_perf_delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_path_tab = []\n",
    "direct_path_tab.append(\"./tagged_feature_sets/msd-ssd_dev/msd-ssd_dev.csv\") # best =>31%\n",
    "direct_path_tab.append(\"./tagged_feature_sets/msd-jmirmfccs_dev/msd-jmirmfccs_dev.csv\")\n",
    "direct_path_tab.append(\"./tagged_feature_sets/msd-jmirspectral_dev/msd-jmirspectral_dev.csv\")\n",
    "direct_path_tab.append(\"./tagged_feature_sets/msd-jmirderivatives_dev/msd-jmirderivatives_dev.csv\") # 3rd => 25%\n",
    "direct_path_tab.append(\"./tagged_feature_sets/msd-jmirlpc_dev/msd-jmirlpc_dev.csv\")\n",
    "direct_path_tab.append(\"./tagged_feature_sets/msd-jmirmoments_dev/msd-jmirmoments_dev.csv\")\n",
    "direct_path_tab.append(\"./tagged_feature_sets/msd-marsyas_dev_new/msd-marsyas_dev_new.csv\") # 2nd => 27%\n",
    "direct_path_tab.append(\"./tagged_feature_sets/msd-mvd_dev/msd-mvd_dev.csv\")\n",
    "direct_path_tab.append(\"./tagged_feature_sets/msd-rh_dev_new/msd-rh_dev_new.csv\")\n",
    "direct_path_tab.append(\"./tagged_feature_sets/msd-trh_dev/msd-trh_dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP hyperparamaters\n",
    "layer_sizes = [500]\n",
    "epochs = 50\n",
    "learning_rate = 0.0005\n",
    "batch_size = 500\n",
    "\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_delay_RN = []\n",
    "predicting_delay_RN = []\n",
    "history_obj = []\n",
    "cpt = 0\n",
    "best_accuracy_RN = 0\n",
    "f1_RN = []\n",
    "acc_RN = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:      \n",
    "    shutil.rmtree('./logs')\n",
    "except:\n",
    "    print(\"nothing to delete\")\n",
    "# Callbacks pour affichage des performances dans tensorboard : callback pour chaque hyperparamètre\n",
    "tensorboard_callback = []\n",
    "for i in range(len(direct_path_tab)):\n",
    "    tensorboard_callback.append(TensorBoard(log_dir=\"logs\\{}\".format(i)))\n",
    "# Par invité de commande : \n",
    "# tensorboard --logdir=\"./logs\" --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\David\\Anaconda3\\envs\\GTI 770-BIS\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Train on 143644 samples, validate on 35911 samples\n",
      "Epoch 1/50\n",
      "143644/143644 [==============================] - 8s 55us/sample - loss: 2.7794 - acc: 0.1648 - val_loss: 2.5986 - val_acc: 0.2095\n",
      "Epoch 2/50\n",
      "143644/143644 [==============================] - 6s 45us/sample - loss: 2.5813 - acc: 0.2148 - val_loss: 2.5192 - val_acc: 0.2351\n",
      "Epoch 3/50\n",
      "143644/143644 [==============================] - 7s 49us/sample - loss: 2.5229 - acc: 0.2301 - val_loss: 2.4789 - val_acc: 0.2454\n",
      "Epoch 4/50\n",
      "143644/143644 [==============================] - 7s 46us/sample - loss: 2.4906 - acc: 0.2403 - val_loss: 2.4528 - val_acc: 0.2506\n",
      "Epoch 5/50\n",
      "143644/143644 [==============================] - 7s 46us/sample - loss: 2.4654 - acc: 0.2469 - val_loss: 2.4299 - val_acc: 0.2593\n",
      "Epoch 6/50\n",
      "143644/143644 [==============================] - 6s 44us/sample - loss: 2.4467 - acc: 0.2534 - val_loss: 2.4162 - val_acc: 0.2635\n",
      "Epoch 7/50\n",
      "143644/143644 [==============================] - 7s 47us/sample - loss: 2.4296 - acc: 0.2571 - val_loss: 2.3996 - val_acc: 0.2685\n",
      "Epoch 8/50\n",
      " 93000/143644 [==================>...........] - ETA: 2s - loss: 2.4184 - acc: 0.2600"
     ]
    }
   ],
   "source": [
    "cpt = 0\n",
    "for path_ in direct_path_tab:\n",
    "    # Get data / normalize it / split into train&test\n",
    "    X, Y = get_data(path_)\n",
    "    X = preprocessing.normalize(X, norm='max',axis = 0)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8,random_state=60, stratify=Y)  # 70% training and 30% test\n",
    "\n",
    "    # Calcul du nombres de features/classes et taille du dataset\n",
    "    nb_features = len(X[0])\n",
    "    nb_classes = max(Y)+1\n",
    "    train_size = len(X)\n",
    "\n",
    "    model = RN_model(layer_sizes, dropout, learning_rate, nb_features, nb_classes)\n",
    "    \n",
    "    #### Apprentissage                                                                                                                                                               \n",
    "    start = time.time()                                                                                                                   \n",
    "    hist_obj = model.fit(X_train[0:train_size], Y_train[0:train_size], batch_size = batch_size, epochs = epochs, validation_data=(X_test, Y_test), callbacks = [tensorboard_callback[cpt]]) \n",
    "    end = time.time()\n",
    "    training_delay_RN.append(end - start)\n",
    "    \n",
    "    history_obj.append( list(hist_obj.history.values()))\n",
    "\n",
    "    #### Prédiction                                                                                                                                                                  \n",
    "    start = time.time()\n",
    "    Y_pred_temp = model.predict(X_test)\n",
    "    end = time.time()\n",
    "    predicting_delay_RN.append(end - start)\n",
    "\n",
    "    # Remise en forme de Y_pred\n",
    "    Y_pred = []\n",
    "    for i in Y_pred_temp:\n",
    "        Y_pred.append(np.argmax(i)) \n",
    "    \n",
    "    f1 = metrics.f1_score(Y_test, Y_pred,average='weighted')\n",
    "    acc = metrics.accuracy_score(Y_test, Y_pred)\n",
    "    print(\"acc :\", acc,\"f1 :\", f1)\n",
    "    \n",
    "    f1_RN.append(f1)\n",
    "    acc_RN.append(acc)\n",
    "    cpt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mise en forme des données pour l'affichage\n",
    "ho = np.array(history_obj)\n",
    "ho = ho.transpose(1,2,0)  \n",
    "\n",
    "# Pour affichage\n",
    "sub_title = ['loss','acc','val_loss','val_acc']\n",
    "x_lab = \"epochs\"\n",
    "leg = [str(i) for i in range(len(direct_path_tab))]  \n",
    "titre = \"RN : Dataset test\"                                                                                                                                         \n",
    "\n",
    "plot_perf_epochs(ho, leg, titre ,sub_title)\n",
    "plot_perf_delay(f1_RN,acc_RN,training_delay_RN,predicting_delay_RN,titre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration de l'environnement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitionnement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prétraitement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "### Description des modèles et justifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "### Matrice des expérimentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice des résultats de l'étude des hyperparamètres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "### Présentation de la conception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "### Hyperparamètres des modèles choisis dans la conception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice des résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7 \n",
    "### Formulation des recommandations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
