{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratoire 2 : Arbre de désision, Bayes naïf et KNN\n",
    "#### Département du génie logiciel et des technologies de l’information\n",
    "\n",
    "| Étudiants             | Alexandre Bleau — BLEA14058906 / David Létinaud  — LETD05129708 / Thomas Lioret   — LIOT20069605|\n",
    "|-----------------------|---------------------------------------------------------|\n",
    "| Cours                 | GTI770 - Systèmes intelligents et apprentissage machine |\n",
    "| Session               | Automne 2019                                            |\n",
    "| Groupe                | 1                                                       |\n",
    "| Numéro du laboratoire | 2                                                       |\n",
    "| Professeur            | Prof. LOMBAERT                                          |\n",
    "| Chargé de laboratoire | Pierre-Luc Delisle                                      |\n",
    "| Date                  | 27/10/2019                                              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Au cours de ce second laboratoire, nous allons étudier l’utilisation de trois algorithmes de classification. Nous utiliserons les arbres de décision comme vu précédemment, Bayes naïf et les K plus proches voisins (KNN). Nous allons aborder deux problèmes : la classification des galaxies ansi que la classification des courriels indésirables.\n",
    "Afin de classer les galaxies en « spirales » ou « smooth », nous allons nous appuyer sur deux nouveaux algorithmes (Bayes naïf et KNN) ainsi que des prétraitements de données adaptées (MinMaxScaler). Dans le deuxième cas nous utiliserons également ces deux nouveaux algorithmes et ajouterons un cas de prétraitement supplémentaire (Discrétisation non-supervisée).\n",
    "Enfin nous appliquerons le concept de validation croisée (cross-validation) et nous le comparerons à la validation holdout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "### Méthode de création des ensembles de données\n",
    "Pour éviter de recalculer plusieurs fois les features, il peut être plus efficace de les enregistrer pour pouvoir les réutiliser par la suite. On va par exemple enregistrer les features utilisées dans le TP1 dans un fichier csv. Pour cela, on utilise la bibliothèque 'csv' et on ouvre un fichier en écriture. On utilisera ensuite la fonction 'writerows' pour écrire dans ce fichier une list en format compatible csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "\n",
    "import csv\n",
    "from color import crop_center\n",
    "from main_functions import  FeaturesProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/home/ens/AQ38840/Desktop/data/data/csv/galaxy/galaxy_label_data_set.csv\"\n",
    "image_path = \"/home/ens/AQ38840/Desktop/data/data/images/\"\n",
    "# Fichier de sortie\n",
    "TP1_features_path = \"/home/ens/AQ38840/Desktop/data/data/csv/galaxy/TP1_features.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ens/AQ38840/Desktop/data/data/csv/galaxy/galaxy_label_data_set.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0da9ca840707>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m########################################   Lecture   ########################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;31m# Lecture du fichier CSV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[0mf_csv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_csv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# On passe la 1ere ligne d'entête\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ens/AQ38840/Desktop/data/data/csv/galaxy/galaxy_label_data_set.csv'"
     ]
    }
   ],
   "source": [
    "# Taille de rognage de l'image\n",
    "crop_size = 180\n",
    "\n",
    "TP1_feat_lignes = []\n",
    "\n",
    "# Hyperaramètres de chaque features determinées au TP1\n",
    "fft_threshold = 140\n",
    "color_center_size = 18\n",
    "bp_calibration = [100,50]\n",
    "  \n",
    "########################################   Lecture   ########################################\n",
    "# Lecture du fichier CSV\n",
    "with open(dataset_path) as f:\n",
    "    f_csv = csv.reader(f)\n",
    "    next(f_csv) # On passe la 1ere ligne d'entête\n",
    "    \n",
    "    # Lecture ligne par ligne\n",
    "    for ligne in f_csv:#,i in zip(f_csv,range(nb_img)):\n",
    "        l_CSV = []\n",
    "        # Lecture et rognage de l'image\n",
    "        image = crop_center(io.imread( image_path + ligne[0] + \".jpg\" ),crop_size,crop_size)\n",
    "        X = FeaturesProcess(image, color_center_size, fft_threshold, bp_calibration)\n",
    "\n",
    "        l_CSV.append(ligne[0])   # numéro d'image\n",
    "        l_CSV.append(str(X[0]))  # feature 1\n",
    "        l_CSV.append(str(X[1]))  # feature 2  \n",
    "        l_CSV.append(str(X[2]))  # feature 3\n",
    "        l_CSV.append(str(1 * (ligne[1]==\"spiral\"))) # classe de l'image\n",
    "        TP1_feat_lignes.append(l_CSV)\n",
    "\n",
    "f.close()\n",
    "########################################   Ecriture   ########################################\n",
    "print(TP1_feat_lignes)\n",
    "with open(TP1_features_path, 'w') as writeFile:\n",
    "    writer = csv.writer(writeFile)\n",
    "    writer.writerows(TP1_feat_lignes)\n",
    "    writeFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Détails des ensembles produits\n",
    "L'ensemble de données ainsi produit contient toutes les informations utiles pour de l'apprentissage supervisée et sont disposées de cette façon :\n",
    "['numero_image', 'feature_1','feature_2','feature_3', 'classe_image']\n",
    "\n",
    "Le fichier TP1_features.csv sera ainsi utilisé pour compléter les features contenu dans le fichiers 'galaxy_feature_vectors.csv'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "### Approche de validation proposée et justification\n",
    "Nous allons ici utiliser 3 approches de classification différentes (arbre de décision, Bayes naïf et KNN) pour traiter des mails et des images.\n",
    "\n",
    "Pour valider et comparer l'efficacité des modèles nous utiliserons la précision en combinaison avec le F1-score.\n",
    "\n",
    "La précision mesure quelle proportion de bonne classification. Elle permet de savoir immédiatement si un modèle est correctement entraîné et comment il peut fonctionner en général. Toutefois, il ne donne pas d'informations détaillées concernant son application au problème, c'est pour cela qu'on le combine avec le F1 score.\n",
    "\n",
    "F1 score est une mesure globale de la précision d'un modèle de classification binaire, qui combine précision et rappel. La précision répond à la question : Quelle proportion d'identifications positives était effectivement correcte ? alors que le Rappel (ou sensitivité) : Quelle proportion de résultats positifs réels a été identifiée correctement ?\n",
    "Un bon F1 score signifie que le modèle prédit peu de faux positifs et peu de faux négatifs. Le F1 score est considéré comme parfait lorsqu'il est de 1, alors que le modèle est un échec total lorsqu'il est de 0.\n",
    "\n",
    "Nous avons utilisé la méthode de validation holdout (split stratify) qui est la plus simple et la plus rapide pour tester les algorithmes de classification. Stratify nous permet de garder la même répartition des données dans les données de tests et d'entrainement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "### Matrice des expérimentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "### Étude des hyperparamètres et des modèles\n",
    "Pour choisir les meilleurs hyperparamètres des différents modèles nous allons mesurer les performances de prédiction pour chaque valeur des hyperparamètres.\n",
    "\n",
    "Pour ce faire, nous allons entraîner chaque modèle avec 100% du dataset. La méthode de validation holdout nous permet de le faire. Chaque modèle est entrainé successivement avec des valeurs d'hyperparamètres compris dans un intervalle choisi judicieusement. La précision et le F1-score sont à chaque fois sauvegardés dans une matrice. On pourra ainsi choisir les hyperparamètres qui donne les meilleures performances au modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from main_functions import  *\n",
    "\n",
    "from Tree import decision_tree\n",
    "from Knn import KNN\n",
    "from Bayes import bayes_gaussian_noProcess, bayes_mutltinomial_scaleData, bayes_multinomial_kbinDiscretization\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################   Initialisations   ########################################\n",
    "dataset_path = \"/home/ens/AQ38840/Desktop/data/data/csv/galaxy/galaxy_feature_vectors.csv\"\n",
    "TP1_features_path = \"/home/ens/AQ38840/Desktop/data/data/csv/galaxy/TP1_features.csv\"\n",
    "\n",
    "# Nombre d'images total du dataset (training + testing)\n",
    "nb_img = 16000\n",
    "# Pourcentage de données utilisées pour l'entrainement\n",
    "ratio_train = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "Y=[]\n",
    "\n",
    "########################################   Lecture   ########################################\n",
    "# Lecture du fichier CSV\n",
    "with open(dataset_path, 'r') as f:\n",
    "    with open(TP1_features_path, 'r') as f_TP1:\n",
    "        TP1_features_list = list(csv.reader(f_TP1, delimiter=','))\n",
    "        features_list = list(csv.reader(f, delimiter=','))\n",
    "\n",
    "        # Recuperation des numéros des images dans l'ordre généré par le TP1\n",
    "        TP1_features_list_np = np.array(TP1_features_list)[:,0]\n",
    "\n",
    "        # Lecture ligne par ligne\n",
    "        for c in range(nb_img):\n",
    "            features = [float(i) for i in features_list[0][1:75]]\n",
    "\n",
    "            num_img = str(int(float(features_list[0][0])))\n",
    "\n",
    "            try :\n",
    "                # Cherche l'index de l'image num_img dans TP1_features_list\n",
    "                # pour faire correspondre les features du TP1 avec les nouveaux features\n",
    "                index = np.where(TP1_features_list_np==num_img)[0]\n",
    "\n",
    "                features_TP1 = [float(i) for i in TP1_features_list[index[0]][1:4]]\n",
    "\n",
    "                # concatenation des features\n",
    "                features = features_TP1 + features\n",
    "\n",
    "                galaxy_class = int(float(features_list[0][75]))\n",
    "\n",
    "                X.append(features)\n",
    "                Y.append(galaxy_class)\n",
    "            except :\n",
    "                print(\"Image {} not find\".format(num_img) )\n",
    "\n",
    "            features_list.pop(0)\n",
    "            #print(type(features),type(galaxy_class))\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=ratio_train,random_state=1, stratify=Y)  # 80% training and 20% test\n",
    "\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puisque le dataset des galaxies est grand et assez bien équilibré, l'option 'stratify' n'est pas forcemment très utile mais le sera pour le dataset des courriels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etendu de test des hyperparamètres\n",
    "list_zt = [None, 3, 5, 10, 30, 50]\n",
    "list_K = np.arange(1, 50, 2)\n",
    "\n",
    "list_nbins = np.arange(3, 15, 1)\n",
    "list_var_smoothing = [i for i in np.linspace(1e-11, 1e-8, 10)]  # On fait varier l'hyperparamètre pour le\n",
    "list_scaler = [i for i in np.linspace(0.2, 3, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la meilleure profondeur de l'arbre de décision \n",
    "max_acc, max_f1, elem_acc, elem_f1, x_plot, acc_plot, f1_plot = best_hyper_param(decision_tree,X_train, X_test, Y_train, Y_test, list_zt)\n",
    "print(\"zoo_tree :\")\n",
    "print(\"    Best acc :\", max_acc, elem_acc)\n",
    "print(\"    Best f1 : \", max_f1, elem_f1)\n",
    "plot_hyper_param( x_plot, acc_plot, f1_plot, \"Profondeur TREE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du meilleur nombre de voisins K\n",
    "max_acc, max_f1, elem_acc, elem_f1, x_plot, acc_plot, f1_plot = best_hyper_param(KNN,X_train, X_test, Y_train, Y_test, list_K)\n",
    "print(\"KNN :\")\n",
    "print(\"    Best acc :\", max_acc, elem_acc)\n",
    "print(\"    Best f1 : \", max_f1, elem_f1)\n",
    "plot_hyper_param( x_plot, acc_plot, f1_plot, \"K\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce cas, l'évolution de la précision de KNN en fonction de K est assez surprenante. En effet, on s'attendait plutôt que la précision chute avec l'augmentation de K. QU'EST-CE QUI EST A REVOIR!!!!!?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du meilleur var_smooth\n",
    "max_acc, max_f1, elem_acc, elem_f1, x_plot, acc_plot, f1_plot = best_hyper_param(bayes_gaussian_noProcess,X_train, X_test, Y_train, Y_test, list_var_smoothing)\n",
    "print(\"Bayes gauss no process :\")\n",
    "print(\"    Best acc :\", max_acc, elem_acc)\n",
    "print(\"    Best f1 : \", max_f1, elem_f1)\n",
    "plot_hyper_param( x_plot, acc_plot, f1_plot, \"var_smooth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du meilleur scale\n",
    "max_acc, max_f1, elem_acc, elem_f1, x_plot, acc_plot, f1_plot = best_hyper_param(bayes_mutltinomial_scaleData,X_train, X_test, Y_train, Y_test, list_scaler)\n",
    "print(\"Bayes multinomial scale :\")\n",
    "print(\"    Best acc :\", max_acc, elem_acc)\n",
    "print(\"    Best f1 : \", max_f1, elem_f1)\n",
    "plot_hyper_param( x_plot, acc_plot, f1_plot, \"scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du meilleur nbins\n",
    "max_acc, max_f1, elem_acc, elem_f1, x_plot, acc_plot, f1_plot = best_hyper_param(bayes_multinomial_kbinDiscretization,X_train, X_test, Y_train, Y_test, list_nbins)\n",
    "print(\"Bayes Discretization :\")\n",
    "print(\"    Best acc :\", max_acc, elem_acc)\n",
    "print(\"    Best f1 : \", max_f1, elem_f1)\n",
    "plot_hyper_param( x_plot, acc_plot, f1_plot, \"nbins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut ainsi faire une étude similaire dans le cas du dataset des courriels.\n",
    "Il faut toutefois faire attention ici, le dataset est plutôt restreint et les données ne sont pas également réparties. L'option 'Stratify' est donc importante, nous verifierons son bon fonctionnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut vérifier que l'option 'stratify' fonctionne bien comme voulue :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "den = np.size(Y_train)\n",
    "num_train = np.size(np.where(Y_train ==0))\n",
    "num_test = np.size(np.where(Y_train ==0))\n",
    "print(num_train/den, num_test/den)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les ratios sont quasi-identiques, le jeu de donnée est donc correctement balancé entre X_train et X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "### Impact de la taille des ensembles de données sur la performance de classification\n",
    "Plus l'ensemble de données est grand et plus le modèle est efficace. Si on utilise un trop petit nombre d'échantillons pour entraîner un modèle, il risque de ne pas être très performant lors de la prédiction. En effet, il est probable que le nombre d'échantillons atypiques soient sur-représentés.\n",
    "Toutefois, de part le fonctionnement de la cross-validation, cette méthode s'avère plus efficace quand il s'agit de traiter des petits ensembles de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "### Impact du bruit dans les ensembles de données sur la performance de classification\n",
    "Dans un ensemble de données, le bruit peu diminuer les performances d'un modèle. Il est important de prendre un nombre d'échantillons assez grand pour éviter d'entraîner un modèle avec un nombre d'échantillons atypiques trop grand par rapport aux autres.\n",
    "Toutefois, le bruit peu avoir des impacts différents sur certains modèles. Les arbres de décision sont peu robuste aux bruits des données. On peut aussi prendre comme exemple KNN. Plus le bruit est important plus il est préférable d'augmenter l'hyperparamètre K (nombre de voisins) pour lisser l'effet du bruit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "### Discussion sur la nature des données\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "### Recommandations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "### Améliorations possibles\n",
    "Dans ce lab, nous avons exploré 3 types de modèles, mais un autre modèle pourrait peut-être être plus performant dans notre cas. On pourrait tester un plus grand nombre de modèles comme par exemple un réseau de neurones ou des SVM. \n",
    "Par ailleurs, il serait judicieux d'augmenter le nombre d'échantillons du dataset des mails et faire en sorte que le nombre de spams soit environ égale à celui des mails. On pourrait par exemple utiliser des techniques d'augmentation de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Ici, le problème de base était la classification de mail et d'images. Toutefois, pour obtenir les meilleurs résultats, cela soulève plusieurs problématiques. Il a été par exemple utile de tester plusieurs types de modèles pour être capable de choisir celui qui performe le mieux. D'autre part, l'analyse de performances a également été cruciale. Il a été nécessaire d'étudier les hyperparamètres qui donnent les meilleurs résultats."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
